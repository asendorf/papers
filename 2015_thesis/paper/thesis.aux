\relax 
\@writefile{toc}{\contentsline {chapter}{DEDICATION}{ii}}
\@writefile{toc}{\contentsline {chapter}{ACKNOWLEDGEMENTS}{iii}}
\newlabel{Acknowledgements}{{}{v}}
\citation{fawcett2006introduction}
\citation{fawcett2006introduction}
\citation{fawcett2006introduction}
\@writefile{toc}{\contentsline {chapter}{LIST OF FIGURES}{xii}}
\citation{bao2014canonical}
\citation{rao2008polynomial}
\citation{fawcett2006introduction}
\citation{fawcett2006introduction}
\@writefile{lof}{\noindent \relax $\@@underline {\hbox {\bf  Figure}}\mathsurround \z@ $\relax \hfill \rm  \newline }
\@writefile{toc}{\contentsline {chapter}{LIST OF TABLES}{xxix}}
\@writefile{lot}{\noindent \relax $\@@underline {\hbox {\bf  Table}}\mathsurround \z@ $\relax \hfill \rm  \newline }
\@writefile{toc}{\contentsline {chapter}{LIST OF APPENDICES}{xxxi}}
\@writefile{loa}{\noindent \relax $\@@underline {\hbox {\bf  Appendix}}\mathsurround \z@ $\relax \hfill \rm  \newline }
\@writefile{toc}{\contentsline {chapter}{ABSTRACT}{xxxii}}
\newlabel{Abstract}{{}{xxxiii}}
\@writefile{toc}{\mbox { }\newline \noindent {\bf  CHAPTER}\newline }
\@writefile{toc}{\hbox { }}
\citation{hotelling1936relations}
\@writefile{toc}{\hbox { }}
\@writefile{toc}{\contentsline {chap}{\numberline {\hbox { }\hfill \bf  I.\hspace  {5pt}}{\bf  Introduction}}{1}}
\@writefile{toc}{\hbox { }}
\newlabel{sec:intro}{{I}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1.1}Canonical Correlation Analysis (CCA)}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.1}What is it? What is it not?}{1}}
\citation{gunderson1997estimating}
\citation{pezeshki2004empirical}
\citation{pezeshki2004empirical}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Illustration of multi-modal data fusion\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:data_fusion}{{1.1}{2}}
\citation{ge2009does}
\citation{nadakuditi2011fundamental}
\citation{vinod1976canonical}
\citation{thum2014supervised}
\citation{cruz2014fast}
\citation{tenenhaus2014regularized}
\citation{akaho2006kernel}
\citation{welling2005kcca}
\citation{waaijenborg2009correlating}
\citation{mandal2013non}
\citation{chang2013canonical}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.2}Variations on CCA}{3}}
\citation{hardoon2011sparse}
\citation{yan2014accelerating}
\citation{sun2013canonical}
\citation{shin2015canonical}
\citation{tao2014exploring}
\citation{gao2014efficient}
\citation{zhang2013binary}
\citation{witten2009penalized}
\citation{klami2013bayesian}
\citation{chu2013sparse}
\citation{hardoon2004canonical}
\citation{dhillon2011multi}
\citation{melzer2001nonlinear}
\citation{zhai2015instance}
\citation{lisanti2014matching}
\citation{ahsan2014clustering}
\citation{hardoon2006correlation}
\citation{chaudhuri2009multi}
\citation{deleus2011functional}
\citation{arbabshirani2010comparison}
\citation{khalid2013improving}
\citation{guccione2013functional}
\citation{correa2010canonical}
\citation{lin2013identifying}
\citation{seoane2014canonical}
\citation{lin2013group}
\citation{zhang2013l1}
\citation{nakanishi2014enhancing}
\citation{zhang2014frequency}
\citation{spuler2013spatial}
\citation{campi2013non}
\citation{chen2014removal}
\citation{kuzilek2014comparison}
\citation{via2005canonical}
\citation{pezeshki2006canonical}
\citation{scharf1998wiener}
\citation{li2009joint}
\citation{nielsen2002multiset}
\citation{ge2009does}
\citation{scharf2000canonical}
\citation{manco2014kernel}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1.3}Applications}{4}}
\citation{todros2012measure}
\citation{torres2007finding}
\citation{wilks2014probabilistic}
\citation{prera2014using}
\citation{steward2014assimilating}
\citation{li2010canonical}
\@writefile{toc}{\contentsline {section}{\numberline {1.2}Contributions of this thesis}{5}}
\citation{kettenring1971canonical}
\citation{nielsen1994analysis}
\citation{scharf1991statistical}
\citation{friedman2001elements}
\citation{besson2005matched}
\citation{besson2006cfar}
\citation{bandiera2007adaptive}
\citation{bandiera2007glrt}
\citation{maris2003resampling}
\citation{soong1995principal}
\citation{besson2005matched}
\citation{besson2006cfar}
\citation{bandiera2007adaptive}
\citation{bandiera2007glrt}
\citation{scharf1994matched}
\citation{vincent2008matched}
\citation{mcwhorter2003matched}
\citation{jin2005cfar}
\citation{scharf1994matched}
\@writefile{toc}{\hbox { }}
\@writefile{toc}{\contentsline {chap}{\numberline {\hbox { }\hfill \bf  II.\hspace  {5pt}}{\bf  Performance of Matched Subspace Detectors Using Finite Training Data}}{8}}
\@writefile{toc}{\hbox { }}
\newlabel{sec:chpt_msd}{{II}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Introduction}{8}}
\newlabel{sec:ieee_msd_intro}{{2.1}{8}}
\citation{elden2007matrix}
\citation{hwritingurl}
\citation{elden2007matrix}
\citation{thai2002invariant}
\citation{healey1999models}
\citation{kwon2006kernel}
\citation{nadakuditi2008sample}
\citation{paul2007asymptotics}
\citation{benaych2011eigenvalues}
\citation{benaych2011singular}
\citation{asendorf2011msd}
\citation{besson2005matched}
\citation{besson2006cfar}
\citation{bandiera2007adaptive}
\citation{bandiera2007glrt}
\citation{maris2003resampling}
\citation{soong1995principal}
\citation{thai2002invariant}
\citation{healey1999models}
\citation{kwon2006kernel}
\citation{healey1999models}
\citation{kwon2006kernel}
\citation{bandiera2007adaptive}
\citation{bandiera2007glrt}
\citation{thai2002invariant}
\citation{healey1999models}
\citation{zhu2006automatic}
\citation{nadakuditi2010fundamental}
\citation{johnstone2001distribution}
\citation{el2007tracy}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Data Models and Parameter Estimation}{11}}
\newlabel{sec:ieee_msd_data_models}{{2.2}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}Training Data Model}{11}}
\newlabel{sec:ieee_msd_training_data}{{2.2.1}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Parameter Estimation}{11}}
\newlabel{sec:ieee_msd_param_estim}{{2.2.2}{11}}
\citation{muirhead1982aspects}
\newlabel{eq:chpt2:param_estims_stoch}{{2.1}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Testing Data Model}{12}}
\newlabel{eq:stoch_setup}{{2.2}{12}}
\newlabel{eq:chpt2:determ_setup}{{2.3}{12}}
\citation{van1968detection}
\citation{scharf1991statistical}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Standard Detector Derivations}{13}}
\newlabel{sec:ieee_msd_std_detecs}{{2.3}{13}}
\newlabel{eq:lrt}{{2.4}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}Stochastic Testing Model}{13}}
\newlabel{sec:ieee_msd_plugin_stoch}{{2.3.1}{13}}
\newlabel{eq:stoch_lrt}{{2.5}{13}}
\newlabel{eq:oracle_stat_stoch_y}{{2.6}{13}}
\newlabel{eq:oracle_stat_stoch_w}{{2.7}{13}}
\citation{mcwhorter2003matched}
\citation{jin2005cfar}
\newlabel{eq:plugin_stat_stoch}{{2.8}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}Deterministic Testing Model}{14}}
\newlabel{sec:ieee_msd_plugin_determ}{{2.3.2}{14}}
\newlabel{eq:determ_stat_oracle_y}{{2.9}{14}}
\citation{scharf1991statistical}
\citation{fawcett2006introduction}
\citation{fawcett2006introduction}
\newlabel{eq:determ_stat_oracle_w}{{2.10}{15}}
\newlabel{eq:glrt_determ}{{2.11}{15}}
\newlabel{eq:plugin_stat_determ}{{2.12}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.3}Effect of the Number of Training Samples}{15}}
\newlabel{sec:training_effect}{{2.3.3}{15}}
\newlabel{fig:stoch_oracle}{{2.1(a)}{16}}
\newlabel{sub@fig:stoch_oracle}{{(a)}{16}}
\newlabel{fig:determ_oracle}{{2.1(b)}{16}}
\newlabel{sub@fig:determ_oracle}{{(b)}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Empirical ROC curves for the plug-in and oracle detectors. Empirical ROC curves were simulated with $n=200$, $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}=k=2$, and $\Sigma =\mathop {\bf  diag}\left (10,0.1\right )$. The empirical ROC curves were computed using $10000$ test samples and averaged over 100 trials using algorithms 2 and 4 of \cite  {fawcett2006introduction}. (a) Shows results for the stochastic MSD. (b) Shows results for the deterministic MSD when $x=[0.75,0.75]^T$. For both settings, as $m$ decreases, the performance of the plug-in detector degrades.\relax }}{16}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Stochastic Setting}}}{16}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Deterministic Setting}}}{16}}
\newlabel{fig:plugin_v_oracle}{{2.1}{16}}
\citation{fawcett2006introduction}
\citation{fawcett2006introduction}
\newlabel{eq:epsilon}{{2.13}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Problem Statements}{17}}
\newlabel{sec:ieee_msd_prob_state}{{2.4}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.1}Problem 1: Derive a New Detector that Exploits Predictions of Subspace Accuracy}{17}}
\newlabel{sec:ieee_msd_ps_prob2}{{2.4.1}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Empirically determined number of training samples, $m$, needed for the stochastic plug-in detector to achieve a desired performance loss, $\epsilon $, as defined in (2.13\hbox {}). The required false alarm rate is $P_F=0.1$. Empirical ROC curves were generated for $n=200$, $\Sigma =\mathop {\bf  diag}(10,0.1)$, $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}=k=2$ using 10000 testing samples and averaged over 100 trials using algorithms 2 and 4 of \cite  {fawcett2006introduction}.\relax }}{18}}
\newlabel{fig:epsilon_graph}{{2.2}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4.2}Problem 2: Characterize ROC Performance Curves}{18}}
\newlabel{sec:ieee_msd_problem_1}{{2.4.2}{18}}
\newlabel{eq:detector_form}{{2.14}{18}}
\citation{paul2007asymptotics}
\citation{benaych2011eigenvalues}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Pertinent Results from Random Matrix Theory}{19}}
\newlabel{sec:ieee_msd_rmt}{{2.5}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Eigenvector Aspects}{19}}
\newlabel{sec:ieee_msd_eigvect_aspects}{{2.5.1}{19}}
\newlabel{th:chpt2:angles}{{2.5.1}{19}}
\newlabel{eq:chpt2:angles}{{2.15}{19}}
\citation{cox1973resolving}
\citation{nadakuditi2008sample}
\citation{nadakuditi2010fundamental}
\citation{nadakuditi2010fundamental}
\citation{nadakuditi2010fundamental}
\citation{johnstone2001distribution}
\citation{el2007tracy}
\newlabel{eq:chpt2:keff}{{2.16}{20}}
\newlabel{th:other angles}{{2.5.1}{20}}
\newlabel{conj:angles}{{2.5.1}{20}}
\citation{paul2007asymptotics}
\citation{benaych2011singular}
\newlabel{corr:matrix}{{2.5.1}{21}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Eigenvalue Aspects}{21}}
\newlabel{th:eigvals_rmt}{{2.5.2}{21}}
\citation{paul2007asymptotics}
\citation{paul2007asymptotics}
\citation{benaych2011singular}
\newlabel{th:eigenvalues}{{2.5.3}{22}}
\newlabel{eq:cov}{{2.17}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {2.6}Derivation of New RMT Matched Subspace Detectors}{22}}
\newlabel{sec:ieee_msd_rmt_detecs}{{2.6}{22}}
\citation{nadakuditi2010fundamental}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.1}Stochastic RMT Detector}{23}}
\newlabel{sec:ieee_msd_rmt_stoch}{{2.6.1}{23}}
\newlabel{eq:stoch_distr}{{2.18}{23}}
\newlabel{eq:cov mat}{{2.19}{23}}
\newlabel{eq:optimal_stat_stoch}{{2.20}{24}}
\@writefile{lot}{\contentsline {table}{\numberline {2.1}{\ignorespaces Summary of the plug-in and RMT stochastic MSDs. See Sections 2.3.1\hbox {} and 2.6.1\hbox {} for derivations.\relax }}{24}}
\newlabel{table:summary_stoch}{{2.1}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6.2}Deterministic RMT Detector}{24}}
\newlabel{sec:ieee_msd_rmt_detec_determ}{{2.6.2}{24}}
\@writefile{lot}{\contentsline {table}{\numberline {2.2}{\ignorespaces Summary of the conditional distributions of the plug-in and RMT stochastic MSDs.\relax }}{25}}
\newlabel{table:summary_stoch2}{{2.2}{25}}
\newlabel{eq:optimal_stat_determ}{{2.21}{25}}
\@writefile{lot}{\contentsline {table}{\numberline {2.3}{\ignorespaces Summary of the plug-in and RMT deterministic MSDs. See Sections 2.3.2\hbox {} and 2.6.2\hbox {} for derivations.\relax }}{26}}
\newlabel{table:summary_determ}{{2.3}{26}}
\@writefile{lot}{\contentsline {table}{\numberline {2.4}{\ignorespaces Summary of the conditional distributions of the plug-in and RMT deterministic MSDs. \relax }}{26}}
\newlabel{table:summary_determ2}{{2.4}{26}}
\@writefile{toc}{\contentsline {section}{\numberline {2.7}Theoretical ROC Curve Predictions}{26}}
\newlabel{sec:ieee_msd_roc_theory}{{2.7}{26}}
\citation{fawcett2006introduction}
\newlabel{eq:chpt2:target_cdf}{{2.22}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.1}Stochastic Testing Setting}{27}}
\newlabel{sec:ieee_msd_roc_stoch}{{2.7.1}{27}}
\newlabel{eq:stoch_stat_distr}{{2.23}{27}}
\citation{wood1993saddlepoint}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7.2}Deterministic Testing Setting}{28}}
\newlabel{sec:ieee_msd_roc_determ}{{2.7.2}{28}}
\newlabel{eq:determ_stat_distr}{{2.24}{28}}
\newlabel{eq:delta}{{2.25}{28}}
\citation{fawcett2006introduction}
\citation{fawcett2006introduction}
\citation{fawcett2006introduction}
\citation{fawcett2006introduction}
\newlabel{eq:roc}{{2.26}{29}}
\@writefile{toc}{\contentsline {section}{\numberline {2.8}Discussion and Insights}{29}}
\newlabel{sec:ieee_msd_results}{{2.8}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.1}Simulation Protocol}{29}}
\newlabel{sec:sim_proto}{{2.8.1}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.2}Convergence and Accuracy of ROC Curve Predictions}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.3}Effect of the Number of Training Samples}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8.4}Effect of $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}$}{32}}
\citation{nadakuditi2010fundamental}
\citation{johnstone2001distribution}
\citation{el2007tracy}
\citation{benaych2011singular}
\@writefile{toc}{\contentsline {section}{\numberline {2.9}Conclusion}{33}}
\newlabel{sec:ieee_msd_conclusion}{{2.9}{33}}
\newlabel{fig:stoch_n_effect1}{{2.3(a)}{35}}
\newlabel{sub@fig:stoch_n_effect1}{{(a)}{35}}
\newlabel{fig:stoch_n_effect4}{{2.3(b)}{35}}
\newlabel{sub@fig:stoch_n_effect4}{{(b)}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Empirical and theoretical ROC curves for the stochastic plug-in detector. Empirical ROC curves were simulated using $10000$ test samples and averaged over 50 trials using algorithms 2 and 4 of \cite  {fawcett2006introduction}. (a) $\Sigma =\mathop {\bf  diag}(10,2)$, $c=1$, $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}=k=2$ so that $k_\text  {eff}=2$. (b) $\Sigma =\mathop {\bf  diag}(10,2,0.5,0.1)$, $c=10$, $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}=k=4$ so that $k_\text  {eff}=1$. Each figure plots empirical ROC curves for $n=50,200,1000$. Theoretical ROC curves were computed as described in Section 2.7\hbox {}. As $n$ increases, the empirical ROC curves approach the theoretically predicted one. However, this convergence is slower for larger $k$ and $c$.\relax }}{35}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$k=2$, $c=1$}}}{35}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$k=4$, $c=10$}}}{35}}
\newlabel{fig:stoch_roc_pred}{{2.4(a)}{36}}
\newlabel{sub@fig:stoch_roc_pred}{{(a)}{36}}
\newlabel{fig:determ_roc_pred}{{2.4(b)}{36}}
\newlabel{sub@fig:determ_roc_pred}{{(b)}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Empirical and theoretical ROC curves for the plug-in and RMT detectors. Empirical ROC curves were simulated using $10000$ test vectors and averaged over 100 trials with $n=1000$, $m=500$, and $\Sigma =\alpha \mathop {\bf  diag}\left (10,5\right )$. The theoretical ROC curves were computed as described in Section 2.7\hbox {}. (a) Stochastic testing setting. Results are plotted for $\alpha =1,0.5,0.25$. For $\alpha =1$ and $\alpha =0.5$, $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}=k=k_\text  {eff}=2$ by (2.16\hbox {}). For $\alpha =0.25$, $k_\text  {eff}=1$.Since $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k} > k_{\text  {eff}}$ when $\alpha =0.25$, we observe a performance gain when using the RMT detector. (b) Deterministic testing setting. Results are plotted for $\alpha =1$ so that $k_\text  {eff}=2$. Three values of the deterministic signal vector were used: $x=[1,1]^T$, $x=[0.5,0.5]^T$, and $x=[0.25,0.25]^T$. The resulting ROC curves depend on the choice of $x$, however, since $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k} = k_{\text  {eff}}$, the plug-in and RMT detector achieve the same performance for all $x$. For both the stochastic and deterministic detectors, the theoretically predicted ROC curves match the empirical ROC curves, reflecting the accuracy of Corollary 2.5.1\hbox {} and the Lugannani-Rice formula.\relax }}{36}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Stochastic}}}{36}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Deterministic}}}{36}}
\newlabel{fig:stoch_m_large}{{2.5(a)}{37}}
\newlabel{sub@fig:stoch_m_large}{{(a)}{37}}
\newlabel{fig:stoch_m_small}{{2.5(b)}{37}}
\newlabel{sub@fig:stoch_m_small}{{(b)}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces Empirical and theoretical ROC curves for the plug-in and RMT stochastic detectors. Empirical ROC curves were computed with 10000 test samples and averaged over 100 trials. Here, $n=5000$, $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}=k=4$ and $\Sigma = \mathop {\bf  diag}({\bf  {10,3,2.5,2}})$. The empirical oracle ROC curve is provided for relative comparison purposes. (a) $m=5000$ so that $c=1$ and $k_\text  {eff}=\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}=4$. The plug-in and RMT detectors achieve relatively the same performance. (b) $m=250$ so that $c=20$ and $k_\text  {eff}=1<\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}=4$. The RMT detector avoids some of the performance loss realized by the plug-in detector. As seen in Section 2.3\hbox {}, limited training samples degrades detector performance. However, the new RMT detector does not suffer as badly as the plug-in detector because it accounts for subspace estimation errors due to finite training data. The disagreement between the theoretical and empirical ROC curves is attributed to finite dimensionality.\relax }}{37}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$m=5000$}}}{37}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$m=250$}}}{37}}
\newlabel{fig:stoch_m_effect}{{2.5}{37}}
\newlabel{fig:stoch_theory_epsilon}{{2.6(a)}{38}}
\newlabel{sub@fig:stoch_theory_epsilon}{{(a)}{38}}
\newlabel{fig:determ_theory_epsilon}{{2.6(b)}{38}}
\newlabel{sub@fig:determ_theory_epsilon}{{(b)}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Theoretically determined number of training samples, $m$, needed to achieve a desired performance loss, $\epsilon $, as defined in (2.13\hbox {}). The required false alarm rate is $P_F=0.1$ with $n=200$, $\Sigma = \mathop {\bf  diag}(10,0.1)$, and $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}=k=2$. (a) Results for the stochastic detectors. We see that for a given $\epsilon $, the new RMT detector requires less training samples. (b) Results for the deterministic detectors when $x=[0.75,0.75]^T$. Again, for a given $\epsilon $, the new RMT detector requires less training samples. In the deterministic setting, the limiting performance loss is different (and non-zero) for the plug-in and RMT detectors. This arises in estimation errors of $x$ in the GLRT.\relax }}{38}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Stochastic}}}{38}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Deterministic}}}{38}}
\newlabel{fig:epsilon_combined}{{2.6}{38}}
\newlabel{fig:stoch_khat}{{\caption@xref {fig:stoch_khat}{ on input line 123}}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Empirical exploration of the achieved probability of detection, $P_D$, for a fixed probability of false alarm, $P_F=0.01$, for various $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}$. Empirical ROC curves were computed using 10000 test samples and averaged over 100 trials with $n=1000$, $m=500$, and $\Sigma = \mathop {\bf  diag}({\bf  {10,5,4}},0.75,0.5,0.25)$ so that $k_{\text  {eff}}=3$. Results for the stochastic detectors. The optimal $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}$ resulting in the largest $P_D$ is not the true $k$, but rather $k_\text  {eff}$.\relax }}{39}}
\newlabel{fig:khat_graphs}{{2.7}{39}}
\citation{cui2013performance}
\citation{arribas2013antenna}
\citation{gorji2013widely}
\citation{zhou2013space}
\@writefile{toc}{\hbox { }}
\@writefile{toc}{\contentsline {chap}{\numberline {\hbox { }\hfill \bf  III.\hspace  {5pt}}{\bf  Extensions of Deterministic Matched Subspace Detectors: Missing Data and Useful Subspace Components}}{40}}
\@writefile{toc}{\hbox { }}
\newlabel{sec:chpt_msd_exten}{{III}{40}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}Introduction}{40}}
\newlabel{sec:intro}{{3.1}{40}}
\newlabel{eq:general_setup}{{3.1}{40}}
\citation{cui2013performance}
\citation{santiago2013noise}
\citation{hu2013doa}
\citation{liao2013direction}
\citation{arribas2013antenna}
\citation{chen2013adaptive}
\citation{gorji2013widely}
\citation{zhou2013space}
\citation{kwon2013multi}
\citation{besson2005matched}
\citation{bandiera2007glrt}
\citation{bandiera2007adaptive}
\citation{sirianunpiboon2013multiple}
\citation{vazquez2011spatial}
\citation{scharf1994matched}
\citation{vincent2008matched}
\citation{besson2006cfar}
\citation{asendorf2013performance}
\citation{asendorf2013performance}
\citation{balzano2010high}
\citation{cui2013performance}
\citation{santiago2013noise}
\citation{arribas2013antenna}
\citation{chen2013adaptive}
\citation{gorji2013widely}
\citation{zhou2013space}
\citation{kwon2013multi}
\citation{hu2013doa}
\citation{liao2013direction}
\citation{van1968detection}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Problem Formulation}{42}}
\newlabel{sec:energy_detector}{{3.2}{42}}
\newlabel{eq:lrt_form}{{3.2}{42}}
\newlabel{eq:energy_detector}{{3.3}{43}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.1}ROC Curve Analysis}{43}}
\newlabel{eq:roc}{{3.4}{43}}
\newlabel{eq:distr_energy}{{3.5}{43}}
\citation{cui2013performance}
\citation{arribas2013antenna}
\citation{zhou2013space}
\newlabel{eq:roc_energy}{{3.6}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2.2}Problem Statement}{44}}
\newlabel{sec:prob_state}{{3.2.2}{44}}
\newlabel{eq:the_detector}{{3.7}{44}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Useful Components In Energy Detectors}{44}}
\newlabel{sec:useful}{{3.3}{44}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Definition and Computation of $k_{\text  {useful}}$}{45}}
\newlabel{eq:opt_prob}{{3.8}{45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Discussion of Test Statistic Distributions}{45}}
\newlabel{eq:d_distrs}{{3.9}{45}}
\newlabel{eq:nc_param}{{3.10}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Algorithm to determine $k_{\text  {useful}}$. This is computable in an oracle setting  where $\delta $ is known.\relax }}{46}}
\newlabel{algo:kuse}{{3.1}{46}}
\newlabel{eq:simple_opt}{{3.11}{46}}
\newlabel{fig:small}{{3.2(a)}{47}}
\newlabel{sub@fig:small}{{(a)}{47}}
\newlabel{fig:med}{{3.2(b)}{47}}
\newlabel{sub@fig:med}{{(b)}{47}}
\newlabel{fig:big}{{3.2(c)}{47}}
\newlabel{sub@fig:big}{{(c)}{47}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Probability density function (p.d.f.) of $\Lambda (w)\tmspace  +\thinmuskip {.1667em}|\tmspace  +\thinmuskip {.1667em}H_0$ and $\Lambda (w)\tmspace  +\thinmuskip {.1667em}|\tmspace  +\thinmuskip {.1667em}H_1$ for three combinations of the number of components $d$ and non-centrality parameter $\lambda _d$. (a) Baseline: $d=1$, $\lambda _d=2$ (b) Increases $d$ but keeps $\lambda _d$ fixed. The distributions are less separable. (c) Increases both $d$ and $\lambda _d$. The distributions are more separable.\relax }}{47}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$d=1$, $\lambda _d=2$}}}{47}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$d=2$, $\lambda _d=2$}}}{47}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$d=2$, $\lambda _d=3$}}}{47}}
\newlabel{fig:distributions}{{3.2}{47}}
\citation{asendorf2013performance}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces The corresponding ROC curves to the three choices of $d$ and $\lambda _d$ in Figure 3.2\hbox {}. ROC curves were generated from (3.4\hbox {}). When adding an additional subspace component, the non-centrality parameter must increase sufficiently in order to achieve improved detection. \relax }}{48}}
\newlabel{fig:dist_roc}{{3.3}{48}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Useful Components in Deterministic Matched Subspace Detectors}{48}}
\newlabel{sec:msd}{{3.4}{48}}
\citation{chen2013adaptive}
\citation{arribas2013antenna}
\citation{he2013near}
\citation{liao2013direction}
\citation{kwon2013multi}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Minimum increase in non-centrality parameter necessary for increased detector performance. Results are shown for multiple choices of $P_F$. $\lambda _1$ indicates the non-centrality parameter when $d=1$ and $\Delta \lambda $ indicates the increase in non-centrality parameter when increasing the number of components to $d=2$.\relax }}{49}}
\newlabel{fig:nc_lines}{{3.4}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Training Data Model}{49}}
\newlabel{sec:training_data}{{3.4.1}{49}}
\newlabel{eq:taes_train}{{3.12}{49}}
\citation{muirhead1982aspects}
\citation{asendorf2013performance}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}Testing Data Model}{50}}
\newlabel{eq:determ_setup}{{3.13}{50}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Subspace Estimation and Accuracy}{50}}
\newlabel{sec:param_estim}{{3.4.3}{50}}
\newlabel{eq:param_estims_stoch}{{3.14}{50}}
\citation{asendorf2013performance}
\citation{nadakuditi2008sample}
\citation{scharf1994matched}
\citation{vincent2008matched}
\citation{fuchs2007robust}
\citation{asendorf2013performance}
\citation{santiago2013noise}
\citation{arribas2013antenna}
\citation{asendorf2013performance}
\newlabel{eq:angles}{{3.15}{51}}
\newlabel{eq:keff}{{3.16}{51}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Plug-in and RMT Detectors}{51}}
\newlabel{eq:plugin_stat}{{3.17}{51}}
\newlabel{eq:rmt_stat}{{3.18}{51}}
\citation{asendorf2013performance}
\newlabel{eq:msd_nc_param}{{3.19}{52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.5}Relationship between $k_{\text  {useful}}$ and $k_\text  {eff}$}{52}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.6}Numerical Example}{53}}
\newlabel{fig:det_perf}{{3.5(a)}{54}}
\newlabel{sub@fig:det_perf}{{(a)}{54}}
\newlabel{fig:dvals}{{3.5(b)}{54}}
\newlabel{sub@fig:dvals}{{(b)}{54}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Deterministic energy detector performance as a function of the number of training samples. In this experiment $n=200$, $\Sigma =\mathop {\bf  diag}(5,2,0.5)$, $x=[1.5,1.5,1.5]^T$, and the required false alarm rate is $P_F=0.1$. (a) The theoretical probability of detection achieved by the plug-in, RMT, and useful detectors. $P_D(P_F)$ is calculated in (3.4\hbox {}). The plug-in detector sets $d=k$, the RMT detector sets $k=k_\text  {eff}$ as defined in (3.16\hbox {}), and the useful detector sets $d=k_{\text  {useful}}$ as calculated in Figure 3.1\hbox {} using the non-centrality parameter defined in (3.19\hbox {}). The useful detector achieves the optimal performance. (b) The number of subspace components used by the plug-in, RMT, and useful detectors. Whenever $k_\text  {eff}\not =k_{\text  {useful}}$, the RMT detector realizes a suboptimal detector performance. Even though these subspace components are \textit  {informative}, there is not enough training data to make them \textit  {useful} in detection.\relax }}{54}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Detector Performance}}}{54}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Number of Subspace Components}}}{54}}
\newlabel{fig:main_result}{{3.5}{54}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Extension - Weighted Energy Detector}{54}}
\newlabel{sec:ext}{{3.5}{54}}
\newlabel{eq:weighted_detector}{{3.20}{54}}
\citation{wood1993saddlepoint}
\citation{fawcett2006introduction}
\newlabel{eq:roc_weighted}{{3.21}{55}}
\newlabel{eq:weighted_pd}{{3.22}{55}}
\newlabel{fig:weights_easy}{{3.6(a)}{56}}
\newlabel{sub@fig:weights_easy}{{(a)}{56}}
\newlabel{fig:weights_hard}{{3.6(b)}{56}}
\newlabel{sub@fig:weights_hard}{{(b)}{56}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Empirically achieved probability of detection ($P_D$) as a function of the weighting coefficient $a$ for a fixed false alarm rate of $P_F=0.1$. (a) Two detectors, one using the deterministic vector $\delta =[1,1]^T$ and the second using $\delta =[1,0]^T$. The first detector achieves its maximum performance around $a=0.5$ indicating that both components are equally informative. The second detector achieves its maximum performance at $a=1$ indicating the second subspace component is not useful in detection. (b) Two detectors, one using $\delta =[1, 0.75]^T$ and the other using $\delta =[1, 0.5]^T$. The maximum performance of each detector is no longer achieved at $a=0.5$ or $a=1$ as the entries of $\delta $ are non-zero and are not equal. The maximum performance is indicated by a black circle.\relax }}{56}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Easy Optimal Weights}}}{56}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Difficult Optimal Weights}}}{56}}
\newlabel{fig:weighted}{{3.6}{56}}
\citation{benaych2011singular}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Deterministic Matched Subspace Detectors with Missing Data}{57}}
\newlabel{sec:chpt3:missing}{{3.6}{57}}
\newlabel{eq:data_model_miss}{{3.23}{57}}
\newlabel{assum:msd_coher}{{3.6.1}{57}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.1}Pertinent Results from RMT}{57}}
\newlabel{sec:rmt}{{3.6.1}{57}}
\newlabel{th:angles}{{3.6.1}{57}}
\citation{balzano2010high}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.2}Plug-in and RMT Detectors}{58}}
\newlabel{se:miss_detects}{{3.6.2}{58}}
\newlabel{eq:plugin missing}{{3.24}{59}}
\newlabel{eq:rmt_missing}{{3.25}{59}}
\newlabel{eq:opt classifier}{{3.26}{59}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6.3}Theoretical ROC Curve Derivation}{59}}
\newlabel{sec:roc_missing}{{3.6.3}{59}}
\newlabel{eq:target cdf}{{3.27}{59}}
\@writefile{toc}{\contentsline {section}{\numberline {3.7}Simulation Results and Discussion}{59}}
\newlabel{sec:results}{{3.7}{59}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.1}ROC Curves}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Empirical and theoretical ROC curves for the plug-in and RMT matched subspace detectors. Empirical ROC curves were simulated with $n=500$, $m=500$, $k=2$, $\Sigma =\mathop {\bf  diag}(3,0.1)$, and $p=0.8$. However, as $\sigma _2$ is below the critical threshold, $k_{\text  {eff}} = 1$. The empirical ROC curves were computed using $5000$ test samples and averaged over 25 trials. $x$ was generated randomly for training samples but fixed for test samples. The theoretical ROC curves were obtained using (3.4\hbox {}). Note the excellent agreement and the performance gain realized by the RMT detector.\relax }}{60}}
\newlabel{fig:roc1}{{3.7}{60}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7.2}Effect of Missing Data}{60}}
\@writefile{toc}{\contentsline {section}{\numberline {3.8}Conclusion}{60}}
\newlabel{sec:conclusion}{{3.8}{60}}
\citation{asendorf2013performance}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Empirically computed probability of detection, $P_D$, for a fixed probability of false alarm, $P_F=0.1$, for various $p$. Here, $n=1000$, $m=1000$, $k=2$, $\Sigma =\mathop {\bf  diag}(3,0.1)$. $P_D$ was computed using (3.4\hbox {}) and $x$ was generated as described in Figure 3.7\hbox {}. For values of $p \leq 1/9$, $k_\text  {eff}=0$ and performance degrades to $P_D = P_F +o(1)$ for both detectors. As $p$ increases, $k_\text  {eff}=1$ allowing the detectors to achieve better than random guessing performance. When $k_\text  {eff}>0$ the plug-in detector is sub-optimal for all values of $p$.\relax }}{61}}
\newlabel{fig:sparsity}{{3.8}{61}}
\citation{hotelling1936relations}
\citation{hardoon2004canonical}
\citation{dhillon2011multi}
\citation{melzer2001nonlinear}
\citation{zhai2015instance}
\citation{lisanti2014matching}
\citation{ahsan2014clustering}
\citation{hardoon2006correlation}
\citation{chaudhuri2009multi}
\citation{deleus2011functional}
\citation{arbabshirani2010comparison}
\citation{khalid2013improving}
\citation{guccione2013functional}
\citation{correa2010canonical}
\citation{lin2013identifying}
\citation{seoane2014canonical}
\citation{lin2013group}
\citation{zhang2013l1}
\citation{nakanishi2014enhancing}
\citation{zhang2014frequency}
\citation{spuler2013spatial}
\citation{campi2013non}
\citation{chen2014removal}
\citation{kuzilek2014comparison}
\citation{todros2012measure}
\citation{wilks2014probabilistic}
\citation{prera2014using}
\citation{steward2014assimilating}
\citation{scharf1998wiener}
\citation{ge2009does}
\citation{manco2014kernel}
\citation{pezeshki2004empirical}
\citation{nadakuditi2011fundamental}
\@writefile{toc}{\hbox { }}
\@writefile{toc}{\contentsline {chap}{\numberline {\hbox { }\hfill \bf  IV.\hspace  {5pt}}{\bf  Using CCA and ICCA to Detect Correlations in Low-Rank Signal-Plus-Noise Datasets}}{63}}
\@writefile{toc}{\hbox { }}
\newlabel{sec:chpt_cca_det}{{IV}{63}}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Introduction}{63}}
\citation{welling2005kcca}
\citation{yu2007learning}
\@writefile{toc}{\contentsline {section}{\numberline {4.2}Data Model}{65}}
\newlabel{sec:chpt4:model}{{4.2}{65}}
\newlabel{eq:chpt4:data_model}{{4.1}{65}}
\newlabel{eq:chpt4:true_scm}{{4.2}{65}}
\newlabel{assum:chpt4:noise}{{4.2.1}{65}}
\@writefile{toc}{\contentsline {section}{\numberline {4.3}Canonical Correlation Analysis}{66}}
\newlabel{sec:chpt4:cca}{{4.3}{66}}
\newlabel{eq:chpt4:opt_cca}{{4.3}{66}}
\newlabel{eq:chpt4:cca}{{4.4}{66}}
\newlabel{eq:chpt4:cca_svd}{{4.5}{66}}
\newlabel{eq:chpt4:c_cca}{{4.6}{66}}
\@writefile{toc}{\contentsline {section}{\numberline {4.4}Empirical CCA for Correlation Detection}{67}}
\newlabel{sec:chpt4:emp_cca}{{4.4}{67}}
\citation{nadakuditi2011fundamental}
\newlabel{eq:chpt4:data_matrices}{{4.7}{68}}
\newlabel{eq:chpt4:rhohatcca}{{4.8}{68}}
\citation{hardle2007applied}
\citation{bartlett1954note}
\newlabel{eq:chpt4:cca_vects}{{4.9}{69}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.1}Classical Wilks Lambda Correlation Test}{69}}
\newlabel{sec:chpt4:wilks}{{4.4.1}{69}}
\newlabel{eq:chpt4:wilks}{{4.10}{69}}
\citation{pezeshki2004empirical}
\citation{nadakuditi2011fundamental}
\citation{nadakuditi2011fundamental}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.2}Informative CCA (ICCA)}{70}}
\newlabel{eq:chpt4:trimmed_svds}{{4.11}{70}}
\citation{constantine1976asymptotic}
\citation{johnstone2008multivariate}
\newlabel{eq:chpt4:rhohaticca}{{4.12}{71}}
\newlabel{eq:chpt4:icca_vects}{{4.13}{71}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4.3}New Statistical Tests for Correlation Detection}{71}}
\newlabel{eq:chpt4:khats}{{4.14}{71}}
\newlabel{eq:chpt4:taus}{{4.15}{71}}
\citation{pezeshki2004empirical}
\@writefile{toc}{\contentsline {section}{\numberline {4.5}Empirical CCA Theory}{72}}
\newlabel{sec:chpt4:cca_theory}{{4.5}{72}}
\newlabel{prop:pezeshki}{{4.5.1}{72}}
\newlabel{eq:chpt4:bao_cca}{{4.16}{72}}
\newlabel{eq:chpt4:rc}{{4.17}{72}}
\citation{bao2014canonical}
\citation{bao2014canonical}
\citation{benaych2012singular}
\citation{benaych2012singular}
\citation{benaych2012singular}
\@writefile{toc}{\contentsline {section}{\numberline {4.6}New Results for CCA and ICCA Consistency}{74}}
\newlabel{sec:chpt4:new_results}{{4.6}{74}}
\newlabel{lem:mip}{{4.6.1}{74}}
\newlabel{th:w_ip}{{4.6.1}{74}}
\citation{benaych2012singular}
\newlabel{eq:chpt4:step1}{{4.18}{75}}
\newlabel{eq:chpt4:w_thm_main}{{4.19}{75}}
\newlabel{corr:w_ip}{{4.6.1}{76}}
\newlabel{eq:chpt4:alphas}{{4.20}{76}}
\newlabel{eq:chpt4:v_orth}{{4.21}{76}}
\citation{nadakuditi2011fundamental}
\citation{nadakuditi2011fundamental}
\newlabel{th:khat_lims}{{4.6.2}{77}}
\citation{benaych2012singular}
\citation{benaych2011eigenvalues}
\citation{paul2007asymptotics}
\citation{nadakuditi2010fundamental}
\citation{benaych2012singular}
\citation{nadakuditi2014optshrink}
\@writefile{toc}{\contentsline {section}{\numberline {4.7}Extension to Missing Data}{81}}
\newlabel{sec:chpt4:missing}{{4.7}{81}}
\newlabel{eq:chpt4:data_model_miss}{{4.22}{81}}
\citation{nadakuditi2014optshrink}
\newlabel{assum:chpt4:coher}{{4.7.1}{82}}
\newlabel{th:missing_data}{{4.7.1}{82}}
\citation{latala2005some}
\newlabel{eq:chpt4:P}{{4.23}{83}}
\newlabel{eq:chpt4:P_2}{{4.24}{83}}
\citation{nadakuditi2014optshrink}
\newlabel{eq:chpt4:ip_missing_orig}{{4.25}{84}}
\citation{nadakuditi2014optshrink}
\citation{nadakuditi2010fundamental}
\newlabel{eq:chpt4:ip_missing}{{4.7}{85}}
\@writefile{toc}{\contentsline {section}{\numberline {4.8}Empirical Results}{85}}
\newlabel{sec:chpt4:emp}{{4.8}{85}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.1}Simulated Data}{85}}
\citation{bao2014canonical}
\citation{bao2014canonical}
\newlabel{fig:chpt4:cca_rho7}{{4.1(a)}{87}}
\newlabel{sub@fig:chpt4:cca_rho7}{{(a)}{87}}
\newlabel{fig:chpt4:cca_rho9}{{4.1(b)}{87}}
\newlabel{sub@fig:chpt4:cca_rho9}{{(b)}{87}}
\newlabel{fig:chpt4:icca_rho7}{{4.1(c)}{87}}
\newlabel{sub@fig:chpt4:icca_rho7}{{(c)}{87}}
\newlabel{fig:chpt4:icca_rho9}{{4.1(d)}{87}}
\newlabel{sub@fig:chpt4:icca_rho9}{{(d)}{87}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces We generate data from (4.1\hbox {}) for $p=q=150$, $k_x=k_y=1$, $k=1$, and various $\rho =P_{xy}$ and sweep over $\theta = \theta ^{(x)}_1=\theta ^{(y)}_1$ and $n$. We compute $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}_x$ and $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}_y$ as outlined in Appendix D for a significance value of $\alpha =0.01$. Using these estimates, we compute $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \rho $}\mathaccent "0362{\rho }_\text  {cca}^{(1)}$ as the largest singular value of $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle C$}\mathaccent "0362{C}_{\text  {cca}}$ as in (4.8\hbox {}) and $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \rho $}\mathaccent "0362{\rho }_\text  {icca}^{(1)}$ as the largest singular value of $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle C$}\mathaccent "0362{C}_{\text  {icca}}$ as in (4.12\hbox {}). We then estimate the number of correlated signals $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}_{\text  {cca}}$ and $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}_{\text  {icca}}$ via (4.14\hbox {}) for a significance level of $\alpha =0.01$. We repeat this for 10000 trials and compute the percentage of trials where $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}_{\text  {cca}}=1$ and $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}_{\text  {icca}}=1$. We plot $\qopname  \relax o{log}_{10}$ of these percentages for multiples values of $\theta $ and $n$. We plot the theoretical consistency boundary of CCA (given in Theorem 4.6.2\hbox {} that relies on \cite  {bao2014canonical}) in a solid white line and the theoretical consistency boundary of ICCA (given in Theorem 4.6.2\hbox {}) in a dashed white line.\relax }}{87}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Empirical CCA $\rho =0.7$}}}{87}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Empirical CCA $\rho =0.9$}}}{87}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {ICCA $\rho =0.7$}}}{87}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {ICCA $\rho =0.9$}}}{87}}
\newlabel{fig:chpt4:cca_pt}{{4.1}{87}}
\newlabel{fig:chpt4:cca_contours}{{4.2(a)}{88}}
\newlabel{sub@fig:chpt4:cca_contours}{{(a)}{88}}
\newlabel{fig:chpt4:icca_contours}{{4.2(b)}{88}}
\newlabel{sub@fig:chpt4:icca_contours}{{(b)}{88}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Contour lines for minimum $1/c$ necessary for reliable detection of $k=1$ correlated component. The quantity $1/c = n/p$ is equivalent to the number of samples per dimension of data. Figure 4.2(a)\hbox {} plots the contours for empirical CCA and Figure 4.2(b)\hbox {} plots the ICCA contours using the limits give in Theorem 4.6.2\hbox {} for $c=c_x=c_y$. We plot the contours for $1/c=10$ to $1/c=3$. These plots clearly demonstrate the ICCA limits are independent of $\rho =P_{xy}$ while CCA is highly dependent on $\rho =P_{xy}$. For a fixed number of samples (fixed $c$), ICCA is reliably detect the presence of a correlated signal at lower SNR values than empirical CCA.\relax }}{88}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Empirical CCA}}}{88}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {ICCA}}}{88}}
\newlabel{fig:chpt4:contours}{{4.2}{88}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.3}{\ignorespaces Theoretical detection regions for ICCA for a rank-1 setting where $c_x=c_y=1$. In this setting, $\theta >1$ implies that the corresponding subspace is informative. Therefore, in light of Theorem 4.6.2\hbox {}, we see that when both $\theta _x<1$ and $\theta _y<1$, neither subspace component is informative and we cannot detect the presence of a correlated signal. This corresponds to the blue region. When only one of $\theta _x$ or $\theta _y$ is above the phase transition (green region), we still cannot detect the presence of a correlated signal even though we have one informative signal. However, when both $\theta _x$ and $\theta _y$ are above the phase transition (yellow region), we can detect the presence of a correlated signal between the datasets. This detection ability is independent of the value of correlation between the datasets. \relax }}{88}}
\newlabel{fig:chpt4:theta_theta_heatmap}{{4.3}{88}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.4}{\ignorespaces We generate data from (4.1\hbox {}) for $p=q=150$, $k_x=k_y=1$, $k=1$, $\theta ^{(x)}_1=\theta ^{(y)}_1=2$, $P_{xy}=1$, and $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}_x=\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}_y=1$. We compute $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \rho $}\mathaccent "0362{\rho }_\text  {cca}^{(1)}$ as the largest singular value of $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle C$}\mathaccent "0362{C}_{\text  {cca}}$ as in (4.8\hbox {}) and $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \rho $}\mathaccent "0362{\rho }_\text  {icca}^{(1)}$ as the largest singular value of $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle C$}\mathaccent "0362{C}_{\text  {icca}}$ as in (4.12\hbox {}). We then estimate the number of correlated signals $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}_{\text  {cca}}$ and $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}_{\text  {icca}}$ using the Wilks test in (4.10\hbox {}) and our new test in (4.14\hbox {}) for a significance level of $\alpha =0.01$. We repeat this for 250 trials and compute the percentage of trials where $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}_{\text  {cca}}=1$ and $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}_{\text  {icca}}=1$ for each significance test. We repeat this for multiple values of $n$ and plot the results. We observe that the classical Wilk's Lambda test is suboptimal and results in a large number of false alarms.\relax }}{89}}
\newlabel{fig:chpt4_wilks}{{4.4}{89}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.2}Comparison to Wilks Lambda Test}{89}}
\citation{nadakuditi2010fundamental}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.3}Simulated Missing Data}{90}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.4}Controlled Flashing Lights Experiment}{91}}
\newlabel{fig:chpt4:cca_rho3_75}{{4.5(a)}{92}}
\newlabel{sub@fig:chpt4:cca_rho3_75}{{(a)}{92}}
\newlabel{fig:chpt4:cca_rho5_75}{{4.5(b)}{92}}
\newlabel{sub@fig:chpt4:cca_rho5_75}{{(b)}{92}}
\newlabel{fig:chpt4:cca_rho7_75}{{4.5(c)}{92}}
\newlabel{sub@fig:chpt4:cca_rho7_75}{{(c)}{92}}
\newlabel{fig:chpt4:cca_rho9_75}{{4.5(d)}{92}}
\newlabel{sub@fig:chpt4:cca_rho9_75}{{(d)}{92}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.5}{\ignorespaces We generate data from (4.22\hbox {}) for $p=q=150$, $k_x=k_y=1$, $k=1$, $n=1200$, and various $\rho =P_{xy}$ and sweep over $\theta = \theta ^{(x)}_1=\theta ^{(y)}_1$ and $\gamma =\gamma _x=\gamma _y$. We compute $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}_x$ and $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}_y$ as outline in Appendix D for a significance value of $\alpha =0.01$. Using these estimates, we compute $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \rho $}\mathaccent "0362{\rho }_\text  {cca}^{(1)}$ as the largest singular value of $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle C$}\mathaccent "0362{C}_{\text  {cca}}$ as in (4.8\hbox {}) and $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \rho $}\mathaccent "0362{\rho }_\text  {icca}^{(1)}$ as the largest singular value of $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle C$}\mathaccent "0362{C}_{\text  {icca}}$ as in (4.12\hbox {}). We then estimate the number of correlated signals $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}_{\text  {cca}}$ and $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}_{\text  {icca}}$ via (4.14\hbox {}) for a significance level of $\alpha =0.01$. We repeat this for 10000 trials and compute the percentage of trials where $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}_{\text  {cca}}=1$ and $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}_{\text  {icca}}=1$. We plot $\qopname  \relax o{log}_{10}$ of these percentages for multiples values of $\theta $ and $n$. We plot the theoretical consistency boundary of empirical CCA (given in Theorem 4.7.1\hbox {}) in a solid white line and the theoretical consistency boundary of ICCA (given in Theorem 4.7.1\hbox {}) in a dashed white line.\relax }}{92}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Empirical CCA $\rho =0.7$}}}{92}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Empirical $\rho =0.9$}}}{92}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {ICCA $\rho =0.7$}}}{92}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {ICCA $\rho =0.9$}}}{92}}
\newlabel{fig:chpt4:cca_missing_75}{{4.5}{92}}
\newlabel{fig:chpt4:flashing_left}{{4.6(a)}{93}}
\newlabel{sub@fig:chpt4:flashing_left}{{(a)}{93}}
\newlabel{fig:chpt4:flashing_right}{{4.6(b)}{93}}
\newlabel{sub@fig:chpt4:flashing_right}{{(b)}{93}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.6}{\ignorespaces Left and right camera views of our experiment with boxes manually identifying each source. Both cameras share a common flashing phone, outlined in a red rectangle. Each camera has two independent sources besides the shared flashing phone.\relax }}{93}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Left Camera}}}{93}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Right Camera}}}{93}}
\newlabel{fig:chpt4:flashing_sources}{{4.6}{93}}
\newlabel{fig:chpt4:lsv}{{4.7(a)}{93}}
\newlabel{sub@fig:chpt4:lsv}{{(a)}{93}}
\newlabel{fig:chpt4:rsv}{{4.7(b)}{93}}
\newlabel{sub@fig:chpt4:rsv}{{(b)}{93}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.7}{\ignorespaces Singular value spectra of $X_{\text  {left}}$ and $Y_{\text  {right}}$ for the flashing light experiment.\relax }}{93}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Left camera}}}{93}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Right camera}}}{93}}
\newlabel{fig:chpt4:flashing_svs}{{4.7}{93}}
\newlabel{fig:chpt4:ul1}{{4.8(a)}{94}}
\newlabel{sub@fig:chpt4:ul1}{{(a)}{94}}
\newlabel{fig:chpt4:ul2}{{4.8(b)}{94}}
\newlabel{sub@fig:chpt4:ul2}{{(b)}{94}}
\newlabel{fig:chpt4:ul3}{{4.8(c)}{94}}
\newlabel{sub@fig:chpt4:ul3}{{(c)}{94}}
\newlabel{fig:chpt4:ul_overlay}{{4.8(d)}{94}}
\newlabel{sub@fig:chpt4:ul_overlay}{{(d)}{94}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.8}{\ignorespaces (a)-(c) Left singular vectors of $X_{\text  {left}}$ corresponding to the top 3 singular values in Figure 4.7(a)\hbox {}. (d) Thresholded singular vectors from (a)-(c) overlayed onto original scene. We use a threshold of $\qopname  \relax o{log}(p)/\sqrt  {p}$ where $p=32400$ pixels. These correspond to the 3 light sources visible in the left camera.The green pixels identify BPL; the magenta pixels identify PH1; the red pixels identify PH2.\relax }}{94}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$u_1$}}}{94}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$u_2$}}}{94}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$u_3$}}}{94}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Overlay}}}{94}}
\newlabel{fig:chpt4:flashing_UL}{{4.8}{94}}
\newlabel{fig:chpt4:ur1}{{4.9(a)}{95}}
\newlabel{sub@fig:chpt4:ur1}{{(a)}{95}}
\newlabel{fig:chpt4:ur2}{{4.9(b)}{95}}
\newlabel{sub@fig:chpt4:ur2}{{(b)}{95}}
\newlabel{fig:chpt4:ur3}{{4.9(c)}{95}}
\newlabel{sub@fig:chpt4:ur3}{{(c)}{95}}
\newlabel{fig:chpt4:ur_overlay}{{4.9(d)}{95}}
\newlabel{sub@fig:chpt4:ur_overlay}{{(d)}{95}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.9}{\ignorespaces (a)-(c) Left singular vectors of $Y_{\text  {right}}$ corresponding to the top 3 singular values in Figure 4.7(b)\hbox {}. (d) Thresholded singular vectors from (a)-(c) overlayed onto original scene. We use a threshold of $\qopname  \relax o{log}(p)/\sqrt  {p}$ where $p=32400$ pixels. These correspond to the 3 light sources visible in the right camera. The dark blue pixels identify PH2; the cyan pixels identify T1; the white pixels identify RPL.\relax }}{95}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$u_1$}}}{95}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$u_2$}}}{95}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$u_3$}}}{95}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Overlay}}}{95}}
\newlabel{fig:chpt4:flashing_UR}{{4.9}{95}}
\newlabel{fig:chpt4:flashing_cca_corrs}{{4.10(a)}{96}}
\newlabel{sub@fig:chpt4:flashing_cca_corrs}{{(a)}{96}}
\newlabel{fig:chpt4:flashing_icca_corrs}{{4.10(b)}{96}}
\newlabel{sub@fig:chpt4:flashing_icca_corrs}{{(b)}{96}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.10}{\ignorespaces (a) Top three singular values returned by empirical CCA as defined in (4.8\hbox {}). As we are in the sample deficient regime, these singular values are deterministically 1. (b) Top three singular values returned by ICCA as defined in (4.12\hbox {}). ICCA correctly identifies two sources of correlation.\relax }}{96}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {CCA}}}{96}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {ICCA}}}{96}}
\newlabel{fig:chpt4:flashing_corrs}{{4.10}{96}}
\newlabel{fig:chpt4:flashing_icca_sig}{{4.11(a)}{97}}
\newlabel{sub@fig:chpt4:flashing_icca_sig}{{(a)}{97}}
\newlabel{fig:chpt4:flashing_cca_sig}{{4.11(b)}{97}}
\newlabel{sub@fig:chpt4:flashing_cca_sig}{{(b)}{97}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.11}{\ignorespaces Significance of the top singular value returned by ICCA in Figure 4.10(b)\hbox {} using (4.14\hbox {}) with $\alpha =0.01$. A value of zero represents that the singular value is not significant. A value of one represents that the singular value is significant. (a) Significance over all 800 frame. (b) Zoomed in to the first 50 frames in (a).\relax }}{97}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {ICCA}}}{97}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {ICCA - first 50 frames}}}{97}}
\newlabel{fig:chpt4:flashing_sigs}{{4.11}{97}}
\newlabel{fig:chpt4:flashing_left_cca}{{4.12(a)}{97}}
\newlabel{sub@fig:chpt4:flashing_left_cca}{{(a)}{97}}
\newlabel{fig:chpt4:flashing_right_cca}{{4.12(b)}{97}}
\newlabel{sub@fig:chpt4:flashing_right_cca}{{(b)}{97}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.12}{\ignorespaces Top 3 threholded empirical CCA canonical vectors overlayed on the original scene after 800 frames as computed in (4.9\hbox {}). The red pixels correspond to the vector with the highest correlation, the green pixels correspond to the vector with the second highest correlation, and the blue pixels correspond to the vector with the third highest correlation. We use a threshold of $\qopname  \relax o{log}(p)/\sqrt  {p}$ where $p=32400$ pixels.\relax }}{97}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Left Camera}}}{97}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Right Camera}}}{97}}
\newlabel{fig:chpt4:flashing_cca}{{4.12}{97}}
\newlabel{fig:chpt4:flashing_left_icca}{{4.13(a)}{98}}
\newlabel{sub@fig:chpt4:flashing_left_icca}{{(a)}{98}}
\newlabel{fig:chpt4:flashing_right_icca}{{4.13(b)}{98}}
\newlabel{sub@fig:chpt4:flashing_right_icca}{{(b)}{98}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.13}{\ignorespaces Top 2 threholded ICCA canonical vectors overlayed on video after 800 frames as computed in (4.13\hbox {}). The red pixels correspond to the vector with the highest correlation and the green pixels correspond to the vector with the second highest correlation. We use a threshold of $\qopname  \relax o{log}(p)/\sqrt  {p}$ where $p=32400$ pixels.\relax }}{98}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Left Camera}}}{98}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Right Camera}}}{98}}
\newlabel{fig:chpt4:flashing_icca}{{4.13}{98}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.5}Controlled Flashing Lights with Missing Data}{98}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.14}{\ignorespaces A portion of the right singular vectors of $X_{\text  {left}}$ (blue) and $Y_{\text  {right}}$ (red) corresponding the flashing police lights in each camera view. Both sources have very similar periods and are approximately in antiphase and therefore are correlated. \relax }}{99}}
\newlabel{fig:chpt4:flashing_v}{{4.14}{99}}
\newlabel{fig:chpt4:flashing_lmiss}{{4.15(a)}{99}}
\newlabel{sub@fig:chpt4:flashing_lmiss}{{(a)}{99}}
\newlabel{fig:chpt4:flashing_rmiss}{{4.15(b)}{99}}
\newlabel{sub@fig:chpt4:flashing_rmiss}{{(b)}{99}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.15}{\ignorespaces Left and right camera views of our five sources in the presence of missing data.\relax }}{99}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Left Camera}}}{99}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Right Camera}}}{99}}
\newlabel{fig:chpt4:flashing_miss}{{4.15}{99}}
\newlabel{fig:chpt4:flashing_left_cca}{{4.16(a)}{100}}
\newlabel{sub@fig:chpt4:flashing_left_cca}{{(a)}{100}}
\newlabel{fig:chpt4:flashing_right_cca}{{4.16(b)}{100}}
\newlabel{sub@fig:chpt4:flashing_right_cca}{{(b)}{100}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.16}{\ignorespaces Top 3 threholded PCA vectors overlayed on each video after 800 frames. This is analogous to Figures 4.8\hbox {} and 4.9\hbox {} but with missing data. Again we use the threshold $\qopname  \relax o{log}(p)/\sqrt  {p}$ for $p=32400$. A different color is used for the every vector. We note that the middle source of the left camera violates the low-coherence assumption in Assumption 4.7.1\hbox {} and so PCA does not detect it.\relax }}{100}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Left Camera}}}{100}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Right Camera}}}{100}}
\newlabel{fig:chpt4:flashing_pca_miss}{{4.16}{100}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.6}Controlled Audio Visual Experiment}{100}}
\newlabel{fig:chpt4:flashing_left_cca}{{4.17(a)}{101}}
\newlabel{sub@fig:chpt4:flashing_left_cca}{{(a)}{101}}
\newlabel{fig:chpt4:flashing_right_cca}{{4.17(b)}{101}}
\newlabel{sub@fig:chpt4:flashing_right_cca}{{(b)}{101}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.17}{\ignorespaces Top 2 threholded empirical CCA canonical vectors overlayed on missing data video as computed in (4.9\hbox {}). Again we use the threshold $\qopname  \relax o{log}(p)/\sqrt  {p}$ for $p=32400$. The red pixels correspond to the vector with the highest correlation and the green pixels correspond to the vector with the second highest correlation in Figure 4.19(a)\hbox {}.\relax }}{101}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Left Camera}}}{101}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Right Camera}}}{101}}
\newlabel{fig:chpt4:flashing_cca_miss}{{4.17}{101}}
\newlabel{fig:chpt4:flashing_left_icca_miss}{{4.18(a)}{101}}
\newlabel{sub@fig:chpt4:flashing_left_icca_miss}{{(a)}{101}}
\newlabel{fig:chpt4:flashing_right_icca_miss}{{4.18(b)}{101}}
\newlabel{sub@fig:chpt4:flashing_right_icca_miss}{{(b)}{101}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.18}{\ignorespaces Top 2 threholded ICCA canonical vectors overlayed on missing data video as computed in (4.13\hbox {}). Again we use the threshold $\qopname  \relax o{log}(p)/\sqrt  {p}$ for $p=32400$. The red pixels correspond to the vector with the highest correlation and the green pixels correspond to the vector with the second highest correlation in Figure 4.19(a)\hbox {}.\relax }}{101}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Left Camera}}}{101}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Right Camera}}}{101}}
\newlabel{fig:chpt4:flashing_icca_miss}{{4.18}{101}}
\newlabel{fig:chpt4:flashing_cca_miss_svs}{{4.19(a)}{102}}
\newlabel{sub@fig:chpt4:flashing_cca_miss_svs}{{(a)}{102}}
\newlabel{fig:chpt4:flashing_icca_miss_svs}{{4.19(b)}{102}}
\newlabel{sub@fig:chpt4:flashing_icca_miss_svs}{{(b)}{102}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.19}{\ignorespaces (a) Top three singular values returned by empirical CCA as defined in (4.8\hbox {}). As we are in the sample deficient regime, these singular values are deterministically 1. (b) Top three singular values returned by ICCA as defined in (4.12\hbox {}). ICCA correctly identifies two sources of correlation. As our data matrices now have missing data, it takes more frames for ICCA to identify the two sources of correlations.\relax }}{102}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {CCA}}}{102}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {ICCA}}}{102}}
\newlabel{fig:chpt4:flashing_corrs_miss}{{4.19}{102}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.20}{\ignorespaces Significance of the top singular value returned by ICCA in Figure 4.19(b)\hbox {} using (4.14\hbox {}) with $\alpha =0.01$. A value of zero represents that the singular value is not significant. A value of one represents that the singular value is significant.\relax }}{102}}
\newlabel{fig:chpt4:flashing_sigs_miss}{{4.20}{102}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.21}{\ignorespaces Still shot of the video. The block M on the left is amplitude modulated at 1 Hz, the same rate as $s_1(t)$ in (4.27\hbox {}), while the block M on the right is amplitude modulate at 2.15 Hz, which is different than all other audio and visual sources.\relax }}{103}}
\newlabel{fig:chpt4:av_scene}{{4.21}{103}}
\newlabel{fig:chpt4:av_raw}{{4.22(a)}{103}}
\newlabel{sub@fig:chpt4:av_raw}{{(a)}{103}}
\newlabel{fig:chpt4:av_man}{{4.22(b)}{103}}
\newlabel{sub@fig:chpt4:av_man}{{(b)}{103}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.22}{\ignorespaces (a) Full spectrogram of the audio signal in (4.26\hbox {}). (b) Zoomed in spectrogram of (a) to see the 3 audio sources at 250 Hz, 400 Hz, and 550 Hz. Each sources is also amplitude modulated at a different frequency as described in (4.27\hbox {}).\relax }}{103}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Full Spectrogram}}}{103}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Zoomed Spectrogram}}}{103}}
\newlabel{fig:chpt4:av_spectrograms}{{4.22}{103}}
\newlabel{eq:chpt4_av_total}{{4.26}{103}}
\newlabel{eq:chpt4:av_signals}{{4.27}{103}}
\@writefile{lot}{\contentsline {table}{\numberline {4.1}{\ignorespaces Summary of the audio and visual sources and their amplitude modulated signals. The audio sources are described in (4.27\hbox {}) and the video sources are shown in Figure 4.21\hbox {}. The 250 Hz pure tone is amplitude modulated at the same frequency as the left block M and is thus correlated with it.\relax }}{104}}
\newlabel{tab:av_descrp}{{4.1}{104}}
\newlabel{fig:chpt4:ul1}{{4.23(a)}{105}}
\newlabel{sub@fig:chpt4:ul1}{{(a)}{105}}
\newlabel{fig:chpt4:ul2}{{4.23(b)}{105}}
\newlabel{sub@fig:chpt4:ul2}{{(b)}{105}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.23}{\ignorespaces Singular value spectra of $X_{\text  {video}}$ and $Y_{\text  {audio}}$.\relax }}{105}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Video}}}{105}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Audio}}}{105}}
\newlabel{fig:chpt4:av_sv}{{4.23}{105}}
\newlabel{fig:chpt4:av_vu1}{{4.24(a)}{105}}
\newlabel{sub@fig:chpt4:av_vu1}{{(a)}{105}}
\newlabel{fig:chpt4:av_vu2}{{4.24(b)}{105}}
\newlabel{sub@fig:chpt4:av_vu2}{{(b)}{105}}
\newlabel{fig:chpt4:av_vu_overlay}{{4.24(c)}{105}}
\newlabel{sub@fig:chpt4:av_vu_overlay}{{(c)}{105}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.24}{\ignorespaces (a)-(b)) Left singular vectors of $X_{\text  {video}}$ corresponding to the top 2 singular values. (c) Thresholded singular vectors from (a)-(b) overlayed onto original scene with pixels from (a) in red and pixels from (b) in green. We use the threshold $\qopname  \relax o{log}(p)/\sqrt  {p}$ for $p=33250$. These vectors correspond to the 2 block M's.\relax }}{105}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$u_1$}}}{105}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$u_2$}}}{105}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$u_3$}}}{105}}
\newlabel{fig:chpt4:av_vu}{{4.24}{105}}
\newlabel{fig:chpt4:av_audio_pca_full}{{4.25(a)}{106}}
\newlabel{sub@fig:chpt4:av_audio_pca_full}{{(a)}{106}}
\newlabel{fig:chpt4:av_audio_pca_zoom}{{4.25(b)}{106}}
\newlabel{sub@fig:chpt4:av_audio_pca_zoom}{{(b)}{106}}
\newlabel{fig:chpt4:av_audio_pca_mask}{{4.25(c)}{106}}
\newlabel{sub@fig:chpt4:av_audio_pca_mask}{{(c)}{106}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.25}{\ignorespaces (a) Left singular vectors of $Y_{\text  {audio}}$ corresponding to the top 3 singular values. (b) Zoomed in version of (a) to see the three audio sources. (c) Masked principle component formed by thresholding the singular vectors with $\sqrt  {\qopname  \relax o{log}(q)/q}$ for $q=1025$.\relax }}{106}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Audio principle components}}}{106}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Zoomed-in of (a)}}}{106}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Example Filter}}}{106}}
\newlabel{fig:chpt4:av_audio_pca}{{4.25}{106}}
\newlabel{fig:chpt4:av_cca_corrs}{{4.26(a)}{107}}
\newlabel{sub@fig:chpt4:av_cca_corrs}{{(a)}{107}}
\newlabel{fig:chpt4:av_icca_corrs}{{4.26(b)}{107}}
\newlabel{sub@fig:chpt4:av_icca_corrs}{{(b)}{107}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.26}{\ignorespaces (a) Top two singular values returned by empirical CCA as defined in (4.8\hbox {}). As we are in the sample deficient regime, these singular values are deterministically 1. (b) Top two singular values returned by ICCA as defined in (4.12\hbox {}). ICCA correctly identifies the one source of correlation present in the audio-video dataset.\relax }}{107}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Empirical CCA}}}{107}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {ICCA}}}{107}}
\newlabel{fig:chpt4:av_corrs}{{4.26}{107}}
\newlabel{fig:chpt4:av_icca_sig}{{4.27(a)}{108}}
\newlabel{sub@fig:chpt4:av_icca_sig}{{(a)}{108}}
\newlabel{fig:chpt4:av_cca_sig}{{4.27(b)}{108}}
\newlabel{sub@fig:chpt4:av_cca_sig}{{(b)}{108}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.27}{\ignorespaces Significance of the top singular value returned by ICCA in Figure 4.26\hbox {} using (4.14\hbox {}) with $\alpha =0.01$. A value of zero represents that the singular value is not significant. A value of one represents that the singular value is significant. (a) Significance for all 540 frames. (b) Zoomed in of (a) to examine the first 75 frames.\relax }}{108}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {ICCA}}}{108}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {ICCA - first 75 frames}}}{108}}
\newlabel{fig:chpt4:av_sigs}{{4.27}{108}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8.7}Controlled Audio Audio Experiment}{108}}
\newlabel{eq:chpt4:aa_sources}{{4.28}{108}}
\newlabel{fig:chpt4:av_audio_cca}{{4.28(a)}{109}}
\newlabel{sub@fig:chpt4:av_audio_cca}{{(a)}{109}}
\newlabel{fig:chpt4:av_audio_cca_zoom}{{4.28(b)}{109}}
\newlabel{sub@fig:chpt4:av_audio_cca_zoom}{{(b)}{109}}
\newlabel{fig:chpt4:av_video_cca}{{4.28(c)}{109}}
\newlabel{sub@fig:chpt4:av_video_cca}{{(c)}{109}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.28}{\ignorespaces Thresholded canonical vectors (as computed in (4.9\hbox {})) corresponding to the top singular value returned by empirical CCA in Figure 4.26(a)\hbox {}. We use a threshold of $\qopname  \relax o{log}(p)/\sqrt  {p}$ for $p=33250$ and $p=1025$ for video and audio vectors, respectively. (a) Spectrogram of the original audio stream filtered using the thresholded empirical CCA top canonical vector and the overlap-save filter method. (b) Zoomed in spectrogram of (a). (c) Red colored pixels represent the pixels that empirical CCA marks as correlated to the audio stream in (a).\relax }}{109}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Audio}}}{109}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Audio - zoomed}}}{109}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Video}}}{109}}
\newlabel{fig:chpt4:av_cca}{{4.28}{109}}
\newlabel{fig:chpt4:av_audio_icca}{{4.29(a)}{110}}
\newlabel{sub@fig:chpt4:av_audio_icca}{{(a)}{110}}
\newlabel{fig:chpt4:av_audio_icca_zoom}{{4.29(b)}{110}}
\newlabel{sub@fig:chpt4:av_audio_icca_zoom}{{(b)}{110}}
\newlabel{fig:chpt4:av_video_icca}{{4.29(c)}{110}}
\newlabel{sub@fig:chpt4:av_video_icca}{{(c)}{110}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.29}{\ignorespaces Thresholded canonical vectors (as computed in (4.13\hbox {})) corresponding to the top singular value returned by ICCA in Figure 4.26(b)\hbox {}. We use a threshold of $\qopname  \relax o{log}(p)/\sqrt  {p}$ for $p=33250$ and $p=1025$ for video and audio vectors, respectively. (a) Spectrogram of the original audio stream filtered using the thresholded ICCA top canonical vector and the overlap-save filter method. (b) Zoomed in spectrogram of (a). (c) Red colored pixels represent the pixels that ICCA marks as correlated to the audio stream in (a).\relax }}{110}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Audio}}}{110}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Audio - zoomed}}}{110}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Video}}}{110}}
\newlabel{fig:chpt4:av_icca}{{4.29}{110}}
\newlabel{eq:chpt4:aa_signals}{{4.29}{111}}
\@writefile{lot}{\contentsline {table}{\numberline {4.2}{\ignorespaces Summary of the audio sources in (4.28\hbox {}) and their components in (4.29\hbox {}). The 250 Hz pure tone in $a_1(t)$ is amplitude modulated at the same frequency as the 300 Hz pure tone in $a_2(t)$ and is thus correlated with it.\relax }}{111}}
\newlabel{tab:aa_descrp}{{4.2}{111}}
\newlabel{fig:chpt4:aa1_full_spec}{{4.30(a)}{112}}
\newlabel{sub@fig:chpt4:aa1_full_spec}{{(a)}{112}}
\newlabel{fig:chpt4:aa1_spec_zoom}{{4.30(b)}{112}}
\newlabel{sub@fig:chpt4:aa1_spec_zoom}{{(b)}{112}}
\newlabel{fig:chpt4:aa2_full_spec}{{4.30(c)}{112}}
\newlabel{sub@fig:chpt4:aa2_full_spec}{{(c)}{112}}
\newlabel{fig:chpt4:aa2_spec_zoom}{{4.30(d)}{112}}
\newlabel{sub@fig:chpt4:aa2_spec_zoom}{{(d)}{112}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.30}{\ignorespaces (a) Full spectrogram of $a_1(t)$ defined in (4.28\hbox {}). (b) Zoomed in spectrogram of $a_1(t)$ to see the 2 sources at 250 Hz and 400 Hz. (c) Full spectrogram of $a_2(t)$ defined in (4.28\hbox {}) (d) Zoomed in spectrogram of $a_2(t)$ to see the 2 sources at 300 Hz and 550 Hz. The 250 Hz signal in $a_1(t)$ is amplitude modulated at the same frequency as the 300 Hz signal in $a_2(t)$.\relax }}{112}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Full Spectrogram of $a_1(t)$}}}{112}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Zoomed Spectrogram of $a_1(t)$}}}{112}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Full Spectrogram of $a_2(t)$}}}{112}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Zoomed Spectrogram of $a_2(t)$}}}{112}}
\newlabel{fig:chpt4:aa_spectrograms}{{4.30}{112}}
\newlabel{fig:chpt4:aa_sv1}{{4.31(a)}{113}}
\newlabel{sub@fig:chpt4:aa_sv1}{{(a)}{113}}
\newlabel{fig:chpt4:aa_sv2}{{4.31(b)}{113}}
\newlabel{sub@fig:chpt4:aa_sv2}{{(b)}{113}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.31}{\ignorespaces Singular value spectra of $X_{a_1}$ and $Y_{a_2}$.\relax }}{113}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Video}}}{113}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Audio}}}{113}}
\newlabel{fig:chpt4:aa_sv}{{4.31}{113}}
\newlabel{fig:chpt4:aa_pca1}{{4.32(a)}{114}}
\newlabel{sub@fig:chpt4:aa_pca1}{{(a)}{114}}
\newlabel{fig:chpt4:aa_pca1_zoom}{{4.32(b)}{114}}
\newlabel{sub@fig:chpt4:aa_pca1_zoom}{{(b)}{114}}
\newlabel{fig:chpt4:aa_pca2}{{4.32(c)}{114}}
\newlabel{sub@fig:chpt4:aa_pca2}{{(c)}{114}}
\newlabel{fig:chpt4:aa_pca2_zoom}{{4.32(d)}{114}}
\newlabel{sub@fig:chpt4:aa_pca2_zoom}{{(d)}{114}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.32}{\ignorespaces (a) Left singular vectors of $X_{a_1}$ corresponding to the top 2 singular values in Figure 4.31(a)\hbox {}. (b) Zoomed in version of (a). (c) Left singular vectors of $Y_{a_2}$ corresponding to the top 2 singular values in Figure 4.31(b)\hbox {}. (d) Zoomed in version of (c).\relax }}{114}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$X_{a_1}$ principle components}}}{114}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$X_{a_1}$ zoomed-in principle components}}}{114}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$Y_{a_2}$ principle components}}}{114}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {$Y_{a_2}$ zoomed-in principle components}}}{114}}
\newlabel{fig:chpt4:aa_pca}{{4.32}{114}}
\newlabel{fig:chpt4:aa_cca_corrs}{{4.33(a)}{115}}
\newlabel{sub@fig:chpt4:aa_cca_corrs}{{(a)}{115}}
\newlabel{fig:chpt4:aa_icca_corrs}{{4.33(b)}{115}}
\newlabel{sub@fig:chpt4:aa_icca_corrs}{{(b)}{115}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.33}{\ignorespaces (a) Top two singular values returned by empirical CCA as defined in (4.8\hbox {}) for the audio-audio experiment. As we are in the sample deficient regime, these singular values are deterministically 1. (b) Top two singular values returned by ICCA as defined in (4.12\hbox {}). ICCA correctly identifies the one source of correlation present in the audio-audio dataset.\relax }}{115}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Empirical CCA}}}{115}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {ICCA}}}{115}}
\newlabel{fig:chpt4:aa_corrs}{{4.33}{115}}
\newlabel{fig:chpt4:aa_icca_sig}{{4.34(a)}{115}}
\newlabel{sub@fig:chpt4:aa_icca_sig}{{(a)}{115}}
\newlabel{fig:chpt4:aa_icca_sig_zoom}{{4.34(b)}{115}}
\newlabel{sub@fig:chpt4:aa_icca_sig_zoom}{{(b)}{115}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.34}{\ignorespaces Significance of the top singular value returned by ICCA in Figure 4.33\hbox {} using (4.14\hbox {}) with $\alpha =0.01$. A value of zero represents that the singular value is not significant. A value of one represents that the singular value is significant. (a) Significance for all 450 frames. (b) Zoomed in of (a) to examine the first 75 frames.\relax }}{115}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {ICCA}}}{115}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {ICCA - first 75 frames}}}{115}}
\newlabel{fig:chpt4:aa_sigs}{{4.34}{115}}
\newlabel{fig:chpt4:aa1_cca_spec}{{4.35(a)}{117}}
\newlabel{sub@fig:chpt4:aa1_cca_spec}{{(a)}{117}}
\newlabel{fig:chpt4:aa1_cca_spec_zoom}{{4.35(b)}{117}}
\newlabel{sub@fig:chpt4:aa1_cca_spec_zoom}{{(b)}{117}}
\newlabel{fig:chpt4:aa2_cca_spec}{{4.35(c)}{117}}
\newlabel{sub@fig:chpt4:aa2_cca_spec}{{(c)}{117}}
\newlabel{fig:chpt4:aa2_cca_spec_zoom}{{4.35(d)}{117}}
\newlabel{sub@fig:chpt4:aa2_cca_spec_zoom}{{(d)}{117}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.35}{\ignorespaces (a) Full spectrogram of $a_1(t)$ (as defined in (4.28\hbox {})) filtered using the thresholded top canonical vector of empirical CCA computed via (4.9\hbox {}). We use a threshold of $\qopname  \relax o{log}(p)/\sqrt  {p}$ for $p=2049$ and use the overlap-save filter method. (b) Zoomed in spectrogram of (a). (c) Full spectrogram of $a_2(t)$ (as defined in (4.28\hbox {})) filtered using the thresholded top canonical vector of empirical CCA computed using (4.9\hbox {}). We use a threshold of $\qopname  \relax o{log}(p)/\sqrt  {p}$ for $p=2049$ and use the overlap-save filter method. (d) Zoomed in spectrogram of (c). Empirical CCA fails to detect the correlated 250 Hz signal in $a_1(t)$ and the 300 Hz signal in $a_2(t)$. Instead, empirical CCA has random bandpass filters across the spectrum.\relax }}{117}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$a_1(t)$ filtered with empirical CCA}}}{117}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Zoomed in of (a)}}}{117}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$a_2(t)$ filtered with empirical CCA}}}{117}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Zoomed in of (c)}}}{117}}
\newlabel{fig:chpt4:aa_cca_spectrograms}{{4.35}{117}}
\newlabel{fig:chpt4:aa1_icca_spec}{{4.36(a)}{118}}
\newlabel{sub@fig:chpt4:aa1_icca_spec}{{(a)}{118}}
\newlabel{fig:chpt4:aa1_icca_spec_zoom}{{4.36(b)}{118}}
\newlabel{sub@fig:chpt4:aa1_icca_spec_zoom}{{(b)}{118}}
\newlabel{fig:chpt4:aa2_icca_spec}{{4.36(c)}{118}}
\newlabel{sub@fig:chpt4:aa2_icca_spec}{{(c)}{118}}
\newlabel{fig:chpt4:aa2_icca_spec_zoom}{{4.36(d)}{118}}
\newlabel{sub@fig:chpt4:aa2_icca_spec_zoom}{{(d)}{118}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.36}{\ignorespaces (a) Full spectrogram of $a_1(t)$ (as defined in (4.28\hbox {})) filtered using the thresholded top canonical vector of ICCA computed using (4.13\hbox {}). We use a threshold of $\qopname  \relax o{log}(p)/\sqrt  {p}$ for $p=2049$ and use the overlap-save filter method. (b) Zoomed in spectrogram of (a). (c) Full spectrogram of $a_2(t)$ (as defined in (4.28\hbox {})) filtered using the thresholded top canonical vector of ICCA computed using (4.13\hbox {}). We use a threshold of $\qopname  \relax o{log}(p)/\sqrt  {p}$ for $p=2049$ and use the overlap-save filter method. (d) Zoomed in spectrogram of (c). ICCA correctly detects the correlated 250 Hz signal in $a_1(t)$ and the 300 Hz signal in $a_2(t)$ without including any spurious frequencies.\relax }}{118}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$a_1(t)$ filtered with ICCA}}}{118}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Zoomed in of (a)}}}{118}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$a_2(t)$ filtered with ICCA}}}{118}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Zoomed in of (c)}}}{118}}
\newlabel{fig:chpt4:aa_icca_spectrograms}{{4.36}{118}}
\@writefile{toc}{\hbox { }}
\@writefile{toc}{\contentsline {chap}{\numberline {\hbox { }\hfill \bf  V.\hspace  {5pt}}{\bf  On Estimating Population Canonical Vectors}}{119}}
\@writefile{toc}{\hbox { }}
\newlabel{sec:chpt_cca_vects}{{V}{119}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Introduction}{119}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Data Model}{120}}
\newlabel{eq:chpt5:data_model}{{5.1}{120}}
\newlabel{eq:chpt5:true_scm}{{5.2}{120}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Canonical Correlation Analysis}{121}}
\newlabel{eq:chpt5:opt_cca}{{5.3}{121}}
\newlabel{eq:chpt5:cca}{{5.4}{121}}
\newlabel{eq:chpt5:cca_svd}{{5.5}{122}}
\newlabel{eq:chpt5:c_cca}{{5.6}{122}}
\newlabel{eq:chpt5:cca_vectors}{{5.7}{122}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Empirical CCA}{122}}
\citation{nadakuditi2011fundamental}
\citation{nadakuditi2011fundamental}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Estimating Population Canonical Vectors}{124}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Population canonical vectors}{125}}
\newlabel{eq:chpt5:pop_cca_vects}{{5.8}{125}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Empirical CCA canonical vector estimates}{125}}
\newlabel{eq:chpt5:emp_cca_vects}{{5.9}{126}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.3}ICCA canonical vectors}{126}}
\newlabel{eq:chpt5:plugin_cca_vects}{{5.10}{126}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.4}ICCA\texttt  {+} }{126}}
\citation{nadakuditi2014optshrink}
\newlabel{eq:chpt5:opt_cca_vects}{{5.11}{127}}
\newlabel{eq:chpt5:can_vec_opt_prob}{{5.12}{127}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Main Results}{127}}
\newlabel{eq:chpt5:can_opt_sol}{{5.13}{128}}
\newlabel{th:icca_opt}{{5.5.1}{128}}
\citation{nadakuditi2014optshrink}
\newlabel{th:vect_opt}{{5.5.2}{129}}
\newlabel{eq:chpt5:Dhat}{{5.14}{129}}
\newlabel{eq:chpt5:Dprimehat}{{5.15}{129}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Algorithm to compute the ICCA\texttt  {+} canonical vectors.\relax }}{130}}
\newlabel{algo:icca}{{5.1}{130}}
\newlabel{corr:icca_vects}{{5.5.1}{131}}
\newlabel{eq:chpt5:cca_vect_acc}{{5.16}{131}}
\newlabel{th:icca_acc}{{5.5.3}{132}}
\citation{bao2014canonical}
\newlabel{corr:cca_vect_acc}{{5.5.2}{133}}
\newlabel{conj:icca_vect}{{5.5.1}{133}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5.1}Extension to missing data}{134}}
\newlabel{eq:chpt5:data_model_miss}{{5.17}{134}}
\newlabel{assum:coher}{{5.5.1}{134}}
\newlabel{eq:chpt5:can_vec_opt_miss_prob}{{5.18}{134}}
\newlabel{th:icca_vect_miss}{{5.5.4}{135}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Orthogonal Canonical Vector Estimates}{135}}
\newlabel{sec:icca_vec:orth}{{5.6}{135}}
\newlabel{eq:chpt5:orth_cca_vects}{{5.19}{135}}
\newlabel{th:orth_acc}{{5.6.1}{136}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Empirical Results - Synthetic Data}{136}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.1}Performance on non identity $U_{\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle K$}\mathaccent "0365{K}}$}{137}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.2}Convergence}{137}}
\newlabel{fig:chpt5:non_ident1}{{5.2(a)}{138}}
\newlabel{sub@fig:chpt5:non_ident1}{{(a)}{138}}
\newlabel{fig:chpt5:non_ident2}{{5.2(b)}{138}}
\newlabel{sub@fig:chpt5:non_ident2}{{(b)}{138}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Accuracy plots as a function of $n$ for a rank-2 setting where $k_x=k_y=2$, $p=200$, $q=250$, $\Theta _x=\Theta _y=\mathop {\bf  diag}(16,1)$, $P_{xy}=\mathop {\bf  diag}(0.9,0.9)$, $V_K=I_2$, and non-identity $U_K$. Accuracy is defined in (5.16\hbox {}). The left figure plots the accuracy of the first canonical vector and the right figure plots the accuracy of the second canonical vector.\relax }}{138}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$w_x^{(1)}$}}}{138}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$w_x^{(2)}$}}}{138}}
\newlabel{fig:chpt5:non_ident_uktil}{{5.2}{138}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7.3}Robustness to $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}_x$}{138}}
\newlabel{fig:chpt5:convg_p1}{{5.3(a)}{139}}
\newlabel{sub@fig:chpt5:convg_p1}{{(a)}{139}}
\newlabel{fig:chpt5:convg_p2}{{5.3(b)}{139}}
\newlabel{sub@fig:chpt5:convg_p2}{{(b)}{139}}
\newlabel{fig:chpt5:convg_p3}{{5.3(c)}{139}}
\newlabel{sub@fig:chpt5:convg_p3}{{(c)}{139}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Convergence plots of the first canonical vector of each estimate for three values of $p$. Results are plotted for three fixed values of $c_x=0.5,1,2$. The simulation setting is the same as Figure 5.2\hbox {} for a rank-2 setting where $k_x=k_y=2$, $p=200$, $q=250$, $\Theta _x=\Theta _y=\mathop {\bf  diag}(16,1)$, $P_{xy}=\mathop {\bf  diag}(0.9,0.9)$, $V_K=I_2$, and non-identity $U_K$. Errorbars are 1 standard deviation.\relax }}{139}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$p=100$}}}{139}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$p=500$}}}{139}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$p=1000$}}}{139}}
\newlabel{fig:chpt5:icca_convg_w1}{{5.3}{139}}
\newlabel{fig:chpt5:convg_plugin1}{{5.4(a)}{140}}
\newlabel{sub@fig:chpt5:convg_plugin1}{{(a)}{140}}
\newlabel{fig:chpt5:convg_plugin2}{{5.4(b)}{140}}
\newlabel{sub@fig:chpt5:convg_plugin2}{{(b)}{140}}
\newlabel{fig:chpt5:convg_opt1}{{5.4(c)}{140}}
\newlabel{sub@fig:chpt5:convg_opt1}{{(c)}{140}}
\newlabel{fig:chpt5:convg_opt2}{{5.4(d)}{140}}
\newlabel{sub@fig:chpt5:convg_opt2}{{(d)}{140}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Accuracy convergence plots for the top two canonical vectors of the ICCA and ICCA\texttt  {+} estimates. Results are plotted for three fixed values of $c_x=0.5,1,2$ for three different values of $p$. The simulation setting is the same as Figure 5.3\hbox {} for a rank-2 setting where $k_x=k_y=2$, $p=200$, $q=250$, $\Theta _x=\Theta _y=\mathop {\bf  diag}(16,1)$, $P_{xy}=\mathop {\bf  diag}(0.9,0.9)$, $V_K=I_2$, and non-identity $U_K$. Errorbars are 1 standard deviation.\relax }}{140}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {ICCA $w_x^{(1)}$}}}{140}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {ICCA $w_x^{(2)}$}}}{140}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {ICCA\texttt {+} $w_x^{(1)}$}}}{140}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {ICCA\texttt {+} $w_x^{(2)}$}}}{140}}
\newlabel{fig:chpt5:icca_vect_convg1}{{5.4}{140}}
\newlabel{fig:chpt5:convg_orth1}{{5.5(a)}{141}}
\newlabel{sub@fig:chpt5:convg_orth1}{{(a)}{141}}
\newlabel{fig:chpt5:convg_orth2}{{5.5(b)}{141}}
\newlabel{sub@fig:chpt5:convg_orth2}{{(b)}{141}}
\newlabel{fig:chpt5:convg_cca1}{{5.5(c)}{141}}
\newlabel{sub@fig:chpt5:convg_cca1}{{(c)}{141}}
\newlabel{fig:chpt5:convg_cca2}{{5.5(d)}{141}}
\newlabel{sub@fig:chpt5:convg_cca2}{{(d)}{141}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.5}{\ignorespaces Accuracy convergence plots for the top two canonical vectors of the orthogonal and empirical CCA estimates. Results are plotted for three fixed values of $c_x=0.5,1,2$ for three different values of $p$. The simulation setting is the same as Figure 5.3\hbox {} for a rank-2 setting where $k_x=k_y=2$, $p=200$, $q=250$, $\Theta _x=\Theta _y=\mathop {\bf  diag}(16,1)$, $P_{xy}=\mathop {\bf  diag}(0.9,0.9)$, $V_K=I_2$, and non-identity $U_K$. Errorbars are 1 standard deviation.\relax }}{141}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Orthogonal $w_x^{(1)}$}}}{141}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Orthogonal $w_x^{(2)}$}}}{141}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Empirical CCA $w_x^{(1)}$}}}{141}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Empirical CCA $w_x^{(2)}$}}}{141}}
\newlabel{fig:chpt5:icca_vect_convg2}{{5.5}{141}}
\newlabel{fig:chpt5:khat11}{{5.6(a)}{142}}
\newlabel{sub@fig:chpt5:khat11}{{(a)}{142}}
\newlabel{fig:chpt5:khat13}{{5.6(b)}{142}}
\newlabel{sub@fig:chpt5:khat13}{{(b)}{142}}
\newlabel{fig:chpt5:khat13}{{5.6(c)}{142}}
\newlabel{sub@fig:chpt5:khat13}{{(c)}{142}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.6}{\ignorespaces Accuracy plots of the first two canonical vector estimates a function of $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}_x$ for $c=0.2$. The simulation setting is the same as Figure 5.3\hbox {} for a rank-2 setting where $k_x=k_y=2$, $p=200$, $q=250$, $\Theta _x=\Theta _y=\mathop {\bf  diag}(16,1)$, $P_{xy}=\mathop {\bf  diag}(0.9,0.9)$, $V_K=I_2$, and non-identity $U_K$. Errorbars are 1 standard deviation.\relax }}{142}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$w_x^{(1)}$}}}{142}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$w_x^{(2)}$}}}{142}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$w_x^{(3)}$}}}{142}}
\newlabel{fig:chpt5:khat_c1}{{5.6}{142}}
\newlabel{fig:chpt5:khat21}{{5.7(a)}{143}}
\newlabel{sub@fig:chpt5:khat21}{{(a)}{143}}
\newlabel{fig:chpt5:khat22}{{5.7(b)}{143}}
\newlabel{sub@fig:chpt5:khat22}{{(b)}{143}}
\newlabel{fig:chpt5:khat23}{{5.7(c)}{143}}
\newlabel{sub@fig:chpt5:khat23}{{(c)}{143}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.7}{\ignorespaces Accuracy plots of the first two canonical vector estimates a function of $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle k$}\mathaccent "0362{k}_x$ for $c=1$. The simulation setting is the same as Figure 5.3\hbox {} for a rank-2 setting where $k_x=k_y=2$, $p=200$, $q=250$, $\Theta _x=\Theta _y=\mathop {\bf  diag}(16,1)$, $P_{xy}=\mathop {\bf  diag}(0.9,0.9)$, $V_K=I_2$, and non-identity $U_K$. Errorbars are 1 standard deviation.\relax }}{143}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$w_x^{(1)}$}}}{143}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$w_x^{(2)}$}}}{143}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$w_x^{(3)}$}}}{143}}
\newlabel{fig:chpt5:khat_c2}{{5.7}{143}}
\@writefile{toc}{\contentsline {section}{\numberline {5.8}Empirical Results - Real-World Data}{144}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8.1}Video-Video Experiment}{144}}
\newlabel{fig:chpt5:flashing_left}{{5.8(a)}{145}}
\newlabel{sub@fig:chpt5:flashing_left}{{(a)}{145}}
\newlabel{fig:chpt5:flashing_right}{{5.8(b)}{145}}
\newlabel{sub@fig:chpt5:flashing_right}{{(b)}{145}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.8}{\ignorespaces Manual source identification of each camera. Both cameras share a common flashing phone, outlined in a red rectangle. Each camera has two independent sources besides the shared flashing phone.\relax }}{145}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Left Camera}}}{145}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Right Camera}}}{145}}
\newlabel{fig:chpt5:flashing_sources}{{5.8}{145}}
\newlabel{fig:chpt5:flashing2_1_plugin}{{5.9(a)}{147}}
\newlabel{sub@fig:chpt5:flashing2_1_plugin}{{(a)}{147}}
\newlabel{fig:chpt5:flashing2_1_orth}{{5.9(b)}{147}}
\newlabel{sub@fig:chpt5:flashing2_1_orth}{{(b)}{147}}
\newlabel{fig:chpt5:flashing2_1_opt}{{5.9(c)}{147}}
\newlabel{sub@fig:chpt5:flashing2_1_opt}{{(c)}{147}}
\newlabel{fig:chpt5:flashing2_1_cca}{{5.9(d)}{147}}
\newlabel{sub@fig:chpt5:flashing2_1_cca}{{(d)}{147}}
\newlabel{fig:chpt5:flashing2_1_diff1}{{5.9(e)}{147}}
\newlabel{sub@fig:chpt5:flashing2_1_diff1}{{(e)}{147}}
\newlabel{fig:chpt5:flashing2_1_diff2}{{5.9(f)}{147}}
\newlabel{sub@fig:chpt5:flashing2_1_diff2}{{(f)}{147}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.9}{\ignorespaces First canonical vector estimates for the left camera at frame 5. This corresponds to a total capture time of 1/6 of a second. (a)-(d) show the absolute value of the vectors displayed in an image so that large values indicate correlated pixels. (e)-(f) plot the difference between the ICCA estimate and the ICCA\texttt  {+} estimate and the orthogonal estimate and the ICCA\texttt  {+} estimate. Positive values indicate pixels that the ICCA\texttt  {+} estimate thinks are less correlated while negative values indicate pixels that the ICCA\texttt  {+} estimate thinks are more correlated. \relax }}{147}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {ICCA}}}{147}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Orthogonal}}}{147}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {ICCA\texttt {+} }}}{147}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {CCA}}}{147}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {ICCA minus ICCA\texttt {+} }}}{147}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {Orthogonal minus ICCA\texttt {+} }}}{147}}
\newlabel{fig:chpt5:flashing2_1}{{5.9}{147}}
\newlabel{fig:chpt5:flashing3_1_plugin}{{5.10(a)}{148}}
\newlabel{sub@fig:chpt5:flashing3_1_plugin}{{(a)}{148}}
\newlabel{fig:chpt5:flashing3_1_orth}{{5.10(b)}{148}}
\newlabel{sub@fig:chpt5:flashing3_1_orth}{{(b)}{148}}
\newlabel{fig:chpt5:flashing3_1_opt}{{5.10(c)}{148}}
\newlabel{sub@fig:chpt5:flashing3_1_opt}{{(c)}{148}}
\newlabel{fig:chpt5:flashing3_1_cca}{{5.10(d)}{148}}
\newlabel{sub@fig:chpt5:flashing3_1_cca}{{(d)}{148}}
\newlabel{fig:chpt5:flashing3_1_diff1}{{5.10(e)}{148}}
\newlabel{sub@fig:chpt5:flashing3_1_diff1}{{(e)}{148}}
\newlabel{fig:chpt5:flashing3_1_diff2}{{5.10(f)}{148}}
\newlabel{sub@fig:chpt5:flashing3_1_diff2}{{(f)}{148}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.10}{\ignorespaces First canonical vector estimates for the left camera at frame 30. This corresponds to a total capture time of 1 second. (a)-(d) show the absolute value of the vectors displayed in an image so that large values indicate correlated pixels. (e)-(f) plot the difference between the ICCA estimate and the ICCA\texttt  {+} estimate and the orthogonal estimate and the ICCA\texttt  {+} estimate. Positive values indicate pixels that the ICCA\texttt  {+} estimate thinks are less correlated while negative values indicate pixels that the ICCA\texttt  {+} estimate thinks are more correlated. \relax }}{148}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {ICCA}}}{148}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Orthogonal}}}{148}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {ICCA\texttt {+} }}}{148}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {CCA}}}{148}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {ICCA minus ICCA\texttt {+} }}}{148}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {Orthogonal minus ICCA\texttt {+} }}}{148}}
\newlabel{fig:chpt5:flashing3_1}{{5.10}{148}}
\newlabel{fig:chpt5:flashing1_1_plugin}{{5.11(a)}{149}}
\newlabel{sub@fig:chpt5:flashing1_1_plugin}{{(a)}{149}}
\newlabel{fig:chpt5:flashing1_1_orth}{{5.11(b)}{149}}
\newlabel{sub@fig:chpt5:flashing1_1_orth}{{(b)}{149}}
\newlabel{fig:chpt5:flashing1_1_opt}{{5.11(c)}{149}}
\newlabel{sub@fig:chpt5:flashing1_1_opt}{{(c)}{149}}
\newlabel{fig:chpt5:flashing1_1_cca}{{5.11(d)}{149}}
\newlabel{sub@fig:chpt5:flashing1_1_cca}{{(d)}{149}}
\newlabel{fig:chpt5:flashing1_1_diff1}{{5.11(e)}{149}}
\newlabel{sub@fig:chpt5:flashing1_1_diff1}{{(e)}{149}}
\newlabel{fig:chpt5:flashing1_1_diff2}{{5.11(f)}{149}}
\newlabel{sub@fig:chpt5:flashing1_1_diff2}{{(f)}{149}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.11}{\ignorespaces First canonical vector estimates for the left camera at frame 600. This corresponds to a total capture time of 20 seconds. (a)-(d) show the absolute value of the vectors displayed in an image so that large values indicate correlated pixels. (e)-(f) plot the difference between the ICCA estimate and the ICCA\texttt  {+} estimate and the orthogonal estimate and the ICCA\texttt  {+} estimate. Positive values indicate pixels that the ICCA\texttt  {+} estimate thinks are less correlated while negative values indicate pixels that the ICCA\texttt  {+} estimate thinks are more correlated. \relax }}{149}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {ICCA}}}{149}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Orthogonal}}}{149}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {ICCA\texttt {+} }}}{149}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {CCA}}}{149}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {ICCA minus ICCA\texttt {+} }}}{149}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {Orthogonal minus ICCA\texttt {+} }}}{149}}
\newlabel{fig:chpt5:flashing1_1}{{5.11}{149}}
\newlabel{fig:chpt5:flashing2_2_plugin}{{5.12(a)}{150}}
\newlabel{sub@fig:chpt5:flashing2_2_plugin}{{(a)}{150}}
\newlabel{fig:chpt5:flashing2_2_orth}{{5.12(b)}{150}}
\newlabel{sub@fig:chpt5:flashing2_2_orth}{{(b)}{150}}
\newlabel{fig:chpt5:flashing2_2_opt}{{5.12(c)}{150}}
\newlabel{sub@fig:chpt5:flashing2_2_opt}{{(c)}{150}}
\newlabel{fig:chpt5:flashing2_2_cca}{{5.12(d)}{150}}
\newlabel{sub@fig:chpt5:flashing2_2_cca}{{(d)}{150}}
\newlabel{fig:chpt5:flashing2_2_diff1}{{5.12(e)}{150}}
\newlabel{sub@fig:chpt5:flashing2_2_diff1}{{(e)}{150}}
\newlabel{fig:chpt5:flashing2_2_diff2}{{5.12(f)}{150}}
\newlabel{sub@fig:chpt5:flashing2_2_diff2}{{(f)}{150}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.12}{\ignorespaces Second canonical vector estimates for the left camera at frame 5. This corresponds to a total capture time of 1/6 of a second. (a)-(d) show the absolute value of the vectors displayed in an image so that large values indicate correlated pixels. (e)-(f) plot the difference between the ICCA estimate and the ICCA\texttt  {+} estimate and the orthogonal estimate and the ICCA\texttt  {+} estimate. Positive values indicate pixels that the ICCA\texttt  {+} estimate thinks are less correlated while negative values indicate pixels that the ICCA\texttt  {+} estimate thinks are more correlated. \relax }}{150}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {ICCA}}}{150}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Orthogonal}}}{150}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {ICCA\texttt {+} }}}{150}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {CCA}}}{150}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {ICCA minus ICCA\texttt {+} }}}{150}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {Orthogonal minus ICCA\texttt {+} }}}{150}}
\newlabel{fig:chpt5:flashing2_2}{{5.12}{150}}
\newlabel{fig:chpt5:flashing3_2_plugin}{{5.13(a)}{151}}
\newlabel{sub@fig:chpt5:flashing3_2_plugin}{{(a)}{151}}
\newlabel{fig:chpt5:flashing3_2_orth}{{5.13(b)}{151}}
\newlabel{sub@fig:chpt5:flashing3_2_orth}{{(b)}{151}}
\newlabel{fig:chpt5:flashing3_2_opt}{{5.13(c)}{151}}
\newlabel{sub@fig:chpt5:flashing3_2_opt}{{(c)}{151}}
\newlabel{fig:chpt5:flashing3_2_cca}{{5.13(d)}{151}}
\newlabel{sub@fig:chpt5:flashing3_2_cca}{{(d)}{151}}
\newlabel{fig:chpt5:flashing3_2_diff1}{{5.13(e)}{151}}
\newlabel{sub@fig:chpt5:flashing3_2_diff1}{{(e)}{151}}
\newlabel{fig:chpt5:flashing3_2_diff2}{{5.13(f)}{151}}
\newlabel{sub@fig:chpt5:flashing3_2_diff2}{{(f)}{151}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.13}{\ignorespaces Second canonical vector estimates for the left camera at frame 30. This corresponds to a total capture time of 1 second. (a)-(d) show the absolute value of the vectors displayed in an image so that large values indicate correlated pixels. (e)-(f) plot the difference between the ICCA estimate and the ICCA\texttt  {+} estimate and the orthogonal estimate and the ICCA\texttt  {+} estimate. Positive values indicate pixels that the ICCA\texttt  {+} estimate thinks are less correlated while negative values indicate pixels that the ICCA\texttt  {+} estimate thinks are more correlated. \relax }}{151}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {ICCA}}}{151}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Orthogonal}}}{151}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {ICCA\texttt {+} }}}{151}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {CCA}}}{151}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {ICCA minus ICCA\texttt {+} }}}{151}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {Orthogonal minus ICCA\texttt {+} }}}{151}}
\newlabel{fig:chpt5:flashing3_2}{{5.13}{151}}
\newlabel{fig:chpt5:flashing1_2_plugin}{{5.14(a)}{152}}
\newlabel{sub@fig:chpt5:flashing1_2_plugin}{{(a)}{152}}
\newlabel{fig:chpt5:flashing1_2_orth}{{5.14(b)}{152}}
\newlabel{sub@fig:chpt5:flashing1_2_orth}{{(b)}{152}}
\newlabel{fig:chpt5:flashing1_2_opt}{{5.14(c)}{152}}
\newlabel{sub@fig:chpt5:flashing1_2_opt}{{(c)}{152}}
\newlabel{fig:chpt5:flashing1_2_cca}{{5.14(d)}{152}}
\newlabel{sub@fig:chpt5:flashing1_2_cca}{{(d)}{152}}
\newlabel{fig:chpt5:flashing1_2_diff1}{{5.14(e)}{152}}
\newlabel{sub@fig:chpt5:flashing1_2_diff1}{{(e)}{152}}
\newlabel{fig:chpt5:flashing1_2_diff2}{{5.14(f)}{152}}
\newlabel{sub@fig:chpt5:flashing1_2_diff2}{{(f)}{152}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.14}{\ignorespaces Second canonical vector estimates for the left camera at frame 600. This corresponds to a total capture time of 20 seconds. (a)-(d) show the absolute value of the vectors displayed in an image so that large values indicate correlated pixels. (e)-(f) plot the difference between the ICCA estimate and the ICCA\texttt  {+} estimate and the orthogonal estimate and the ICCA\texttt  {+} estimate. Positive values indicate pixels that the ICCA\texttt  {+} estimate thinks are less correlated while negative values indicate pixels that the ICCA\texttt  {+} estimate thinks are more correlated. \relax }}{152}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {ICCA}}}{152}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Orthogonal}}}{152}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {ICCA\texttt {+} }}}{152}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {CCA}}}{152}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {ICCA minus ICCA\texttt {+} }}}{152}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {Orthogonal minus ICCA\texttt {+} }}}{152}}
\newlabel{fig:chpt5:flashing1_2}{{5.14}{152}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8.2}Audio-Audio Experiment}{153}}
\newlabel{fig:chpt4:aa1_full_spec}{{5.15(a)}{154}}
\newlabel{sub@fig:chpt4:aa1_full_spec}{{(a)}{154}}
\newlabel{fig:chpt4:aa1_spec_zoom}{{5.15(b)}{154}}
\newlabel{sub@fig:chpt4:aa1_spec_zoom}{{(b)}{154}}
\newlabel{fig:chpt4:aa2_full_spec}{{5.15(c)}{154}}
\newlabel{sub@fig:chpt4:aa2_full_spec}{{(c)}{154}}
\newlabel{fig:chpt4:aa2_spec_zoom}{{5.15(d)}{154}}
\newlabel{sub@fig:chpt4:aa2_spec_zoom}{{(d)}{154}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.15}{\ignorespaces (a) Full spectrogram of $a_1(t)$. (b) Zoomed in spectrogram of $a_1(t)$ to see the 2 sources at 250 Hz and 400 Hz. (c) Full spectrogram of $a_2(t)$ (d) Zoomed in spectrogram of $a_2(t)$ to see the 2 sources at 300 Hz and 550 Hz. The 250 Hz signal in $a_1(t)$ is amplitude modulated at the same frequency as the 300 Hz signal in $a_2(t)$.\relax }}{154}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Full Spectrogram of $a_1(t)$}}}{154}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Zoomed Spectrogram of $a_1(t)$}}}{154}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Full Spectrogram of $a_2(t)$}}}{154}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Zoomed Spectrogram of $a_2(t)$}}}{154}}
\newlabel{fig:chpt5:aa_spectrograms}{{5.15}{154}}
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Summary of the audio sources. The 250 Hz pure tone in Audio 1 is amplitude modulated at the same frequency as the 300 Hz pure tone in Audio 2 and is thus correlated with it.\relax }}{155}}
\newlabel{tab:chpt5:aa_descrp}{{5.1}{155}}
\@writefile{toc}{\contentsline {section}{\numberline {5.9}Proof of Theorem 5.5.1\hbox {}, Theorem 5.5.2\hbox {}, Corollary 5.5.1\hbox {}, and Theorem 5.5.4\hbox {}}{155}}
\newlabel{sec:chpt5:proofs1}{{5.9}{155}}
\newlabel{fig:chpt5:aa1_1}{{5.16(a)}{156}}
\newlabel{sub@fig:chpt5:aa1_1}{{(a)}{156}}
\newlabel{fig:chpt5:aa1_2}{{5.16(b)}{156}}
\newlabel{sub@fig:chpt5:aa1_2}{{(b)}{156}}
\newlabel{fig:chpt5:aa1_3}{{5.16(c)}{156}}
\newlabel{sub@fig:chpt5:aa1_3}{{(c)}{156}}
\newlabel{fig:chpt5:aa1_3_zoom}{{5.16(d)}{156}}
\newlabel{sub@fig:chpt5:aa1_3_zoom}{{(d)}{156}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.16}{\ignorespaces Canonical vectors estimates for the first audio stream at 3 different frames. One frame corresponds to 1/15 seconds. Frequencies with large weights are those that the algorithms mark as correlated with frequencies with large weights in Figure 5.17\hbox {}.\relax }}{156}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Frame 5}}}{156}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Frame 15}}}{156}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Frame 300}}}{156}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Frame 300 - Zoomed}}}{156}}
\newlabel{fig:chpt5:aa1}{{5.16}{156}}
\newlabel{fig:chpt5:aa2_1}{{5.17(a)}{157}}
\newlabel{sub@fig:chpt5:aa2_1}{{(a)}{157}}
\newlabel{fig:chpt5:aa2_2}{{5.17(b)}{157}}
\newlabel{sub@fig:chpt5:aa2_2}{{(b)}{157}}
\newlabel{fig:chpt5:aa2_3}{{5.17(c)}{157}}
\newlabel{sub@fig:chpt5:aa2_3}{{(c)}{157}}
\newlabel{fig:chpt5:aa2_3_zoom}{{5.17(d)}{157}}
\newlabel{sub@fig:chpt5:aa2_3_zoom}{{(d)}{157}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.17}{\ignorespaces Canonical vectors estimates for the second audio stream at 3 different frames. One frame corresponds to 1/15 seconds. Frequencies with large weights are those that the algorithms mark as correlated with frequencies with large weights in Figure 5.16\hbox {}.\relax }}{157}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Frame 5}}}{157}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Frame 15}}}{157}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Frame 300}}}{157}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Frame 300 - Zoomed}}}{157}}
\newlabel{fig:chpt5:aa2}{{5.17}{157}}
\citation{nadakuditi2014optshrink}
\citation{benaych2012singular}
\citation{benaych2012singular}
\newlabel{eq:chpt5:sig_def}{{5.20}{158}}
\citation{paul2007asymptotics}
\citation{asendorf2013performance}
\newlabel{eq:chpt5:svd_acc_vect}{{5.21}{159}}
\newlabel{eq:chpt5:svd_acc_sv}{{5.22}{159}}
\citation{benaych2012singular}
\@writefile{toc}{\contentsline {section}{\numberline {5.10}Proof of Theorem 5.5.3\hbox {}, Corollary 5.5.2\hbox {}, and Theorem 5.6.1\hbox {}}{160}}
\newlabel{sec:chpt5:proofs2}{{5.10}{160}}
\newlabel{eq:chpt5:alpha_D}{{5.23}{161}}
\citation{hardoon2006correlation}
\citation{hardoon2004canonical}
\citation{chaudhuri2009multi}
\citation{deleus2011functional}
\citation{correa2010canonical}
\citation{wilms2013sparse}
\citation{zhang2013l1}
\citation{singanamalli2014supervised}
\citation{yan2014accelerating}
\citation{spuler2013spatial}
\citation{lin2014correspondence}
\citation{campi2013non}
\citation{todros2012measure}
\citation{doscanonical}
\citation{wilks2014probabilistic}
\citation{prera2014using}
\citation{vilsaint2013ecology}
\citation{travis2014creativity}
\citation{paul2007asymptotics}
\citation{benaych2011eigenvalues}
\citation{asendorf2013performance}
\citation{benaych2012singular}
\citation{hotelling1936relations}
\citation{pezeshki2004empirical}
\citation{nadakuditi2011fundamental}
\@writefile{toc}{\hbox { }}
\@writefile{toc}{\contentsline {chap}{\numberline {\hbox { }\hfill \bf  VI.\hspace  {5pt}}{\bf  The Top Singular Values of $XY^H$}}{163}}
\@writefile{toc}{\hbox { }}
\newlabel{sec:chpt_rcca}{{VI}{163}}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Introduction}{163}}
\newlabel{sec:chpt6:intro}{{6.1}{163}}
\citation{chen2010new}
\citation{gu20082}
\citation{gu2007joint}
\citation{kikuchi2006pair}
\citation{diamantaras1994cross}
\citation{worsley2005comparing}
\newlabel{fig:chpt6:motiv_1}{{6.1(a)}{165}}
\newlabel{sub@fig:chpt6:motiv_1}{{(a)}{165}}
\newlabel{fig:chpt6:motiv_2}{{6.1(b)}{165}}
\newlabel{sub@fig:chpt6:motiv_2}{{(b)}{165}}
\newlabel{fig:chpt6:motiv_3}{{6.1(c)}{165}}
\newlabel{sub@fig:chpt6:motiv_3}{{(c)}{165}}
\newlabel{fig:chpt6:motiv_all}{{6.1(d)}{165}}
\newlabel{sub@fig:chpt6:motiv_all}{{(d)}{165}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.1}{\ignorespaces Motivational example of the singular value spectra of $\frac  {1}{n}XY^H$ for three different sets of parameters. In all figures $p=q=200$, $n=500$, and $\theta =\theta _x=\theta _y$. In the settings in (a) and (b), the singular value spectra are very similar, with one singular value separating from the bulk of the singular values. In the setting in (c) where there is no correlation between the datasets, two singular values separate from the bulk of the singular values.\relax }}{165}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\rho =0.9$, $\theta =3$}}}{165}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\rho =0.5$, $\theta =4$}}}{165}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$\rho =0$, $\theta =10$}}}{165}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Combined}}}{165}}
\newlabel{fig:chpt6:motiv}{{6.1}{165}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Data Model and Background}{166}}
\newlabel{sec:chpt6:data_model}{{6.2}{166}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Data Model}{166}}
\newlabel{eq:model}{{6.1}{166}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.2}Empirical CCA}{166}}
\newlabel{eq:cca_opt}{{6.2}{166}}
\citation{pezeshki2004empirical}
\citation{nadakuditi2011fundamental}
\newlabel{eq:cca_svd_sol}{{6.3}{167}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.3}RCCA}{167}}
\newlabel{eq:rcca_opt}{{6.4}{167}}
\newlabel{eq:rcca_sol}{{6.5}{167}}
\citation{asendorf2013performance}
\citation{nadakuditi2008sample}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.4}ICCA}{168}}
\newlabel{eq:icca_chat}{{6.6}{168}}
\newlabel{eq:icca_rho}{{6.7}{168}}
\@writefile{toc}{\contentsline {section}{\numberline {6.3}Main Results}{168}}
\newlabel{sec:chpt6:main_results}{{6.3}{168}}
\newlabel{thm:lrcca}{{6.3.1}{168}}
\newlabel{eq:lrcca}{{6.8}{169}}
\newlabel{thm:xy_sv}{{6.3.2}{169}}
\newlabel{eq:thm}{{6.9}{169}}
\@writefile{toc}{\contentsline {section}{\numberline {6.4}Empirical Simulations}{170}}
\newlabel{sec:chpt6:emp_results}{{6.4}{170}}
\citation{benaych2011eigenvalues}
\citation{benaych2012singular}
\citation{paul2007asymptotics}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.1}Performance of RCCA}{171}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.2}Numerical Accuracy of Theorem 6.3.2\hbox {}}{171}}
\newlabel{fig:chpt6:rcca1}{{6.2(a)}{172}}
\newlabel{sub@fig:chpt6:rcca1}{{(a)}{172}}
\newlabel{fig:chpt6:rcca2}{{6.2(b)}{172}}
\newlabel{sub@fig:chpt6:rcca2}{{(b)}{172}}
\newlabel{fig:chpt6:rcca3}{{6.2(c)}{172}}
\newlabel{sub@fig:chpt6:rcca3}{{(c)}{172}}
\newlabel{fig:chpt6:rcca4}{{6.2(d)}{172}}
\newlabel{sub@fig:chpt6:rcca4}{{(d)}{172}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.2}{\ignorespaces AUC performance of RCCA for various regularization parameters. For all figures, $p=100$, $q=150$, $r=1$, and $\rho _1=0.9$. Each figure plots an AUC heatmap while sweeping over $\theta =\theta _{x1}=\theta _{y1}$ and $n$. AUC points are generated from an ROC formed from 500 points of each distribution. Increasing the regularization parameter increases the performance of CCA. This gives rise to LRCCA, which sets $\eta \to \infty $.\relax }}{172}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\eta =0.0001$}}}{172}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\eta =0.1$}}}{172}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$\eta =10$}}}{172}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {$\eta =1000$}}}{172}}
\newlabel{fig:chpt6:rcca}{{6.2}{172}}
\newlabel{eq:pt}{{6.10}{172}}
\citation{nadakuditi2011fundamental}
\@writefile{lof}{\contentsline {figure}{\numberline {6.3}{\ignorespaces Top singular value prediction for the rank-1 case for $p=200$, $q=400$, $n=400$, and $\rho =1$.\relax }}{173}}
\newlabel{fig:chpt6:sv_pred}{{6.3}{173}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4.3}Comparison to ICCA}{173}}
\newlabel{fig:chpt6:rcca1}{{6.4(a)}{174}}
\newlabel{sub@fig:chpt6:rcca1}{{(a)}{174}}
\newlabel{fig:chpt6:rcca2}{{6.4(b)}{174}}
\newlabel{sub@fig:chpt6:rcca2}{{(b)}{174}}
\newlabel{fig:chpt6:theta_theta}{{6.4(e)}{174}}
\newlabel{sub@fig:chpt6:theta_theta}{{(e)}{174}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.4}{\ignorespaces Top singular value of LRCCA plotted for pairs of parameter sweeps. In all plots, $p=200$ and $q=400$. The theoretical boundary where the top singular value is indistinguishable from a noise only setting is plotted for each. Below this line, the top singular value is asymptotically identical to the noise only setting. Above this line, the top singular value is asymptotically different from that of the noise only setting.\relax }}{174}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\rho =1$, $\theta =\theta _{x1}=\theta _{y1}$}}}{174}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\rho =0.1$, $\theta =\theta _{x1}=\theta _{y1}$}}}{174}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$\theta _{x1}=\theta _{y1}=1$}}}{174}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {$n=400$}}}{174}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {$\rho =0.5$, $n=400$}}}{174}}
\newlabel{fig:chpt6:lrcca_pt}{{6.4}{174}}
\newlabel{fig:chpt6:rcca1}{{6.5(a)}{175}}
\newlabel{sub@fig:chpt6:rcca1}{{(a)}{175}}
\newlabel{fig:chpt6:rcca2}{{6.5(b)}{175}}
\newlabel{sub@fig:chpt6:rcca2}{{(b)}{175}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.5}{\ignorespaces KS statistic between the top singular value of LRCCA in signal bearing and noise only settings. In all plots, $p=200$, $q=400$, and $n=400$. The theoretical boundary where the top singular value is indistinguishable from a noise only setting is plotted for each.\relax }}{175}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\rho =1$}}}{175}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$\rho =0.8$}}}{175}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$\rho =0.6$}}}{175}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {$\rho =0.4$}}}{175}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {$\rho =0.2$}}}{175}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {$\rho =0$}}}{175}}
\newlabel{fig:chpt6:lrcca_ks}{{6.5}{175}}
\@writefile{toc}{\contentsline {section}{\numberline {6.5}Proofs of Theorems 6.3.1\hbox {} and 6.3.2\hbox {}}{176}}
\newlabel{sec:chpt6:proofs}{{6.5}{176}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.1}Proof of Theorem 6.3.1\hbox {}}{176}}
\newlabel{sec:lrcca_proof}{{6.5.1}{176}}
\@writefile{lof}{\contentsline {figure}{\numberline {6.6}{\ignorespaces Phase transition for LRCCA (dahsed lines) for various $\rho $ and ICCA. The performance of ICCA is independent of $\rho $. The setting shown in for $c_x=0.5$ and $c_y=1$. \relax }}{177}}
\newlabel{fig:chpt6:icca_comp}{{6.6}{177}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5.2}Proof of Theorem 6.3.2\hbox {}}{178}}
\newlabel{sec:xy_sv_proof}{{6.5.2}{178}}
\newlabel{eq:det1}{{6.12}{179}}
\newlabel{eq:det2}{{6.13}{179}}
\citation{nadakuditi2007thesis}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.2.1}Expression for $\varphi _F$}{183}}
\newlabel{eq:phiF1}{{6.14}{183}}
\newlabel{eq:phiF2}{{6.15}{183}}
\citation{nadakuditi2007thesis}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.2.2}Expression for $\varphi _J$}{185}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.2.3}Expression for $\varphi _G$}{185}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.2.4}Expression for $\varphi _H$}{185}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.5.2.5}Expression for $\varphi _K$}{186}}
\citation{paul2007asymptotics}
\citation{benaych2011eigenvalues}
\citation{asendorf2013performance}
\citation{benaych2012singular}
\@writefile{toc}{\hbox { }}
\@writefile{toc}{\contentsline {chap}{\numberline {\hbox { }\hfill \bf  VII.\hspace  {5pt}}{\bf  The Largest Singular Values of a Random Projection of a Low-Rank Perturbation of a Random Matrix}}{187}}
\@writefile{toc}{\hbox { }}
\newlabel{sec:chpt_svd_proj}{{VII}{187}}
\@writefile{toc}{\contentsline {section}{\numberline {7.1}Introduction}{187}}
\newlabel{eq:chpt7:data_model}{{7.1}{187}}
\citation{belabbas2007fast}
\citation{gu1996efficient}
\citation{rudelson2007sampling}
\citation{hehyperspectral}
\citation{rokhlin2009randomized}
\citation{halko2011algorithm}
\citation{achlioptas2007fast}
\citation{arora2006fast}
\citation{liberty2007randomized}
\citation{ramachandra2011compressive}
\citation{halko2011finding}
\citation{candes2006near}
\citation{donoho2006compressed}
\newlabel{eq:chpt7:yn}{{7.2}{188}}
\newlabel{fig:chpt7:motiv_full1}{{7.1(a)}{189}}
\newlabel{sub@fig:chpt7:motiv_full1}{{(a)}{189}}
\newlabel{fig:chpt7:motiv_orth1}{{7.1(b)}{189}}
\newlabel{sub@fig:chpt7:motiv_orth1}{{(b)}{189}}
\newlabel{fig:chpt7:motiv_gauss1}{{7.1(c)}{189}}
\newlabel{sub@fig:chpt7:motiv_gauss1}{{(c)}{189}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.1}{\ignorespaces Singular value spectra for the full matrix (a), orthogonal projection matrix (b), and Gaussian projection matrix (c). This example uses a rank-1 setting where $n=1000$, $m=100$, $N=1000$, $\theta =\theta _x=\theta _y=4$.\relax }}{189}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle X$}\mathaccent "0365{X}$}}}{189}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$Q^H\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle X$}\mathaccent "0365{X}$}}}{189}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$G^H\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle X$}\mathaccent "0365{X}$}}}{189}}
\newlabel{fig:chpt7:motivation1}{{7.1}{189}}
\newlabel{fig:chpt7:motiv_full2}{{7.2(a)}{189}}
\newlabel{sub@fig:chpt7:motiv_full2}{{(a)}{189}}
\newlabel{fig:chpt7:motiv_orth2}{{7.2(b)}{189}}
\newlabel{sub@fig:chpt7:motiv_orth2}{{(b)}{189}}
\newlabel{fig:chpt7:motiv_gauss2}{{7.2(c)}{189}}
\newlabel{sub@fig:chpt7:motiv_gauss2}{{(c)}{189}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.2}{\ignorespaces Singular value spectra for the full matrix (a), orthogonal projection matrix (b), and Gaussian projection matrix (c). This example uses a rank-1 setting where $n=1000$, $m=100$, $N=1000$, $\theta =\theta _x=\theta _y=2.2$.\relax }}{189}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle X$}\mathaccent "0365{X}$}}}{189}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$Q^H\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle X$}\mathaccent "0365{X}$}}}{189}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$G^H\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle X$}\mathaccent "0365{X}$}}}{189}}
\newlabel{fig:chpt7:motivation2}{{7.2}{189}}
\newlabel{assum:x_limit}{{7.1.1}{189}}
\newlabel{assum:m_limit}{{7.1.2}{190}}
\newlabel{assum:m_b}{{7.1.4}{190}}
\@writefile{toc}{\contentsline {section}{\numberline {7.2}Main Results}{190}}
\newlabel{sec:chpt7:main_results}{{7.2}{190}}
\newlabel{thm:svd_proj}{{7.2.1}{190}}
\newlabel{eq:chpt7:solution}{{7.3}{190}}
\citation{rao2008polynomial}
\citation{paul2007asymptotics}
\citation{benaych2011eigenvalues}
\citation{asendorf2013performance}
\citation{benaych2012singular}
\newlabel{eq:chpt7:ugly}{{7.4}{191}}
\newlabel{corr:svd_proj_unitary}{{7.2.1}{191}}
\newlabel{corr:svd_proj_pt}{{7.2.2}{191}}
\@writefile{toc}{\contentsline {section}{\numberline {7.3}Proof of Theorem 7.2.1\hbox {}}{192}}
\newlabel{sec:chpt7:proof}{{7.3}{192}}
\newlabel{eq:chpt7:det1}{{7.5}{192}}
\citation{nadakuditi2007thesis}
\newlabel{eq:chpt7:det2}{{7.6}{193}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.1}Expression for $\varphi _F$}{195}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.2}Expression for $\varphi _H$}{195}}
\citation{rao2008polynomial}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3.3}Proof of Corollary 7.2.2\hbox {}}{196}}
\newlabel{eq:chpt7:pt}{{7.7}{196}}
\@writefile{toc}{\contentsline {section}{\numberline {7.4}Proof of Corollary 7.2.1\hbox {}}{196}}
\newlabel{sec:chpt7:corr_proof}{{7.4}{196}}
\newlabel{eq:chpt7:ortho_phiF}{{7.8}{197}}
\newlabel{eq:chpt7:ortho_ell}{{7.9}{197}}
\newlabel{eq:chpt7:ortho_phiH}{{7.10}{197}}
\newlabel{eq:chpt7:ortho_b}{{7.11}{198}}
\newlabel{eq:chpt7:ortho_pt}{{7.12}{198}}
\newlabel{eq:chpt7:ortho_summary}{{7.13}{198}}
\@writefile{toc}{\contentsline {section}{\numberline {7.5}Empirical Results}{198}}
\newlabel{sec:chpt7:emp_res}{{7.5}{198}}
\citation{rao2008polynomial}
\citation{rao2008polynomial}
\@writefile{lof}{\contentsline {figure}{\numberline {7.3}{\ignorespaces {\sc  {matlab}} code to compute $\varphi _F$ and $\varphi _H$ for a Gaussian projection matrix and Gaussian noise matrix. This relies on function provided in RMTool \cite  {rao2008polynomial}.\relax }}{199}}
\newlabel{fig:chpt7:g_code}{{7.3}{199}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.1}Gaussian Projection, $G$}{199}}
\newlabel{fig:chpt7:gauss_pred}{{7.4(a)}{200}}
\newlabel{sub@fig:chpt7:gauss_pred}{{(a)}{200}}
\newlabel{fig:chpt7:gauss_like_pred}{{7.4(b)}{200}}
\newlabel{sub@fig:chpt7:gauss_like_pred}{{(b)}{200}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.4}{\ignorespaces (a) Singular value prediction for Gaussian $G$ and $X$ for a rank-1 setting with fixed $n=1000$, $N=1220$ and $m=100$. The theoretical prediction uses (7.3\hbox {}) with approximations from Figure 7.3\hbox {}. Empirical results are averaged over 500 trials. (b) Singular value prediction for Gaussian-like $G$ and Gaussian $X$ for a rank-1 setting with fixed $n=1000$, $N=1220$ and $m=100$. Here, the entries of $G$ are either $\pm 1$ with equal probability. The theoretical prediction is the same for (a). Empirical results are again averaged over 500 trials.\relax }}{200}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Gaussian $G$}}}{200}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Gaussian-like $G$}}}{200}}
\newlabel{fig:chpt7:gauss_sv}{{7.4}{200}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.2}Unitary Projection, $Q$}{201}}
\newlabel{eq:chpt7:fourier}{{7.14}{201}}
\newlabel{fig:chpt7:gauss1}{{7.5(a)}{202}}
\newlabel{sub@fig:chpt7:gauss1}{{(a)}{202}}
\newlabel{fig:chpt7:gauss2}{{7.5(b)}{202}}
\newlabel{sub@fig:chpt7:gauss2}{{(b)}{202}}
\newlabel{fig:chpt7:gauss3}{{7.5(c)}{202}}
\newlabel{sub@fig:chpt7:gauss3}{{(c)}{202}}
\newlabel{fig:chpt7:gauss4}{{7.5(d)}{202}}
\newlabel{sub@fig:chpt7:gauss4}{{(d)}{202}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.5}{\ignorespaces Performance of theoretical phase transition prediction for Gaussian $G$ and $X$ for a rank-1 setting with fixed $n=1000$. The theoretical prediction uses (7.3\hbox {}) with approximations from Figure 7.3\hbox {}. The first row plots the KS statistic between singular values generated from 500 signal bearing and 500 noise only matrices. The bottom row plots the average empirical singular value averaged over 500 trials. The left column sweeps over both $\theta $ and $N$ for a fixed $m=100$ while the right column sweeps over $\theta $ and $m$ for a fixed $N=1000$.\relax }}{202}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {KS Statistic - $N$,$\theta $ sweep}}}{202}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {KS Statistic - $m$,$\theta $ sweep}}}{202}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Maximum singular value - $N$,$\theta $ sweep}}}{202}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Maximum singular value - $m$, $\theta $ sweep}}}{202}}
\newlabel{fig:chpt7:gauss}{{7.5}{202}}
\newlabel{fig:chpt7:ortho_pred}{{7.6(a)}{203}}
\newlabel{sub@fig:chpt7:ortho_pred}{{(a)}{203}}
\newlabel{fig:chpt7:fourier_pred}{{7.6(b)}{203}}
\newlabel{sub@fig:chpt7:fourier_pred}{{(b)}{203}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.6}{\ignorespaces (a) Singular value prediction for unitary projection matrix $Q$ and Gaussian noise matrix $X$ for a rank-1 setting with fixed $n=1000$, $N=1220$ and $m=100$. The theoretical prediction uses Corollary 7.2.1\hbox {}. Empirical results are averaged over 500 trials. (b) Singular value prediction for unitary-like matrix $Q$ and Gaussian noise matrix $X$ for a rank-1 setting with fixed $n=1000$, $N=1220$ and $m=100$. Here, the columns of $Q$ are sampled from the $n\times n$ discrete Fourier matrix defined in (7.14\hbox {}). The theoretical prediction is the same as (a) and uses Corollary 7.2.1\hbox {}. Empirical results are averaged over 500 trials.\relax }}{203}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Unitary $Q$}}}{203}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Fourier $Q$}}}{203}}
\newlabel{fig:chpt7:ortho_sv}{{7.6}{203}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5.3}Comparison}{203}}
\newlabel{fig:chpt7:ortho1}{{7.7(a)}{204}}
\newlabel{sub@fig:chpt7:ortho1}{{(a)}{204}}
\newlabel{fig:chpt7:ortho2}{{7.7(b)}{204}}
\newlabel{sub@fig:chpt7:ortho2}{{(b)}{204}}
\newlabel{fig:chpt7:ortho3}{{7.7(c)}{204}}
\newlabel{sub@fig:chpt7:ortho3}{{(c)}{204}}
\newlabel{fig:chpt7:ortho4}{{7.7(d)}{204}}
\newlabel{sub@fig:chpt7:ortho4}{{(d)}{204}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.7}{\ignorespaces Performance of theoretical phase transition prediction for unitary projection matrix $Q$ and Gaussian noise matrix $X$ for a rank-1 setting with fixed $n=1000$. The theoretical prediction uses Corollary 7.2.1\hbox {}. The first row plots the KS statistic between singular values generated from 500 signal bearing and 500 noise only matrices. The bottom row plots the average empirical singular value averaged over 500 trials. The left column sweeps over both $\theta $ and $N$ for a fixed $m=100$ while the right column sweeps over $\theta $ and $m$ for a fixed $N=1000$.\relax }}{204}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {KS Statistic - $N$,$\theta $ sweep}}}{204}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {KS Statistic - $m$,$\theta $ sweep}}}{204}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Maximum singular value - $N$,$\theta $ sweep}}}{204}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Maximum singular value - $m$, $\theta $ sweep}}}{204}}
\newlabel{fig:chpt7:orth_g}{{7.7}{204}}
\newlabel{fig:chpt7:comp1}{{7.8(a)}{206}}
\newlabel{sub@fig:chpt7:comp1}{{(a)}{206}}
\newlabel{fig:chpt7:comp2}{{7.8(b)}{206}}
\newlabel{sub@fig:chpt7:comp2}{{(b)}{206}}
\@writefile{lof}{\contentsline {figure}{\numberline {7.8}{\ignorespaces Performance difference between using a Gaussian projection matrix, $G$, and a unitary projection matrix, $Q$, for a rank-1 setting with fixed $n=1000$ and Gaussian noise matrix $X$. Positive values indicate that the Gaussian projection can more reliably detect the signal while negative values indicate that the unitary projection can more reliably detect the signal. We observe that the unitary projection outperforms the Gaussian projection.\relax }}{206}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {KS Statistic difference - $N$,$\theta $ sweep}}}{206}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {KS Statistic difference - $m$,$\theta $ sweep}}}{206}}
\newlabel{fig:chpt7:comparison}{{7.8}{206}}
\citation{besson2006cfar}
\citation{besson2005matched}
\citation{bandiera2007glrt}
\citation{bandiera2007adaptive}
\citation{elden2007matrix}
\citation{mcwhorter2003matched}
\citation{vincent2008matched}
\citation{scharf1994matched}
\citation{jin2005cfar}
\citation{pezeshki2006canonical}
\@writefile{toc}{\hbox { }}
\@writefile{toc}{\contentsline {chap}{\numberline {\hbox { }\hfill \bf  VIII.\hspace  {5pt}}{\bf  CCA and ICCA for Regression and Detection}}{207}}
\@writefile{toc}{\hbox { }}
\newlabel{sec:chpt_det_reg}{{VIII}{207}}
\@writefile{toc}{\contentsline {section}{\numberline {8.1}Introduction}{207}}
\@writefile{toc}{\contentsline {section}{\numberline {8.2}Data Model and Parameter Estimation}{208}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.1}Training Data}{208}}
\newlabel{eq:chpt8:cca_regr_model}{{8.1}{208}}
\newlabel{eq:chpt8:true_scm}{{8.2}{209}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.2}Parameter Estimation}{209}}
\newlabel{sec:param_estims}{{8.2.2}{209}}
\newlabel{eq:chpt8:param_estims}{{8.3}{210}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2.3}Testing Data}{210}}
\newlabel{eq:chpt8:cca_detect_model}{{8.4}{210}}
\@writefile{toc}{\contentsline {section}{\numberline {8.3}Standard Regression Techniques}{210}}
\newlabel{sec:reg:plugin}{{8.3}{210}}
\newlabel{eq:chpt8:mle}{{8.5}{211}}
\newlabel{eq:chpt8:mle_model}{{8.6}{211}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.1}Plug-in Predictor}{211}}
\newlabel{eq:chpt8:prior}{{8.7}{211}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3.2}Prediction using CCA, empirical CCA, and ICCA}{211}}
\newlabel{sec:reg:cca}{{8.3.2}{211}}
\@writefile{toc}{\contentsline {section}{\numberline {8.4}Random Matrix Theory Preliminaries}{212}}
\newlabel{sec:reg:rmt}{{8.4}{212}}
\newlabel{prop:subspace_estim}{{8.4.1}{212}}
\citation{paul2007asymptotics}
\citation{benaych2011eigenvalues}
\citation{paul2007asymptotics}
\citation{benaych2011singular}
\newlabel{eq:chpt8:uacc}{{8.8}{213}}
\newlabel{prop:snr_estim}{{8.4.2}{213}}
\newlabel{eq:chpt8:sigacc}{{8.9}{213}}
\citation{bao2014canonical}
\citation{pezeshki2004empirical}
\citation{nadakuditi2011fundamental}
\newlabel{eq:chpt8:bao_cca}{{8.10}{214}}
\newlabel{eq:chpt8:rc}{{8.11}{214}}
\@writefile{toc}{\contentsline {section}{\numberline {8.5}Theoretical MSE Derivations}{215}}
\newlabel{eq:chpt8:gen_mse}{{8.12}{215}}
\newlabel{eq:chpt8:mse_first}{{8.13}{215}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.1}ICCA}{216}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.2}CCA}{218}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.5.3}Plug-in}{220}}
\@writefile{toc}{\contentsline {section}{\numberline {8.6}Rank-1 Empirical Results}{220}}
\citation{van1968detection}
\@writefile{toc}{\contentsline {section}{\numberline {8.7}LRT Detector Derivation}{221}}
\newlabel{fig:chpt8:regr_mse}{{8.1(a)}{222}}
\newlabel{sub@fig:chpt8:regr_mse}{{(a)}{222}}
\newlabel{fig:chpt8:regr_mse_zoom}{{8.1(b)}{222}}
\newlabel{sub@fig:chpt8:regr_mse_zoom}{{(b)}{222}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.1}{\ignorespaces Empirical and theoretically predicted MSE for the plug-in, CCA, and ICCA estimators. We use a rank-1 setting where $k_x=k_y=1$, $\theta ^{(x)}_1=3$, $\theta ^{(y)}_1=4$, $p=100$, $q=200$ and $\rho =0.9$. In the rank-1 setting, $U_{\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle K$}\mathaccent "0365{K}}=V_{\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle K$}\mathaccent "0365{K}}=1$. The plot on the right is a zoomed in version of the plot on the left. The ICCA and plug-in curves lie on top of each other, as we showed that the two are equivalent.\relax }}{222}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Empirical and Theoretical MSE}}}{222}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Zoomed-in}}}{222}}
\newlabel{fig:chpt8:mse}{{8.1}{222}}
\newlabel{eq:chpt8:lrt}{{8.14}{222}}
\newlabel{eq:chpt8:lrt_stat}{{8.15}{222}}
\newlabel{eq:chpt8:lrt_detect}{{8.16}{223}}
\@writefile{toc}{\contentsline {section}{\numberline {8.8}CCA Detector Equivalency}{224}}
\newlabel{eq:chpt8:cca_stat}{{8.17}{225}}
\newlabel{eq:chpt8:cca_detect}{{8.18}{225}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.8.1}CCA Detector for Data Model (8.4\hbox {})}{226}}
\newlabel{eq:chpt8:cca_detec_C}{{8.19}{226}}
\newlabel{eq:chpt8:cca_stat_r}{{8.20}{226}}
\@writefile{toc}{\contentsline {section}{\numberline {8.9}Empirical Detectors}{227}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.9.1}Plug-in Detector}{227}}
\newlabel{eq:chpt8:plugin_lrt_stat}{{8.21}{227}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.9.2}Empirical CCA Detector}{227}}
\newlabel{eq:chpt8:emp_cca_detec_params}{{8.22}{228}}
\newlabel{eq:chpt8:cca_plugin_stat}{{8.23}{228}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.9.3} ICCA Detector}{228}}
\newlabel{eq:chpt8:icca_plugin_stat}{{8.24}{228}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.9.4}Proof that $\Lambda _{\text  {icca}}(\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \xi $}\mathaccent "0365{\xi }) \equiv \Lambda _{\text  {plug-in}}(w)$}{228}}
\citation{fawcett2006introduction}
\citation{fawcett2006introduction}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.9.5}Rank 1 Numerical Simulations}{231}}
\citation{fawcett2006introduction}
\citation{fawcett2006introduction}
\citation{fawcett2006introduction}
\citation{fawcett2006introduction}
\newlabel{fig:chpt8:auc_lrt_high_rho}{{8.2(a)}{233}}
\newlabel{sub@fig:chpt8:auc_lrt_high_rho}{{(a)}{233}}
\newlabel{fig:chpt8:auc_icca_high_rho}{{8.2(b)}{233}}
\newlabel{sub@fig:chpt8:auc_icca_high_rho}{{(b)}{233}}
\newlabel{fig:chpt8:auc_cca_high_rho}{{8.2(c)}{233}}
\newlabel{sub@fig:chpt8:auc_cca_high_rho}{{(c)}{233}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.2}{\ignorespaces AUC results for the plug-in LRT, empirical CCA, and ICCA detectors in (8.21\hbox {}), (8.23\hbox {}), and (8.24\hbox {}), respectively. Empirical ROC curves were simulated using $2000$ test samples for each hypothesis and averaged over $50$ trials using algorithms 2 and 4 of \cite  {fawcett2006introduction}. Simulations parameters were $p=200$, $q=150$, and $\rho =0.8$. Each figure plots the AUC for the average ROC curve at a different values of SNR, $\theta =\theta ^{(x)}_1=\theta ^{(y)}_1$, and training samples, $n$.\relax }}{233}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Plug-in LRT}}}{233}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {ICCA}}}{233}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Empirical CCA}}}{233}}
\newlabel{fig:chpt8:auc_high_rho}{{8.2}{233}}
\newlabel{fig:chpt8:auc_lrt_low_rho}{{8.3(a)}{234}}
\newlabel{sub@fig:chpt8:auc_lrt_low_rho}{{(a)}{234}}
\newlabel{fig:chpt8:auc_icca_low_rho}{{8.3(b)}{234}}
\newlabel{sub@fig:chpt8:auc_icca_low_rho}{{(b)}{234}}
\newlabel{fig:chpt8:auc_cca_low_rho}{{8.3(c)}{234}}
\newlabel{sub@fig:chpt8:auc_cca_low_rho}{{(c)}{234}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.3}{\ignorespaces AUC results for the plug-in LRT, empirical CCA, and ICCA detectors in (8.21\hbox {}), (8.23\hbox {}), and (8.24\hbox {}), respectively. Empirical ROC curves were simulated using $2000$ test samples for each hypothesis and averaged over $50$ trials using algorithms 2 and 4 of \cite  {fawcett2006introduction}. Simulations parameters were $p=200$, $q=150$, and $\rho =0.2$. Each figure plots the AUC for the average ROC curve at a different value of SNR, $\theta =\theta ^{(x)}_1=\theta ^{(y)}_1$, and training samples, $n$.\relax }}{234}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Plug-in LRT}}}{234}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {ICCA}}}{234}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {CCA}}}{234}}
\newlabel{fig:chpt8:auc_low_rho}{{8.3}{234}}
\newlabel{fig:chpt8:auc_cca_diff}{{\caption@xref {fig:chpt8:auc_cca_diff}{ on input line 1070}}{235}}
\@writefile{lof}{\contentsline {figure}{\numberline {8.4}{\ignorespaces Difference between ICCA AUC heatmaps in Figures 8.3(c)\hbox {} and 8.2(c)\hbox {}. Positive values indicate when the setting of $\rho =0.8$ achieves a higher AUC. Negative values indicate when the setting of $\rho =0.2$ achieves a higher AUC.\relax }}{235}}
\newlabel{fig:chpt8:auc_diff}{{8.4}{235}}
\citation{smeulders2000content}
\@writefile{toc}{\hbox { }}
\@writefile{toc}{\contentsline {chap}{\numberline {\hbox { }\hfill \bf  IX.\hspace  {5pt}}{\bf  Content Based Image Retrieval and Automatic Image Annotation Using Correlation Methods}}{236}}
\@writefile{toc}{\hbox { }}
\newlabel{sec:chpt_ia}{{IX}{236}}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Introduction}{236}}
\citation{chaudhuri2009multi}
\citation{hardoon2004canonical}
\citation{hardoon2006correlation}
\citation{pezeshki2004empirical}
\citation{hotelling1936relations}
\citation{hardoon2004canonical}
\citation{via2005canonical}
\citation{deleus2011functional}
\citation{correa2010canonical}
\citation{chaudhuri2009multi}
\citation{pezeshki2006canonical}
\@writefile{toc}{\contentsline {section}{\numberline {9.2}Correlation Methods}{238}}
\newlabel{sec:cca}{{9.2}{238}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.1}Mathematical Formulation of CCA}{238}}
\newlabel{sec:cca_form}{{9.2.1}{238}}
\citation{nielsen2002multiset}
\citation{nadakuditi2011fundamental}
\citation{welling2005kcca}
\citation{kettenring1971canonical}
\citation{nielsen1994analysis}
\newlabel{eq:chpt9:cca_opt}{{9.1}{239}}
\newlabel{eq:chpt9:cca_opt2}{{9.2}{239}}
\newlabel{eq:chpt9:cca_eigval_sys}{{9.3}{239}}
\newlabel{eq:chpt9:x2}{{9.4}{239}}
\newlabel{eq:chpt9:eigval_sys2}{{9.5}{239}}
\newlabel{eq:chpt9:cca_C_eigval}{{9.6}{239}}
\citation{pezeshki2004empirical}
\newlabel{eq:chpt9:cca_svd_sol}{{9.7}{240}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.2}Empirical CCA}{240}}
\newlabel{sec:emp_cca}{{9.2.2}{240}}
\newlabel{eq:chpt9:scm}{{9.8}{240}}
\newlabel{eq:chpt9:cca_Chat}{{9.9}{240}}
\newlabel{eq:chpt9:cca_svd_sol}{{9.10}{240}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2.3}Informative CCA}{241}}
\newlabel{eq:chpt9:icca_chat}{{9.11}{241}}
\newlabel{eq:chpt9:icca_rho}{{9.12}{241}}
\@writefile{toc}{\contentsline {section}{\numberline {9.3}System Implementation}{241}}
\newlabel{sec:propose}{{9.3}{241}}
\citation{yang2007evaluating}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.1}Text Processing}{242}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.2}Image Processing}{242}}
\citation{lowe1999object}
\@writefile{lof}{\contentsline {figure}{\numberline {9.1}{\ignorespaces Shared training pipeline for image retrieval and annotation when using raw images as the second dataset. The system takes training images and captions as inputs and returns the canonical bases $W_x$ and $W_y$ and the correlation coefficients $P$.\relax }}{243}}
\newlabel{fig:chpt9:training}{{9.1}{243}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.2}{\ignorespaces Shared training pipeline for image retrieval and annotation when the second dataset is associated text documents. The system takes an training captions and associated documents as inputs and returns the canonical bases $W_x$ and $W_y$ and the correlation coefficients $P$.\relax }}{244}}
\newlabel{fig:chpt9:training_tt}{{9.2}{244}}
\citation{solem2012programming}
\newlabel{fig:chpt9:sift_orig}{{9.3(a)}{245}}
\newlabel{sub@fig:chpt9:sift_orig}{{(a)}{245}}
\newlabel{fig:chpt9:sift_keypoints}{{9.3(b)}{245}}
\newlabel{sub@fig:chpt9:sift_keypoints}{{(b)}{245}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.3}{\ignorespaces (a) Original image. (b) Original image with SIFT keypoint identification.\relax }}{245}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Original Image}}}{245}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {SIFT Keypoints}}}{245}}
\newlabel{fig:chpt9:sift}{{9.3}{245}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.4}{\ignorespaces Image retrieval pipeline. This system takes a text query as input and the correlation model from the training pipeline and will return relevant images.\relax }}{246}}
\newlabel{fig:chpt9:test1}{{9.4}{246}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.3}Correlation Algorithm}{246}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.5}{\ignorespaces Image annotation pipeline when using the raw image as input. This system takes an image and correlation model from the training system as inputs and will return a list of words that are most relevant for the image.\relax }}{247}}
\newlabel{fig:chpt9:test2}{{9.5}{247}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.4}Image Retrieval}{247}}
\newlabel{eq:chpt9:ir_scores}{{9.13}{247}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.6}{\ignorespaces Image annotation pipeline when using associated text documents. This system takes a text document and correlation model from the training system as inputs and will return a list of words that are most relevant for the associated image.\relax }}{248}}
\newlabel{fig:chpt9:test3}{{9.6}{248}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3.5}Image Annotation}{248}}
\citation{leong2010text}
\citation{rashtchian2010collecting}
\newlabel{eq:chpt9:ia_scores}{{9.14}{249}}
\@writefile{toc}{\contentsline {section}{\numberline {9.4}Experiments}{249}}
\newlabel{sec:results}{{9.4}{249}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.1}Pascal Image Dataset}{249}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.7}{\ignorespaces Example of an image and its captions in the Pascal dataset\relax }}{250}}
\newlabel{fig:chpt9:pascal}{{9.7}{250}}
\newlabel{fig:chpt9:CCA_query_results}{{9.8(a)}{250}}
\newlabel{sub@fig:chpt9:CCA_query_results}{{(a)}{250}}
\newlabel{fig:chpt9:ICCA_query_results}{{9.8(b)}{250}}
\newlabel{sub@fig:chpt9:ICCA_query_results}{{(b)}{250}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.8}{\ignorespaces (a) CCA results for query "airplane". (b) ICCA results for query "airplane".\relax }}{250}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {CCA Results}}}{250}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {ICCA Results}}}{250}}
\newlabel{fig:chpt9:pascal_results}{{9.8}{250}}
\citation{leong2010text}
\citation{leong2010text}
\citation{leong2010text}
\@writefile{lot}{\contentsline {table}{\numberline {9.1}{\ignorespaces Average R-precision values and correlation basis dimension, $k$, for image annotation of the University of Washington Ground Truth Dataset \relax }}{251}}
\newlabel{table:rprec}{{9.1}{251}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.2}Ground Truth Image Dataset}{251}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.3}Gold Standard Web Dataset}{251}}
\citation{feng2008automatic}
\citation{feng2008automatic}
\citation{leong2010text}
\citation{feng2008automatic}
\citation{leong2010text}
\citation{schmid1994probabilistic}
\@writefile{lot}{\contentsline {table}{\numberline {9.2}{\ignorespaces Image annotation for the Web Image-Article dataset.\relax }}{252}}
\newlabel{table:gold_metrics}{{9.2}{252}}
\@writefile{lot}{\contentsline {table}{\numberline {9.3}{\ignorespaces Image annotation for the Web Image-Article dataset.\relax }}{252}}
\newlabel{table:gold}{{9.3}{252}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4.4}BBC News Dataset}{252}}
\@writefile{lot}{\contentsline {table}{\numberline {9.4}{\ignorespaces Image annotation for the Web Image-Article dataset.\relax }}{253}}
\newlabel{table:bbc}{{9.4}{253}}
\@writefile{toc}{\contentsline {section}{\numberline {9.5}Discussion}{253}}
\newlabel{sec:disc}{{9.5}{253}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.1}Pascal Results}{253}}
\citation{pezeshki2004empirical}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.2}Image Annotation of Ground Truth Dataset}{254}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.3}Annotation of Gold Standard Web Dataset}{255}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5.4}Annotation of BBC News Dataset}{255}}
\@writefile{toc}{\contentsline {section}{\numberline {9.6}Conclusion}{256}}
\newlabel{sec:conc}{{9.6}{256}}
\newlabel{fig:chpt9:cca_ir}{{9.9(a)}{258}}
\newlabel{sub@fig:chpt9:cca_ir}{{(a)}{258}}
\newlabel{fig:chpt9:cca_ir_zoom}{{9.9(b)}{258}}
\newlabel{sub@fig:chpt9:cca_ir_zoom}{{(b)}{258}}
\newlabel{fig:chpt9:icca_ir}{{9.9(c)}{258}}
\newlabel{sub@fig:chpt9:icca_ir}{{(c)}{258}}
\newlabel{fig:chpt9:icca_ir_zoom}{{9.9(d)}{258}}
\newlabel{sub@fig:chpt9:icca_ir_zoom}{{(d)}{258}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.9}{\ignorespaces (a) Scores for all 1000 Pascal images for the query ``airplane'' for CCA. (b) Zoomed in version of (a) to highlight the top scores returned in Figure 9.8(a)\hbox {}. (c) Scores for all 1000 Pascal images for the query ``airplane'' for ICCA. (d) Zoomed in version of (c) to highlight the top scores which are returned in Figure 9.8(b)\hbox {}. All scores are the norm in (9.13\hbox {}).\relax }}{258}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {CCA Results}}}{258}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {CCA Results Zoomed}}}{258}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {ICCA Results}}}{258}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {ICCA Results Zoomed}}}{258}}
\newlabel{fig:chpt9:pascal_ir}{{9.9}{258}}
\newlabel{fig:chpt9:pascal_query_image}{{9.10(a)}{259}}
\newlabel{sub@fig:chpt9:pascal_query_image}{{(a)}{259}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.10}{\ignorespaces CCA vs ICCA annotation results for the image query shown in 9.10(a)\hbox {}.\relax }}{259}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Image Query}}}{259}}
\newlabel{fig:chpt9:pascal_annotation}{{9.10}{259}}
\newlabel{fig:chpt9:cca_ia}{{9.11(a)}{260}}
\newlabel{sub@fig:chpt9:cca_ia}{{(a)}{260}}
\newlabel{fig:chpt9:cca_ia_zoom}{{9.11(b)}{260}}
\newlabel{sub@fig:chpt9:cca_ia_zoom}{{(b)}{260}}
\newlabel{fig:chpt9:icca_ia}{{9.11(c)}{260}}
\newlabel{sub@fig:chpt9:icca_ia}{{(c)}{260}}
\newlabel{fig:chpt9:icca_ia_zoom}{{9.11(d)}{260}}
\newlabel{sub@fig:chpt9:icca_ia_zoom}{{(d)}{260}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.11}{\ignorespaces (a) CCA scores for all words in the Pascal database for the image query in Figure 9.10(a)\hbox {}. (b) Zoomed in version of (a) to highlight the top scores returned in Figure 9.10\hbox {}. (c) ICCA scores for all words in the Pascal database for the image query in Figure 9.10(a)\hbox {}. (d) Zoomed in version of (c) to highlight the top scores which are returned in Figure 9.10\hbox {}. All scores are the norm in (9.14\hbox {}).\relax }}{260}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {CCA Results}}}{260}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {CCA Results Zoomed}}}{260}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {ICCA Results}}}{260}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {ICCA Results Zoomed}}}{260}}
\newlabel{fig:chpt9:pascal_ia}{{9.11}{260}}
\newlabel{fig:chpt9:rprec_cca_vw}{{9.12(a)}{261}}
\newlabel{sub@fig:chpt9:rprec_cca_vw}{{(a)}{261}}
\newlabel{fig:chpt9:rprec_icca_vw}{{9.12(b)}{261}}
\newlabel{sub@fig:chpt9:rprec_icca_vw}{{(b)}{261}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.12}{\ignorespaces Empirical probability density functions of R-precision of image annotation of the University of Washington Ground Truth Dataset.\relax }}{261}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {CCA}}}{261}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {ICCA}}}{261}}
\newlabel{fig:chpt9:gt_rprec}{{9.12}{261}}
\@writefile{lof}{\contentsline {figure}{\numberline {9.13}{\ignorespaces Example of an image, its caption, and title in the BBC News dataset\relax }}{261}}
\newlabel{fig:chpt9:bbc_ex}{{9.13}{261}}
\citation{yu2007learning}
\citation{nielsen2002multiset}
\citation{correa2010canonical}
\citation{deleus2011functional}
\citation{vinograde1950canonical}
\citation{steel1951minimum}
\citation{horst1961relations}
\citation{horst1961generalized}
\citation{kettenring1971canonical}
\citation{hotelling1936relations}
\citation{nielsen1994analysis}
\@writefile{toc}{\hbox { }}
\@writefile{toc}{\contentsline {chap}{\numberline {\hbox { }\hfill \bf  X.\hspace  {5pt}}{\bf  Multiset CCA (MCCA)}}{262}}
\@writefile{toc}{\hbox { }}
\newlabel{sec:chpt_mcca}{{X}{262}}
\@writefile{toc}{\contentsline {section}{\numberline {10.1}Introduction}{262}}
\citation{kettenring1971canonical}
\citation{nielsen1994analysis}
\@writefile{toc}{\contentsline {section}{\numberline {10.2}Mathematical Formulation of MCCA}{263}}
\citation{nielsen2002multiset}
\citation{nielsen1994analysis}
\newlabel{eq:chpt10:opt_prob}{{10.1}{264}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.1}Constraint Functions, $h(x,R)$}{264}}
\newlabel{sec:constraints}{{10.2.1}{264}}
\citation{kettenring1971canonical}
\citation{horst1961relations}
\citation{kettenring1971canonical}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2.2}Objective Functions, $J(\Phi (x))$}{265}}
\newlabel{sec:obj_func}{{10.2.2}{265}}
\citation{horst1961relations}
\citation{bach2003kernel}
\citation{kettenring1971canonical}
\citation{steel1951minimum}
\citation{kettenring1971canonical}
\citation{nielsen1994analysis}
\@writefile{toc}{\contentsline {section}{\numberline {10.3}Theoretical and Empirical MCCA Derivations}{266}}
\@writefile{lot}{\contentsline {table}{\numberline {10.1}{\ignorespaces Notation used in MCCA\relax }}{267}}
\newlabel{tab:mcca_notation}{{10.1}{267}}
\citation{boumal2013manopt}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.1}Manopt Software for Optimization on Manifolds}{268}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.2}Successive Canonical Vectors}{268}}
\citation{nielsen2002multiset}
\citation{nielsen1994analysis}
\citation{nielsen1994analysis}
\citation{nielsen2002multiset}
\citation{kettenring1971canonical}
\citation{nielsen1994analysis}
\citation{deleus2011functional}
\citation{nielsen2002multiset}
\citation{via2005canonical}
\citation{nielsen1994analysis}
\citation{yu2007learning}
\citation{nielsen1994analysis}
\citation{nielsen1994analysis}
\citation{correa2010canonical}
\citation{kettenring1971canonical}
\citation{nielsen1994analysis}
\citation{nielsen1994analysis}
\citation{nielsen1994analysis}
\citation{nielsen1994analysis}
\citation{kettenring1971canonical}
\citation{nielsen1994analysis}
\citation{deleus2011functional}
\citation{via2005canonical}
\citation{nielsen1994analysis}
\citation{nielsen1994analysis}
\citation{nielsen1994analysis}
\citation{kettenring1971canonical}
\citation{nielsen1994analysis}
\citation{nielsen1994analysis}
\citation{bach2003kernel}
\citation{nielsen1994analysis}
\citation{nielsen1994analysis}
\citation{kettenring1971canonical}
\citation{nielsen1994analysis}
\citation{nielsen1994analysis}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3.3}MCCA Summary}{269}}
\newlabel{sec:summary}{{10.3.3}{269}}
\@writefile{lot}{\contentsline {table}{\numberline {10.2}{\ignorespaces Summary of MCCA optimization problems. The objective functions are described in Section 10.2.2\hbox {}. The constraints are described in section 10.2.1\hbox {}. The eigenvalue problem column is the theoretical solution while the Empirical problem column describes how to solve the problem given empirical data. All eigenvalue problems solve for the maximum eigenvalue-eigenvector pair except for the MINVAR problems, which solves for the minimum eigenvalue-eigenvector pair. The final column lists references which describe the MCCA optimization problem. \relax }}{270}}
\newlabel{tab:main_results}{{10.2}{270}}
\@writefile{toc}{\contentsline {section}{\numberline {10.4}Proposed Informative MCCA Algorithm}{270}}
\citation{bach2003kernel}
\newlabel{eq:chpt10:C_cca}{{10.2}{271}}
\newlabel{eq:chpt10:R_cca}{{10.3}{271}}
\newlabel{eq:chpt10:R_mcca}{{10.4}{272}}
\citation{pezeshki2004empirical}
\newlabel{th:maxvar}{{10.4.1}{273}}
\newlabel{th:minvar}{{10.4.2}{274}}
\newlabel{conj:minmaxvar}{{10.4.1}{274}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.1}Low-Rank Multi-dataset Model}{275}}
\newlabel{eq:chpt10:mcca_data_model}{{10.5}{275}}
\newlabel{eq:chpt10:mcca_true_scm}{{10.6}{275}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.4.2}Informative MCCA}{277}}
\newlabel{eq:chpt10:that}{{10.7}{278}}
\@writefile{toc}{\contentsline {section}{\numberline {10.5}Controlled Video-Video-Video Experiment}{279}}
\newlabel{fig:chpt10:mcca_left}{{10.1(a)}{280}}
\newlabel{sub@fig:chpt10:mcca_left}{{(a)}{280}}
\newlabel{fig:chpt10:mcca_mid}{{10.1(b)}{280}}
\newlabel{sub@fig:chpt10:mcca_mid}{{(b)}{280}}
\newlabel{fig:chpt10:mcca_right}{{10.1(c)}{280}}
\newlabel{sub@fig:chpt10:mcca_right}{{(c)}{280}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.1}{\ignorespaces Left, middle, and right camera views of our four sources for the controlled MCCA flashing light experiment.\relax }}{280}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Left Camera}}}{280}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Middle Camera}}}{280}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Right Camera}}}{280}}
\newlabel{fig:chpt10:mcca_setup}{{10.1}{280}}
\newlabel{fig:chpt10:mcca_man_left}{{10.2(a)}{280}}
\newlabel{sub@fig:chpt10:mcca_man_left}{{(a)}{280}}
\newlabel{fig:chpt10:mcca_man_mid}{{10.2(b)}{280}}
\newlabel{sub@fig:chpt10:mcca_man_mid}{{(b)}{280}}
\newlabel{fig:chpt10:mcca_man_right}{{10.2(c)}{280}}
\newlabel{sub@fig:chpt10:mcca_man_right}{{(c)}{280}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.2}{\ignorespaces Manual identification of each source in each camera. All three sources share a common flashing tablet, outlined in red. The left and right camera views share a common flashing laptop screen, outlined in green. The left camera has an independent flashing phone light, outlined in dark blue. The right camera has an independent flashing police light, outlined in cyan.\relax }}{280}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Left Camera}}}{280}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Middle Camera}}}{280}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Right Camera}}}{280}}
\newlabel{fig:chpt10:mcca_sources}{{10.2}{280}}
\@writefile{lot}{\contentsline {table}{\numberline {10.3}{\ignorespaces Visual sources for each camera view. All three cameras share Tablet T1. The left and right cameras share Laptop L1. The left and right cameras each have an independent flashing light source.\relax }}{281}}
\newlabel{tab:mcca_descrp}{{10.3}{281}}
\newlabel{fig:chpt10:mcca_left_sv}{{10.3(a)}{281}}
\newlabel{sub@fig:chpt10:mcca_left_sv}{{(a)}{281}}
\newlabel{fig:chpt10:mcca_mid_sv}{{10.3(b)}{281}}
\newlabel{sub@fig:chpt10:mcca_mid_sv}{{(b)}{281}}
\newlabel{fig:chpt10:mcca_right_sv}{{10.3(c)}{281}}
\newlabel{sub@fig:chpt10:mcca_right_sv}{{(c)}{281}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.3}{\ignorespaces Singular value spectra of $Y_{\text  {left}}$, $Y_{\text  {middle}}$, and $Y_{\text  {right}}$\relax }}{281}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Left Camera}}}{281}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Middle Camera}}}{281}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Right Camera}}}{281}}
\newlabel{fig:chpt10:mcca_svs}{{10.3}{281}}
\newlabel{fig:chpt10:mcca_left_u1}{{10.4(a)}{282}}
\newlabel{sub@fig:chpt10:mcca_left_u1}{{(a)}{282}}
\newlabel{fig:chpt10:mcca_left_u2}{{10.4(b)}{282}}
\newlabel{sub@fig:chpt10:mcca_left_u2}{{(b)}{282}}
\newlabel{fig:chpt10:mcca_left_u3}{{10.4(c)}{282}}
\newlabel{sub@fig:chpt10:mcca_left_u3}{{(c)}{282}}
\newlabel{fig:chpt10:mcca_left_overlay}{{10.4(d)}{282}}
\newlabel{sub@fig:chpt10:mcca_left_overlay}{{(d)}{282}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.4}{\ignorespaces (a)-(c) Left singular vectors of $Y_{\text  {left}}$ corresponding to the top 3 singular values. (d) Thresholded singular vectors from (a)-(c) overlayed onto the original scene. These pixels correspond to the flashing light sources visible in the left camera.\relax }}{282}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$u_1$}}}{282}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$u_2$}}}{282}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$u_3$}}}{282}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Overlay}}}{282}}
\newlabel{fig:chpt10:mcca_pca_left}{{10.4}{282}}
\newlabel{fig:chpt10:mcca_mid_u1}{{10.5(a)}{282}}
\newlabel{sub@fig:chpt10:mcca_mid_u1}{{(a)}{282}}
\newlabel{fig:chpt10:mcca_mid_overlay}{{10.5(b)}{282}}
\newlabel{sub@fig:chpt10:mcca_mid_overlay}{{(b)}{282}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.5}{\ignorespaces (a) Left singular vector of $Y_{\text  {middle}}$ corresponding to the top singular value. (b) Thresholded singular vector from (a) overlayed onto the original scene. These pixels correspond to the flashing light source visible in the middle camera.\relax }}{282}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$u_1$}}}{282}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Overlay}}}{282}}
\newlabel{fig:chpt10:mcca_pca_mid}{{10.5}{282}}
\newlabel{fig:chpt10:mcca_right_u1}{{10.6(a)}{283}}
\newlabel{sub@fig:chpt10:mcca_right_u1}{{(a)}{283}}
\newlabel{fig:chpt10:mcca_right_u2}{{10.6(b)}{283}}
\newlabel{sub@fig:chpt10:mcca_right_u2}{{(b)}{283}}
\newlabel{fig:chpt10:mcca_right_u3}{{10.6(c)}{283}}
\newlabel{sub@fig:chpt10:mcca_right_u3}{{(c)}{283}}
\newlabel{fig:chpt10:mcca_right_overlay}{{10.6(d)}{283}}
\newlabel{sub@fig:chpt10:mcca_right_overlay}{{(d)}{283}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.6}{\ignorespaces (a)-(c) Left singular vectors of $Y_{\text  {right}}$ corresponding to the top 3 singular values. (d) Thresholded singular vectors from (a)-(c) overlayed onto the original scene. These pixels correspond to the flashing light sources visible in the right camera.\relax }}{283}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {$u_1$}}}{283}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {$u_2$}}}{283}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {$u_3$}}}{283}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {Overlay}}}{283}}
\newlabel{fig:chpt10:mcca_pca_right}{{10.6}{283}}
\newlabel{fig:chpt10:mcca_mid_u1}{{10.7(a)}{284}}
\newlabel{sub@fig:chpt10:mcca_mid_u1}{{(a)}{284}}
\newlabel{fig:chpt10:mcca_mid_overlay}{{10.7(b)}{284}}
\newlabel{sub@fig:chpt10:mcca_mid_overlay}{{(b)}{284}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.7}{\ignorespaces Top 3 correlations returned by MCCA and IMCCA.\relax }}{284}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {MCCA}}}{284}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {IMCCA}}}{284}}
\newlabel{fig:chpt10:mcca_corrs}{{10.7}{284}}
\newlabel{fig:chpt10:mcca_cca_left}{{10.8(a)}{285}}
\newlabel{sub@fig:chpt10:mcca_cca_left}{{(a)}{285}}
\newlabel{fig:chpt10:mcca_cca_mid}{{10.8(b)}{285}}
\newlabel{sub@fig:chpt10:mcca_cca_mid}{{(b)}{285}}
\newlabel{fig:chpt10:mcca_cca_right}{{10.8(c)}{285}}
\newlabel{sub@fig:chpt10:mcca_cca_right}{{(c)}{285}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.8}{\ignorespaces Top 2 thresholded MCCA canonical vectors overlayed onto the original scene. The red pixels are the pixels corresponding to the largest correlation and the green pixels correspond to the pixels with the second largest correlation. Since we are in the sample deficient regime, MCCA returns random pixels as the canonical vectors are random.\relax }}{285}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Left Camera}}}{285}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Middle Camera}}}{285}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Right Camera}}}{285}}
\newlabel{fig:chpt10:mcca_cca_vects}{{10.8}{285}}
\newlabel{fig:chpt10:mcca_icca_left}{{10.9(a)}{285}}
\newlabel{sub@fig:chpt10:mcca_icca_left}{{(a)}{285}}
\newlabel{fig:chpt10:mcca_icca_mid}{{10.9(b)}{285}}
\newlabel{sub@fig:chpt10:mcca_icca_mid}{{(b)}{285}}
\newlabel{fig:chpt10:mcca_icca_right}{{10.9(c)}{285}}
\newlabel{sub@fig:chpt10:mcca_icca_right}{{(c)}{285}}
\@writefile{lof}{\contentsline {figure}{\numberline {10.9}{\ignorespaces Top 2 thresholded IMCCA canonical vectors overlayed onto the original scene. The red pixels correspond to the largest correlation and the green pixels correspond to the second largest correlation. Clearly, the red pixels identify the shared flashing tablet light in all 3 views and the green pixels identify the shared flashing laptop in the left and right views.\relax }}{285}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Left Camera}}}{285}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Middle Camera}}}{285}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {Right Camera}}}{285}}
\newlabel{fig:chpt10:mcca_icca_vects}{{10.9}{285}}
\@writefile{toc}{\hbox { }}
\@writefile{toc}{\contentsline {chap}{\numberline {\hbox { }\hfill \bf  XI.\hspace  {5pt}}{\bf  Afterword}}{286}}
\@writefile{toc}{\hbox { }}
\newlabel{sec:fw}{{XI}{286}}
\@writefile{toc}{\contentsline {chapter}{APPENDICES}{288}}
\@writefile{loa}{\hbox { }}
\@writefile{loa}{\contentsline {appendix}{\numberline {A.}{\ignorespaces \rm  Proof of Theorem 2.5.1\hbox {}}}{289}}
\newlabel{sec:ieee_msd_appen}{{A}{289}}
\newlabel{eq:eval_master}{{{\rm  A}.1}{289}}
\newlabel{eq:eval_master2}{{{\rm  A}.2}{289}}
\citation{benaych2011eigenvalues}
\citation{benaych2011eigenvalues}
\newlabel{eq:t_trans}{{{\rm  A}.3}{290}}
\newlabel{eq:Mn}{{{\rm  A}.4}{290}}
\citation{silverstein1985smallest}
\citation{barvinok2005measure}
\@writefile{loa}{\hbox { }}
\@writefile{loa}{\contentsline {appendix}{\numberline {B.}{\ignorespaces \rm  Theoretical and Empirical MCCA Derivations}}{292}}
\newlabel{sec:mcca_derivs}{{B}{292}}
\newlabel{eq:1a_prob}{{{\rm  B}.1}{292}}
\newlabel{eq:prob1b}{{{\rm  B}.2}{294}}
\newlabel{eq:rho1b}{{{\rm  B}.3}{294}}
\newlabel{eq:2a_grad}{{{\rm  B}.4}{298}}
\newlabel{eq:genvar_cost}{{{\rm  B}.5}{309}}
\newlabel{eq:genvar_grad}{{{\rm  B}.6}{309}}
\citation{bao2014canonical}
\@writefile{loa}{\hbox { }}
\@writefile{loa}{\contentsline {appendix}{\numberline {C.}{\ignorespaces \rm  Derivations of Empirical CCA Canonical Correlations and Accuracy of Canonical Vectors}}{312}}
\newlabel{eq:appen3:data_model}{{{\rm  C}.1}{312}}
\citation{bao2014canonical}
\newlabel{eq:true_scm}{{{\rm  C}.2}{313}}
\newlabel{eq:bao}{{{\rm  C}.3}{313}}
\newlabel{eq:cca_target}{{{\rm  C}.4}{315}}
\newlabel{eq:cca_reduced}{{{\rm  C}.5}{316}}
\newlabel{eq:M}{{{\rm  C}.6}{318}}
\newlabel{eq:det}{{{\rm  C}.7}{319}}
\newlabel{eq:stiel}{{{\rm  C}.8}{320}}
\newlabel{eq:r_trans}{{{\rm  C}.9}{321}}
\newlabel{eq:blue}{{{\rm  C}.10}{321}}
\newlabel{eq:r_plus}{{{\rm  C}.11}{321}}
\newlabel{eq:f_lam}{{{\rm  C}.12}{322}}
\newlabel{eq:h_lam}{{{\rm  C}.13}{323}}
\newlabel{eq:cca_ip}{{{\rm  C}.14}{325}}
\newlabel{eq:cca_ip_red}{{{\rm  C}.15}{326}}
\citation{benaych2012singular}
\newlabel{eq:cca_wx_acc}{{{\rm  C}.16}{327}}
\newlabel{eq:utt}{{{\rm  C}.17}{328}}
\newlabel{eq:utt_num}{{{\rm  C}.18}{328}}
\newlabel{eq:utt_den}{{{\rm  C}.19}{328}}
\newlabel{eq:R_eigv}{{{\rm  C}.20}{329}}
\citation{johnstone2008multivariate}
\@writefile{loa}{\hbox { }}
\@writefile{loa}{\contentsline {appendix}{\numberline {D.}{\ignorespaces \rm  Significance Test for Canonical Correlations}}{331}}
\newlabel{eq:c_cca}{{{\rm  D}.1}{331}}
\newlabel{eq:c}{{{\rm  D}.2}{331}}
\citation{johnstone2008multivariate}
\citation{johnstone2008multivariate}
\citation{johnstone2008multivariate}
\newlabel{prop:appen4:john}{{D.2.1}{332}}
\newlabel{eq:mu_sigma}{{{\rm  D}.4}{332}}
\newlabel{th:icca_sig_real}{{D.2.2}{332}}
\newlabel{th:icca_sig_imag}{{D.2.1}{333}}
\@writefile{lot}{\contentsline {table}{\numberline {{\rm  D}.1}{\ignorespaces Parameters for distributions of ICCA correlation coefficients. See Table {\rm  D}.2\hbox {} for related parameters necessary for computation.\relax }}{334}}
\newlabel{table:appen4:params}{{{\rm  D}.1}{334}}
\@writefile{lot}{\contentsline {table}{\numberline {{\rm  D}.2}{\ignorespaces Related parameters for distributions of ICCA correlation coefficients presented in Table {\rm  D}.1\hbox {}.\relax }}{334}}
\newlabel{table:appen4:params2}{{{\rm  D}.2}{334}}
\citation{nadakuditi2008sample}
\citation{nadakuditi2008sample}
\bibstyle{IEEEtran}
\bibdata{IEEEabrv,thesis_bib,taes_msd/taes_useful,ieee_msd/IEEE_RMT_MSD_bib}
\@writefile{lot}{\contentsline {table}{\numberline {{\rm  D}.3}{\ignorespaces Percentiles of the Tracy-Widom real and complex distribution.\relax }}{335}}
\newlabel{table:tw_perc}{{{\rm  D}.3}{335}}
\newlabel{fig:icca_sig1_real_cdf}{{4.1(a)}{336}}
\newlabel{sub@fig:icca_sig1_real_cdf}{{(a)}{336}}
\newlabel{fig:icca_sig2_real_cdf}{{4.1(b)}{336}}
\newlabel{sub@fig:icca_sig2_real_cdf}{{(b)}{336}}
\newlabel{fig:icca_sig3_real_cdf}{{4.1(c)}{336}}
\newlabel{sub@fig:icca_sig3_real_cdf}{{(c)}{336}}
\newlabel{fig:icca_sig4_real_cdf}{{4.1(d)}{336}}
\newlabel{sub@fig:icca_sig4_real_cdf}{{(d)}{336}}
\newlabel{fig:icca_sig5_real_cdf}{{4.1(e)}{336}}
\newlabel{sub@fig:icca_sig5_real_cdf}{{(e)}{336}}
\newlabel{fig:icca_sig6_real_cdf}{{4.1(f)}{336}}
\newlabel{sub@fig:icca_sig6_real_cdf}{{(f)}{336}}
\newlabel{fig:icca_sig7_real_cdf}{{4.1(g)}{336}}
\newlabel{sub@fig:icca_sig7_real_cdf}{{(g)}{336}}
\newlabel{fig:icca_sig8_real_cdf}{{4.1(h)}{336}}
\newlabel{sub@fig:icca_sig8_real_cdf}{{(h)}{336}}
\newlabel{fig:icca_sig9_real_cdf}{{4.1(i)}{336}}
\newlabel{sub@fig:icca_sig9_real_cdf}{{(i)}{336}}
\@writefile{lof}{\contentsline {figure}{\numberline {{\rm  D}.1}{\ignorespaces Empirical and theoretically predicted cumulative distribution functions (cdf) for ICCA under various parameters $k_x$, $k_y$ and $n$ for real valued $X$ and $Y$.\relax }}{336}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{336}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{336}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{336}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{336}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {}}}{336}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {}}}{336}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(g)}{\ignorespaces {}}}{336}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(h)}{\ignorespaces {}}}{336}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(i)}{\ignorespaces {}}}{336}}
\newlabel{fig:icca_sig_real_cdf}{{{\rm  D}.1}{336}}
\newlabel{fig:icca_sig1_real_pdf}{{4.2(a)}{337}}
\newlabel{sub@fig:icca_sig1_real_pdf}{{(a)}{337}}
\newlabel{fig:icca_sig2_real_pdf}{{4.2(b)}{337}}
\newlabel{sub@fig:icca_sig2_real_pdf}{{(b)}{337}}
\newlabel{fig:icca_sig3_real_pdf}{{4.2(c)}{337}}
\newlabel{sub@fig:icca_sig3_real_pdf}{{(c)}{337}}
\newlabel{fig:icca_sig4_real_pdf}{{4.2(d)}{337}}
\newlabel{sub@fig:icca_sig4_real_pdf}{{(d)}{337}}
\newlabel{fig:icca_sig5_real_pdf}{{4.2(e)}{337}}
\newlabel{sub@fig:icca_sig5_real_pdf}{{(e)}{337}}
\newlabel{fig:icca_sig6_real_pdf}{{4.2(f)}{337}}
\newlabel{sub@fig:icca_sig6_real_pdf}{{(f)}{337}}
\newlabel{fig:icca_sig7_real_pdf}{{4.2(g)}{337}}
\newlabel{sub@fig:icca_sig7_real_pdf}{{(g)}{337}}
\newlabel{fig:icca_sig8_real_pdf}{{4.2(h)}{337}}
\newlabel{sub@fig:icca_sig8_real_pdf}{{(h)}{337}}
\newlabel{fig:icca_sig9_real_pdf}{{4.2(i)}{337}}
\newlabel{sub@fig:icca_sig9_real_pdf}{{(i)}{337}}
\@writefile{lof}{\contentsline {figure}{\numberline {{\rm  D}.2}{\ignorespaces Empirical and theoretically predicted probability density functions (pdf) for ICCA under various parameters $k_x$, $k_y$ and $n$ for real valued $X$ and $Y$.\relax }}{337}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{337}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{337}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{337}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{337}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {}}}{337}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {}}}{337}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(g)}{\ignorespaces {}}}{337}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(h)}{\ignorespaces {}}}{337}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(i)}{\ignorespaces {}}}{337}}
\newlabel{fig:icca_sig_real_pdf}{{{\rm  D}.2}{337}}
\newlabel{fig:icca_sig1_imag_cdf}{{4.3(a)}{338}}
\newlabel{sub@fig:icca_sig1_imag_cdf}{{(a)}{338}}
\newlabel{fig:icca_sig2_imag_cdf}{{4.3(b)}{338}}
\newlabel{sub@fig:icca_sig2_imag_cdf}{{(b)}{338}}
\newlabel{fig:icca_sig3_imag_cdf}{{4.3(c)}{338}}
\newlabel{sub@fig:icca_sig3_imag_cdf}{{(c)}{338}}
\newlabel{fig:icca_sig4_imag_cdf}{{4.3(d)}{338}}
\newlabel{sub@fig:icca_sig4_imag_cdf}{{(d)}{338}}
\newlabel{fig:icca_sig5_imag_cdf}{{4.3(e)}{338}}
\newlabel{sub@fig:icca_sig5_imag_cdf}{{(e)}{338}}
\newlabel{fig:icca_sig6_imag_cdf}{{4.3(f)}{338}}
\newlabel{sub@fig:icca_sig6_imag_cdf}{{(f)}{338}}
\newlabel{fig:icca_sig7_imag_cdf}{{4.3(g)}{338}}
\newlabel{sub@fig:icca_sig7_imag_cdf}{{(g)}{338}}
\newlabel{fig:icca_sig8_imag_cdf}{{4.3(h)}{338}}
\newlabel{sub@fig:icca_sig8_imag_cdf}{{(h)}{338}}
\newlabel{fig:icca_sig9_imag_cdf}{{4.3(i)}{338}}
\newlabel{sub@fig:icca_sig9_imag_cdf}{{(i)}{338}}
\@writefile{lof}{\contentsline {figure}{\numberline {{\rm  D}.3}{\ignorespaces Empirical and theoretically predicted cumulative distribution functions (cdf) for ICCA under various parameters $k_x$, $k_y$ and $n$ for complex valued $X$ and $Y$.\relax }}{338}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{338}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{338}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{338}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{338}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {}}}{338}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {}}}{338}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(g)}{\ignorespaces {}}}{338}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(h)}{\ignorespaces {}}}{338}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(i)}{\ignorespaces {}}}{338}}
\newlabel{fig:icca_sig_imag_cdf}{{{\rm  D}.3}{338}}
\newlabel{fig:icca_sig1_imag_pdf}{{4.4(a)}{339}}
\newlabel{sub@fig:icca_sig1_imag_pdf}{{(a)}{339}}
\newlabel{fig:icca_sig2_imag_pdf}{{4.4(b)}{339}}
\newlabel{sub@fig:icca_sig2_imag_pdf}{{(b)}{339}}
\newlabel{fig:icca_sig3_imag_pdf}{{4.4(c)}{339}}
\newlabel{sub@fig:icca_sig3_imag_pdf}{{(c)}{339}}
\newlabel{fig:icca_sig4_imag_pdf}{{4.4(d)}{339}}
\newlabel{sub@fig:icca_sig4_imag_pdf}{{(d)}{339}}
\newlabel{fig:icca_sig5_imag_pdf}{{4.4(e)}{339}}
\newlabel{sub@fig:icca_sig5_imag_pdf}{{(e)}{339}}
\newlabel{fig:icca_sig6_imag_pdf}{{4.4(f)}{339}}
\newlabel{sub@fig:icca_sig6_imag_pdf}{{(f)}{339}}
\newlabel{fig:icca_sig7_imag_pdf}{{4.4(g)}{339}}
\newlabel{sub@fig:icca_sig7_imag_pdf}{{(g)}{339}}
\newlabel{fig:icca_sig8_imag_pdf}{{4.4(h)}{339}}
\newlabel{sub@fig:icca_sig8_imag_pdf}{{(h)}{339}}
\newlabel{fig:icca_sig9_imag_pdf}{{4.4(i)}{339}}
\newlabel{sub@fig:icca_sig9_imag_pdf}{{(i)}{339}}
\@writefile{lof}{\contentsline {figure}{\numberline {{\rm  D}.4}{\ignorespaces Empirical and theoretically predicted probability density functions (pdf) for ICCA under various parameters $k_x$, $k_y$ and $n$ for complex valued $X$ and $Y$.\relax }}{339}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{339}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{339}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{339}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{339}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(e)}{\ignorespaces {}}}{339}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(f)}{\ignorespaces {}}}{339}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(g)}{\ignorespaces {}}}{339}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(h)}{\ignorespaces {}}}{339}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(i)}{\ignorespaces {}}}{339}}
\newlabel{fig:icca_sig_imag_pdf}{{{\rm  D}.4}{339}}
\newlabel{fig:icca_conv1_real_pf}{{4.5(a)}{340}}
\newlabel{sub@fig:icca_conv1_real_pf}{{(a)}{340}}
\newlabel{fig:icca_conv1_real_ae}{{4.5(b)}{340}}
\newlabel{sub@fig:icca_conv1_real_ae}{{(b)}{340}}
\newlabel{fig:icca_conv2_real_pf}{{4.5(c)}{340}}
\newlabel{sub@fig:icca_conv2_real_pf}{{(c)}{340}}
\newlabel{fig:icca_conv2_real_ae}{{4.5(d)}{340}}
\newlabel{sub@fig:icca_conv2_real_ae}{{(d)}{340}}
\@writefile{lof}{\contentsline {figure}{\numberline {{\rm  D}.5}{\ignorespaces Convergence plots for the false alarm rate of the proposed ICCA test statistic for real data. The false alarm rate is plotted as a function of $n$ for fixed $k_x/n$, $k_y/n$. The black line shows the desired false alarm rate. The absolute error is also plotted. We show plots for $\alpha =0.05$ and $\alpha =0.01$. We show convergence plots when using the test statistic with and without the correction term.\relax }}{340}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{340}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{340}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{340}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{340}}
\newlabel{fig:icca_conv_real}{{{\rm  D}.5}{340}}
\newlabel{fig:icca_conv1_imag_pf}{{4.6(a)}{341}}
\newlabel{sub@fig:icca_conv1_imag_pf}{{(a)}{341}}
\newlabel{fig:icca_conv1_imag_ae}{{4.6(b)}{341}}
\newlabel{sub@fig:icca_conv1_imag_ae}{{(b)}{341}}
\newlabel{fig:icca_conv2_imag_pf}{{4.6(c)}{341}}
\newlabel{sub@fig:icca_conv2_imag_pf}{{(c)}{341}}
\newlabel{fig:icca_conv2_imag_ae}{{4.6(d)}{341}}
\newlabel{sub@fig:icca_conv2_imag_ae}{{(d)}{341}}
\@writefile{lof}{\contentsline {figure}{\numberline {{\rm  D}.6}{\ignorespaces Convergence plots for the false alarm rate of the proposed ICCA test statistic for complex data. The false alarm rate is plotted as a function of $n$ for fixed $k_x/n$, $k_y/n$. The black line shows the desired false alarm rate. The absolute error is also plotted. We show plots for $\alpha =0.05$ and $\alpha =0.01$. We show convergence plots when using the test statistic with and without the correction term.\relax }}{341}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{341}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{341}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{341}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{341}}
\newlabel{fig:icca_conv_imag}{{{\rm  D}.6}{341}}
\newlabel{fig:pca_conv1_real_pf}{{4.7(a)}{342}}
\newlabel{sub@fig:pca_conv1_real_pf}{{(a)}{342}}
\newlabel{fig:pca_conv1_real_ae}{{4.7(b)}{342}}
\newlabel{sub@fig:pca_conv1_real_ae}{{(b)}{342}}
\newlabel{fig:pca_conv2_real_pf}{{4.7(c)}{342}}
\newlabel{sub@fig:pca_conv2_real_pf}{{(c)}{342}}
\newlabel{fig:pca_conv2_real_ae}{{4.7(d)}{342}}
\newlabel{sub@fig:pca_conv2_real_ae}{{(d)}{342}}
\@writefile{lof}{\contentsline {figure}{\numberline {{\rm  D}.7}{\ignorespaces Convergence plots for the false alarm rate of the PCA test statistic for real data. The false alarm rate is plotted as a function of $n$ for fixed $p/n$. The black line shows the desired false alarm rate. The absolute error is also plotted. We show plots for $\alpha =0.05$ and $\alpha =0.01$. We show convergence plots when using the test statistic with and without the correction term.\relax }}{342}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{342}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{342}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{342}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{342}}
\newlabel{fig:pca_conv_real}{{{\rm  D}.7}{342}}
\newlabel{fig:pca_conv1_imag_pf}{{4.8(a)}{343}}
\newlabel{sub@fig:pca_conv1_imag_pf}{{(a)}{343}}
\newlabel{fig:pca_conv1_imag_ae}{{4.8(b)}{343}}
\newlabel{sub@fig:pca_conv1_imag_ae}{{(b)}{343}}
\newlabel{fig:pca_conv2_imag_pf}{{4.8(c)}{343}}
\newlabel{sub@fig:pca_conv2_imag_pf}{{(c)}{343}}
\newlabel{fig:pca_conv2_imag_ae}{{4.8(d)}{343}}
\newlabel{sub@fig:pca_conv2_imag_ae}{{(d)}{343}}
\@writefile{lof}{\contentsline {figure}{\numberline {{\rm  D}.8}{\ignorespaces Convergence plots for the false alarm rate of the PCA test statistic for complex data. The false alarm rate is plotted as a function of $n$ for fixed $p/n$. The black line shows the desired false alarm rate. The absolute error is also plotted. We show plots for $\alpha =0.05$ and $\alpha =0.01$. We show convergence plots when using the test statistic with and without the correction term.\relax }}{343}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {}}}{343}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {}}}{343}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(c)}{\ignorespaces {}}}{343}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(d)}{\ignorespaces {}}}{343}}
\newlabel{fig:pca_conv_imag}{{{\rm  D}.8}{343}}
\@writefile{toc}{\contentsline {chapter}{BIBLIOGRAPHY}{344}}
\bibcite{fawcett2006introduction}{1}
\bibcite{bao2014canonical}{2}
\bibcite{rao2008polynomial}{3}
\bibcite{hotelling1936relations}{4}
\bibcite{gunderson1997estimating}{5}
\bibcite{pezeshki2004empirical}{6}
\bibcite{ge2009does}{7}
\bibcite{nadakuditi2011fundamental}{8}
\bibcite{vinod1976canonical}{9}
\bibcite{thum2014supervised}{10}
\bibcite{cruz2014fast}{11}
\bibcite{tenenhaus2014regularized}{12}
\bibcite{akaho2006kernel}{13}
\bibcite{welling2005kcca}{14}
\bibcite{waaijenborg2009correlating}{15}
\bibcite{mandal2013non}{16}
\bibcite{chang2013canonical}{17}
\bibcite{hardoon2011sparse}{18}
\bibcite{yan2014accelerating}{19}
\bibcite{sun2013canonical}{20}
\bibcite{shin2015canonical}{21}
\bibcite{tao2014exploring}{22}
\bibcite{gao2014efficient}{23}
\bibcite{zhang2013binary}{24}
\bibcite{witten2009penalized}{25}
\bibcite{klami2013bayesian}{26}
\bibcite{chu2013sparse}{27}
\bibcite{hardoon2004canonical}{28}
\bibcite{dhillon2011multi}{29}
\bibcite{melzer2001nonlinear}{30}
\bibcite{zhai2015instance}{31}
\bibcite{lisanti2014matching}{32}
\bibcite{ahsan2014clustering}{33}
\bibcite{hardoon2006correlation}{34}
\bibcite{chaudhuri2009multi}{35}
\bibcite{deleus2011functional}{36}
\bibcite{arbabshirani2010comparison}{37}
\bibcite{khalid2013improving}{38}
\bibcite{guccione2013functional}{39}
\bibcite{correa2010canonical}{40}
\bibcite{lin2013identifying}{41}
\bibcite{seoane2014canonical}{42}
\bibcite{lin2013group}{43}
\bibcite{zhang2013l1}{44}
\bibcite{nakanishi2014enhancing}{45}
\bibcite{zhang2014frequency}{46}
\bibcite{spuler2013spatial}{47}
\bibcite{campi2013non}{48}
\bibcite{chen2014removal}{49}
\bibcite{kuzilek2014comparison}{50}
\bibcite{via2005canonical}{51}
\bibcite{pezeshki2006canonical}{52}
\bibcite{scharf1998wiener}{53}
\bibcite{li2009joint}{54}
\bibcite{nielsen2002multiset}{55}
\bibcite{scharf2000canonical}{56}
\bibcite{manco2014kernel}{57}
\bibcite{todros2012measure}{58}
\bibcite{torres2007finding}{59}
\bibcite{wilks2014probabilistic}{60}
\bibcite{prera2014using}{61}
\bibcite{steward2014assimilating}{62}
\bibcite{li2010canonical}{63}
\bibcite{kettenring1971canonical}{64}
\bibcite{nielsen1994analysis}{65}
\bibcite{scharf1991statistical}{66}
\bibcite{friedman2001elements}{67}
\bibcite{besson2005matched}{68}
\bibcite{besson2006cfar}{69}
\bibcite{bandiera2007adaptive}{70}
\bibcite{bandiera2007glrt}{71}
\bibcite{maris2003resampling}{72}
\bibcite{soong1995principal}{73}
\bibcite{scharf1994matched}{74}
\bibcite{vincent2008matched}{75}
\bibcite{mcwhorter2003matched}{76}
\bibcite{jin2005cfar}{77}
\bibcite{elden2007matrix}{78}
\bibcite{hwritingurl}{79}
\bibcite{thai2002invariant}{80}
\bibcite{healey1999models}{81}
\bibcite{kwon2006kernel}{82}
\bibcite{nadakuditi2008sample}{83}
\bibcite{paul2007asymptotics}{84}
\bibcite{benaych2011eigenvalues}{85}
\bibcite{benaych2011singular}{86}
\bibcite{asendorf2011msd}{87}
\bibcite{zhu2006automatic}{88}
\bibcite{nadakuditi2010fundamental}{89}
\bibcite{johnstone2001distribution}{90}
\bibcite{el2007tracy}{91}
\bibcite{muirhead1982aspects}{92}
\bibcite{van1968detection}{93}
\bibcite{cox1973resolving}{94}
\bibcite{wood1993saddlepoint}{95}
\bibcite{cui2013performance}{96}
\bibcite{arribas2013antenna}{97}
\bibcite{gorji2013widely}{98}
\bibcite{zhou2013space}{99}
\bibcite{santiago2013noise}{100}
\bibcite{hu2013doa}{101}
\bibcite{liao2013direction}{102}
\bibcite{chen2013adaptive}{103}
\bibcite{kwon2013multi}{104}
\bibcite{sirianunpiboon2013multiple}{105}
\bibcite{vazquez2011spatial}{106}
\bibcite{asendorf2013performance}{107}
\bibcite{balzano2010high}{108}
\bibcite{he2013near}{109}
\bibcite{fuchs2007robust}{110}
\bibcite{yu2007learning}{111}
\bibcite{hardle2007applied}{112}
\bibcite{bartlett1954note}{113}
\bibcite{constantine1976asymptotic}{114}
\bibcite{johnstone2008multivariate}{115}
\bibcite{benaych2012singular}{116}
\bibcite{nadakuditi2014optshrink}{117}
\bibcite{latala2005some}{118}
\bibcite{wilms2013sparse}{119}
\bibcite{singanamalli2014supervised}{120}
\bibcite{lin2014correspondence}{121}
\bibcite{doscanonical}{122}
\bibcite{vilsaint2013ecology}{123}
\bibcite{travis2014creativity}{124}
\bibcite{chen2010new}{125}
\bibcite{gu20082}{126}
\bibcite{gu2007joint}{127}
\bibcite{kikuchi2006pair}{128}
\bibcite{diamantaras1994cross}{129}
\bibcite{worsley2005comparing}{130}
\bibcite{nadakuditi2007thesis}{131}
\bibcite{belabbas2007fast}{132}
\bibcite{gu1996efficient}{133}
\bibcite{rudelson2007sampling}{134}
\bibcite{hehyperspectral}{135}
\bibcite{rokhlin2009randomized}{136}
\bibcite{halko2011algorithm}{137}
\bibcite{achlioptas2007fast}{138}
\bibcite{arora2006fast}{139}
\bibcite{liberty2007randomized}{140}
\bibcite{ramachandra2011compressive}{141}
\bibcite{halko2011finding}{142}
\bibcite{candes2006near}{143}
\bibcite{donoho2006compressed}{144}
\bibcite{smeulders2000content}{145}
\bibcite{yang2007evaluating}{146}
\bibcite{lowe1999object}{147}
\bibcite{solem2012programming}{148}
\bibcite{leong2010text}{149}
\bibcite{rashtchian2010collecting}{150}
\bibcite{feng2008automatic}{151}
\bibcite{schmid1994probabilistic}{152}
\bibcite{vinograde1950canonical}{153}
\bibcite{steel1951minimum}{154}
\bibcite{horst1961relations}{155}
\bibcite{horst1961generalized}{156}
\bibcite{bach2003kernel}{157}
\bibcite{boumal2013manopt}{158}
\bibcite{silverstein1985smallest}{159}
\bibcite{barvinok2005measure}{160}
