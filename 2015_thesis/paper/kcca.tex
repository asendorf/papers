CCA is a linear algorithm, relying on the assumption that features of each dataset are
linearly correlated. Thus, CCA searches for linear transformations, $\xI$ and $\xII$, for
each datasets. However, if there are nonlinear correlations present between the datasets,
these linear transformations found by CCA may not discover such correlations. To overcome
this limitation of CCA, a kernel version of CCA (KCCA) was developed separately in
\cite{akaho2006kernel,melzer2001kernel}.

Similar to other kernel algorithms such as kernel principal component analysis (KPCA) and
kernel support vector machines (KSVM), KCCA first uses a nonlinear mapping to transform
observations to a higher dimensional kernel space. Linear CCA is then performed in this
higher dimensional kernel space. Thankfully, the kernel-trick is used so that this mapping
is done implicitly; the resulting computational time depends only on the size of the
training set and not on the dimensions of the kernel space. This allows us to work with
kernel spaces of arbitrarily high or infinite dimension. The hope of the kernel method is
that the transformed features are linearly correlated in the high dimensional kernel
space.

KCCA has been used in applications such as image pose estimation \cite{melzer2001kernel}
and handwritten digit recognition \cite{yu2007learning}. KCCA has also been applied to
independent component analysis (ICA) \cite{fyfe2000ica} and extended to a weighted version
used for multiple datasets \cite{yu2007learning}. However, the performance of KCCA has not
been extensively studied. In this section, we first provide a derivation of KCCA and give
a closed form SVD solution to find the canonical vectors and correlation coefficients. The
derivation suggests that KCCA will have similar performance issues as CCA and RCCA. To
possibly avoid this performance loss, we apply the idea of informative data fusion used
for ICCA and IRCCA to derive an informative version of KCCA (IKCCA). The performance
analysis of KCCA and IKCCA is left to the thesis.

\section{KCCA Derivation}

We provide a derivation of KCCA for completeness, following \cite{welling2005kcca}, which
provides an excellent first principles discussion of KCCA. KCCA is inherently empirical,
as it relies on training data. We proceed using the previous assumption that we are
presented with $n$ observations of each dataset that we stack into data matrices $Y_1$ and
$Y_2$. We represent the nonlinear data mappings with the functions
$\Phi\left(\cdot\right):\complex^{d_1}\to\complex^{d_{k_1}}$ and
$\Psi\left(\cdot\right):\complex^{d_2}\to\complex^{d_{k_2}}$ where $d_{k_1}$ and $d_{k_2}$
are the dimensions of the first and second kernel spaces. We use these transformations to
map our training data to the kernel space via,
\begin{equation*}
  \yI^{(i)}\to\Phi\left(\yI^{(i)}\right),\,\,\,\,\,\yII^{(i)}\to\Psi\left(\yII^{(i)}\right).
\end{equation*}
Using the observations in kernel space, we then perform empirical CCA. Instead of using
the training data matrices, we use the kernel data matrices,
$\Phi(Y_1)=\left[\Phi\left(\yI^{(1)}\right),\dots,\Phi\left(\yI^{(n)}\right)\right]$ and
$\Psi(Y_2)=\left[\Psi\left(\yI^{(1)}\right),\dots,\Psi\left(\yI^{(n)}\right)\right]$ to
form the sample covariance matrices in kernel space. Substituting these expressions into
(\ref{eq:cca_lagr}), the KCCA Lagrangian is 
\beq{\label{eq:kcca_lagr}}\ba &
L(x_1,x_2,\lambda_1,\lambda_2) &&= &&&\xI^H\RIIIhat\xII - \lambda_1\left(\xI^H\RIhat\xI -
  1\right) -\lambda_2\left(\xII^H\RIIhat\xII - 1\right)\\
&&& = &&&\xI^H\Phi(Y_1)\Psi(Y_2)^H\xII - \lambda_1\left(\xI^H\Phi(Y_1)\Phi(Y_1)^H\xI -
  1\right) \\
&&&&&&-\lambda_2\left(\xII^H\Psi(Y_2)\Psi(Y_2)^H\xII - 1\right),\\
\ea\eeq 
where $\xI\in\complex^{d_{k_1}}$ and $\xII\in\complex^{d_{k_2}}$. Typically, $d_{k_1}$ and
$d_{k_2}$ are very large (possibly infinite) in the hopes that data dependencies become
linear in the high dimensional kernel space. If the dimensionality of the kernel space is
larger than the number of observations, the canonical coefficient vectors must lie in the
span of the mapped observations:
\begin{equation*}
  \xI = \Phi(Y_1)\alpha,\,\,\,\,\,\xII=\Psi(Y_2)\beta
\end{equation*}
where $\alpha=\left[\alpha_1,\dots,\alpha_n\right]^H\in\complex^n$ and
$\beta=\left[\beta_1,\dots,\beta_n\right]^H\in\complex^n$ are weight vectors. Using this fact, the
Lagrangian in (\ref{eq:kcca_lagr}) becomes
\beq\label{eq:kcca_lagr2}\ba
&L(\alpha,\beta,\lambda_1,\lambda_2) =
&&\alpha^H\Phi(Y_1)^H\Phi(Y_1)\Psi(Y_2)^H\Psi(Y_2)\beta\\
&&&-\lambda_1\left(\alpha^H\Phi(Y_1)^H\Phi(Y_1)\Phi(Y_1)^H\Phi(Y_1)\alpha - 1\right) \\
&&&-\lambda_2\left(\beta^H\Psi(Y_2)^H\Psi(Y_2)\Psi(Y_2)^H\Psi(Y_2)\beta - 1\right).\\
\ea\eeq

Defin $K_1=\Phi(Y_1)^H\Phi(Y_1)$ and $K_2 = \Psi(Y_2)^H\Psi(Y_2)$ as the kernel
matrices, which are Gram matrices of the observations in kernel space. The entries of
$K_1$ are $\Phi\left(\yI^{(i)}\right)^H\Phi\left(\yI^{(j)}\right)$ and the entries of
$K_2$ are $\Psi\left(\yII^{(i)}\right)^H\Psi\left(\yII^{(j)}\right)$. We introduce the
kernel trick by letting 
\be\ba
&k_1\left(\yI^{(i)},\yI^{(j)}\right) =
\Phi\left(\yI^{(i)}\right)^H\Phi\left(\yI^{(j)}\right)\\
&k_2\left(\yII^{(i)},\yII^{(j)}\right) =
\Psi\left(\yII^{(i)}\right)^H\Psi\left(\yII^{(j)}\right).\\ 
\ea\ee
The kernel functions $k_1(\cdot):\complex^n\times\complex^n\to\complex$ and
$k_2(\cdot):\complex^n\times\complex^n\to\complex$  are typically easily computable,
nonlinear functions. Therefore, the kernel matrices, $K_1$ and $K_2$ can be computed
without mapping the observations into the kernel spaces.

Using this notation, (\ref{eq:kcca_lagr2}) simplifies to 
\beq
L(\alpha,\beta,\lambda_1,\lambda_2) = \alpha^HK_1K_2\beta -
\lambda_1\left(\alpha^HK_1K_1\alpha -
  1\right) -\lambda_2\left(\beta^HK_2K_2\beta - 1\right).\\
  \eeq
After taking partial derivatives with respect to $\alpha$ and $\beta$, we achieve
  the following system
\begin{equation*}
  \begin{aligned}
    &K_1K_2\beta = 2\lambda_1 K_1^2\alpha\\
    &K_2K_1\alpha = 2\lambda_2 K_2^2\beta.\\
  \end{aligned}
\end{equation*}
It is easily shown that $\rho=2\lambda_1=2\lambda_2$ as in CCA by multiplying the first
equation by $\alpha^H$ and the second by $\beta^H$. Assuming that $K_1$ is invertible,
this system yields the solution $\alpha =\rho^{-1}K_1^{-1}K_2\beta$ implying that
$K_2^2\beta=\rho^2K_2^2\beta$, which always has a solution for $\rho=1$. However,
this implies that the solution represents maximal correlation between the two datasets in
the kernel spaces, which is not necessarily true. To overcome this overfitting, it is
common to use a regularization parameter $\eta$ that penalizes the $\ell_2$ norm of
$\alpha$ and $\beta$. The
regularized Lagrangian is
\begin{equation*}
  L(\alpha,\beta,\lambda_1,\lambda_2) = \alpha^H K_1K_2\beta - \frac{\rho}{2}(\alpha^HK_1^2\alpha+\eta\|\alpha\|^2 -1) - \frac{\rho}{2}(\beta^HK_2^2\beta+\eta\|\beta\|^2-1).
\end{equation*}
After taking partial derivatives, we achieve the following system
\begin{equation*}
  \begin{aligned}
    &K_1K_2\beta = \rho (K_1^2+\eta I_n)\alpha\\
    &K_2K_1\alpha = \rho(K_2^2+\eta I_n)\beta.\\
  \end{aligned}
\end{equation*}
Solving this system yields the following eigenvalue problem
\begin{equation}\label{eq:kcca_eig}
  (K_1^2+\eta I_n)^{-1}K_1K_2(K_2^2+\eta I_n)^{-1}K_2K_1\alpha =\rho^2\alpha.
\end{equation}
Using a similarity transform as in CCA and RCCA, we let $f=(K_1^2+\eta I_n)^{1/2}\alpha$
and $g=(K_2^2+\eta I_n)^{1/2}\beta$ and define
\beq\label{eq:ckcca}
\Ckcca = (K_1^2+\eta I_n)^{-1/2}K_1K_2(K_2^2+\eta I_n)^{-1/2}.
\eeq
Let $FKG^H$ be the SVD of $\Ckcca$ where $F=\left[f_1,\dots, f_n\right]$,
  $K=\diag(k_1,\dots,k_n)$, $G=\left[g_1,\dots,g_n\right]$ so that the solution of KCCA is
\beq\label{eq:kcca_sol}\ba
& \rho = k_1\\
& \alpha = (K_1^2+\eta I_n)^{-1/2} f\\
& \beta  = (K_2^2+\eta I_n)^{-1/2}g.\\
\ea\eeq

The canonical vectors in the kernel spaces, $\xI$ and $\xII$, are not necessarily
computable without knowing the implicit nonlinear mappings $\Phi$ and $\Psi$. We may
simplify this solution using the SVDs of the kernel matrices. Let $K_1=U_1\Sigma_1U_1^H$
and $K_2=U_2\Sigma_2U_2^H$ be the SVDs of our kernel matrices. Because kernel matrices are
always symmetric and positive semi-definite, the SVD is the same as the eigenvalue
decomposition. Using this decomposition, (\ref{eq:ckcca}) becomes
\beq\label{eq:ckcca_svd}\ba
&\Ckcca &&= (U_1\Sigma_1U_1^HU_1\Sigma_1U_1^H+\eta
I_n)^{-1/2}U_1\Sigma_1U_1^HU_2\Sigma_2U_2^H(U_2\Sigma_2U_2^HU_2\Sigma_2U_2^H+\eta
I_n)^{-1/2}\\
&&& = U_1(\Sigma_1^2+\eta I_n)^{-1/2}\Sigma_1U_1^HU_2\Sigma_2(\Sigma_2^2+\eta
I_n)^{-1/2}U_2^H\\
\ea\eeq Notice that $(\Sigma_1^2+\eta I_n)^{-1/2}$ and $(\Sigma_2^2+\eta I_n)^{-1/2}$ are
easily computable as they are diagonal matrices. The KCCA correlation coefficient is
computed by
\begin{equation*}
  \rho= \sigma_1\left(\left(\Sigma_1^2+\eta
      I_n\right)^{-1/2}\Sigma_1U_1^HU_2\Sigma_2\left(\Sigma_2^2+\eta
      I_n\right)^{-1/2}\right).
\end{equation*}

\section{Informative KCCA (IKCCA)}

Motivated by informative versions of CCA and RCCA derived herein, we now derive an
informative version of KCCA (IKCCA). Proposition \ref{prop:raj} demonstrated that the
effective rank of $Y_1$ and $Y_2$ was much less than the inherent dimension of the feature
vectors. Similarly, when examining $\Ckcca$ in (\ref{eq:ckcca_svd}) we see that it uses
the full rank SVD of the kernel matrices $K_1$ and $K_2$, in particular, the full matrix
product $U_1^HU_2$. However, the effective
(informative) rank of these matrices is most certainly much less than the number of
samples $n$. In this spirit, we define the trimmed kernel data matrices as
\begin{equation*}
  \begin{aligned}
    &\widetilde{\Sigma}_1=\Sigma_1(1:r_1,1:r_1)\\
    &\widetilde{\Sigma}_2=\Sigma_2(1:r_2,1:r_2)\\
    &\widetilde{U}_1=U_1(:,1:r_1)\\
    &\widetilde{U}_2=U_2(:,1:r_2)\\
  \end{aligned}
\end{equation*}
where $r_1$ and $r_2$ are the number of informative components in each dataset. We
construct 
\be \Ckccatil= \widetilde{U}_1(\widetilde{\Sigma}_1^2+\eta
I_n)^{-1/2}\widetilde{\Sigma}_1\widetilde{U}_1^H\widetilde{U}_2\widetilde{\Sigma}_2(\widetilde{\Sigma}_2^2+\eta
I_n)^{-1/2}\widetilde{U}_2^H 
\ee
 and then take the SVD $\Ckccatil =
\widetilde{F}\widetilde{K}\widetilde{G}^H$. The IKCCA solution is
\beq\label{eq:ikcca_sol}\ba
& \widetilde{\rho} = \widetilde{k}_1\\
& \widetilde{\alpha} = (K_1^2+\eta I_n)^{-1/2} \widetilde{f}\\
& \widetilde{\beta}  = (K_2^2+\eta I_n)^{-1/2}\widetilde{g}.\\
\ea\eeq
We leave the performance analysis of KCCA and IKCCA to the thesis.
