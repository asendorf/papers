\begin{abstract}
%\boldmath
We consider a stochastic  matched subspace detection problem in high dimensions where the low-rank signal subspace is estimated by taking the eigenvalue decomposition of the sample covariance matrix of noisy labeled training data. In moderate to low signal-to-noise ratio (SNR) settings, the performance of typical plug-in detectors degrades because of subspace estimation errors due to limited noisy training samples. We use random matrix theory to derive an optimal matched subspace detector which accounts for these estimation errors. We develop an effective problem dimension, $k_{\text{eff}}$, which represents the number of signal eigenvalues above a critical threshold. An ROC analysis illustrates the improved performance of the optimal detector over the plug-in detector. We demonstrate that this improved performance results from the optimality of only using the effective number of dimensions. A theoretical derivation of ROC curves for each detector is also provided.
\end{abstract}

\section{Introduction}

\IEEEPARstart{M}{any} signal processing and machine learning problems involve training a detector to distinguish between a known number of classes. Subspace-based methods constitute a powerful and widely used class of algorithms for this purpose \cite{hastie2001elements,laaksonen1996subspace,scharf1994matched,jin2005cfar,mcwhorter2003matched}. Matched subspace detectors solve this problem when the signals can be modeled as residing on a low-dimensional subspace buried in noise. \cite{scharf1994matched,jin2005cfar,mcwhorter2003matched} all consider the case when this low-dimensional signal subspace is known. \cite{scharf1994matched} concludes that the generalized likelihood ratio test (GLRT) is optimal for solving matched subspace detection problems while \cite{mcwhorter2003matched} extend this work to the stochastic setting to conclude that the optimal detector in the known noise case is a matched subspace detector.

The focus of this paper will extend the stochastic matched subspace detector to the setting where the low-dimensional signal subspace is unknown. When labeled training data is available, an estimate of this low-dimensional signal subspace can be formed. Standard plug-in detectors form a GLRT by simply substituting this subspace estimate for the true signal subspace used in an oracle detector \cite{mcwhorter2003matched,jin2005cfar}. However, when the dataset is noisy and there are limited labeled training samples, these subspace estimates will also be noisy. Recent results from random matrix theory precisely quantify the errors in subspace estimation.  By deriving a matched subspace detector which systematically accounts for these subspace estimation errors, we realize improved detection ability over the standard plug-in detector.

The main contribution of this work is the development of effective dimension estimate which we call $k_{\text{eff}}$, defined to be the number of signal eigenvalues above a critical threshold. This estimate arises from the derivation of an optimal matched subspace detector which accounts for subspace estimation errors. This optimal detector improves overall detection ability compared to the standard plug-in detector. We show that $k_{\text{eff}}$ is the optimal dimension estimate for a matched subspace detector. $k_{\text{eff}}$, rather than individual dimension shrinkage (or re-weighting), is shown to be the reason why the optimal detector outperforms the plug-in detector.

The paper is organized as follows. Section \ref{sec:prob} formally states the detection problem and Section \ref{sec:main results} summarizes our main results. Section \ref{sec:params} discusses parameter estimation, using both maximum-likelihood and recent results from random matrix theory. Then, Section \ref{sec:detectors} mathematically derives an oracle, plug-in, and optimal matched subspace detector. Section \ref{sec:roc} discusses a saddlepoint approximation for a theoretical ROC curve for the plug-in and optimal detectors. Simulations highlight the importance of $k_{\text{eff}}$, which is discussed in Section \ref{sec:disc}. Concluding remarks are provided in Section \ref{sec:concl}.


\section{Problem Statement}\label{sec:prob}
We consider the detection problem where our observed data, $y$, may be one of two classes, $H_0$ or $H_1$. We may either observe signal in the presence of noise ($H_1$), or simply noise itself ($H_0$). Our setup is as follows:

\begin{equation}\label{eq:prob state}
y=\left\{
\begin{aligned}
&z
&& y\in H_0\\
&U_1x+z
&& y\in H_1\\
\end{aligned}\right.
\end{equation}

where $z\sim\mathcal{N}(0,I)$, $U_1\in\reals^{n\times k}$ is unknown with orthonormal columns, and $x\sim\mathcal{N}(0,\Sigma_1)$ where $\Sigma_1=\diag(\sigma_1^2,\dots,\sigma_k^2)$ with $\sigma_i^2$ unknown. The dimension of our subspace, $k$, is unknown and $k\ll n$. We also assume that $x$ and $z$ are independent.

To estimate our unknown parameters, $U_1$ and $\Sigma_1$, we are given independent signal bearing training data $\{y_1,\dots,y_m\}$, with $y_i\in H_1 \text{ for } i=1,\dots,m$ and an estimate, $\widehat{k}$ of our unknown dimension, $k$. After forming our subspace estimate, $\widehat{U}_1$,  we consider the test data $w=\widehat{U}_1^Ty\in\reals^{\widehat{k}}$. Our goal is to determine a detector, $g(w)\to\{H_0,H_1\}$ which solves the following problem for an unlabeled testing point, $w$:

\begin{equation}\label{eq:maximization}
\begin{aligned}
&\text{maximize}
&& P_D=P\left(g(w)\to H_1 | w\in H_1\right)\\
&\text{subject to}
&& P_F=P\left(g(w)\to H_1 | w\in H_0\right)\leq\alpha\\
\end{aligned}
\end{equation}

where $\alpha\in[0,1]$.

\section{Pertinent Random Matrix Theory Results}\label{sec:params}
The first step in any detector derivation is to form estimates $\widehat{U}_1$ and $\widehat{\Sigma}_1$ to use in a GLRT. We are given training data $\{y_1,\dots,y_m\}$ where $y_i\in H_1$, for $i=1,\dots,m$. By properties of Gaussian random variables, $y_i\sim\mathcal{N}(0,U_1\Sigma_1U_1^T+I)$.


\subsection{Random Matrix Theory Estimates}
First consider the general problem of estimating the eigenvalues and eigenvectors of a sample covariance matrix, $S=\frac{1}{m}YY^H$, where the columns of $Y$ are drawn from $y\sim\mathcal{N}(0,\Psi)$ where $\Psi=\diag(\lambda_1,\dots,\lambda_k,1,\dots,1)\in\reals^n$. Theorem 3 of \cite{paul2007asymptotics} gives the following distribution on the eigenvalues of the sample covariance matrix when $\lambda_i > 1 + \sqrt{c}$:

\begin{equation*}
\widehat{\lambda}_i\sim\mathcal{N}\left(\lambda_i\left(1+\frac{c}{\lambda_i-1}\right),2\lambda_i^2\left(1-\frac{c}{\left(\lambda_i-1\right)^2}\right)\right)
\end{equation*}

where $c=\frac{n}{m}$ and $\widehat{\lambda}_i$ are the eigenvalues of $S$.

Because the sample covariance matrix $S_1$ takes this form, we may apply this theorem to our problem. After some algebra, when $\sigma_i^2 > \sqrt{c}$

%\begin{equation*}
%\left(\widehat{\sigma}_i^2+1\right)\sim\mathcal{N}\left(\left(\sigma_i^2+1\right)\left(1+\frac{c}{\sigma_i^2}\right),\frac{2\left(\sigma_i^2+1\right)^2}{n}\left(1-\frac{c}{\sigma_i^4}\right)\right)
%\end{equation*}

%or that when $\sigma_i^2 > \sqrt{c}$

\begin{equation*}
\widehat{\sigma}_i^2\sim f_{\widehat{\sigma}_i^2}=\mathcal{N}\left(\left(c+\sigma_i^2+\frac{c}{\sigma_i^2}\right),\frac{2\left(\sigma_i^2+1\right)^2}{n}\left(1-\frac{c}{\sigma_i^4}\right)\right)
\end{equation*}

To improve the estimate of $\sigma_i^2$, we maximizing the log likelihood of $\widehat{\sigma}_i^2$. When $\sigma_i^2 > \sqrt{c}$ the signal variance estimate becomes

\begin{equation}\label{eq:cov}
\widehat{\sigma}^2_{i_\text{rmt}} = \argmax_{\sigma_i^2} \log\left(f_{\widehat{\sigma}_i^2}\right)
\end{equation}

However, (\ref{eq:cov}) depends on whether $\sigma_i^2 > \sqrt{c}$ which is unknown. To determine the number of $\sigma_i^2$ which are larger than $\sqrt{c}$, we utilize the monotonicity property of eigenvalues which requires $\widehat{\sigma}^2_{1_\text{rmt}}\geq \widehat{\sigma}^2_{2_\text{rmt}}\geq\dots\geq\widehat{\sigma}^2_{\widehat{k}_\text{rmt}}$. We calculate (\ref{eq:cov}) for $i=1,\dots,\widehat{k}$ and determine the first index for which the monotonicity requirement fails. The estimated signal variance at this index, and all subsequent indices, is set to 0. More complex methods for determining the number of significant factors exists (see algorithm 2 of \cite{nadakuditi2010fundamental}). However, the algorithm provided here is completely data driven based on our random matrix theory estimates and would have no effect if applied to the plug-in estimated variances as these are already monotonic as they are drawn from the eigenvalues of the sample covariance matrix.

Turning to the estimation of the eigenvectors of a sample covariance matrix, $S$, theorem 4 of \cite{paul2007asymptotics} states

\begin{equation*}
|<v_i,\widehat{v}_i>|^2 \to
\begin{cases}
0 & \text{ if } \lambda_i\leq 1+\sqrt{c}\\
\frac{1-\frac{c}{(\lambda_i-1)^2}}{1+\frac{c}{\lambda_i-1}} & \text{ if } \lambda_i>1+\sqrt{c}
\end{cases}
\end{equation*}

where $\widehat{v}_i$ is the eigenvector of the sample covariance matrix, $S$, corresponding to the eigenvalue $\lambda_i$ and $v_i$ is the true underlying eigenvalue. After some algebra and substituting the expression for $\sigma_i^2$ derived in (\ref{eq:cov}), this theorem tells us that the eigenvector estimates of $S_1$ behave as

%\begin{equation*}
%\begin{aligned}
%&|<u_i,\widehat{u}_i>|^2
%%&& \to \begin{cases}
%%0 & \text{ if } \sigma_i^2+1\leq 1+\sqrt{c}\\
%%\frac{1-\frac{c}{(\sigma_i^2+1-1)^2}}{1+\frac{c}{\sigma_i^2+1-1}} & \text{ if } \sigma_i^2+1>1+\sqrt{c}
%%\end{cases}\\
%&&\to\begin{cases}
%0 & \text{ if } \sigma_i^2\leq\sqrt{c}\\
%\frac{\sigma_i^4-c}{\sigma_i^4+\sigma_i^2c} & \text{ if } \sigma_i^2>\sqrt{c}
%\end{cases}\\
%\end{aligned}
%\end{equation*}

%Substituting the expression for $\sigma_i^2$ derived in (\ref{eq:cov}) results in

\begin{equation}\label{eq:angles}
|<u_i,\widehat{u}_i>|^2_{\text{rmt}}
\to\begin{cases}
0 & \text{ if } \widehat{\sigma}_{i_\text{rmt}}^2\leq\sqrt{c}\\
\frac{\widehat{\sigma}_{i_\text{rmt}}^4-c}{\widehat{\sigma}_{i_\text{rmt}}^4+\widehat{\sigma}_{i_\text{rmt}}^2c} & \text{ if } \widehat{\sigma}_{i_\text{rmt}}^2>\sqrt{c}
\end{cases}
\end{equation}

Finally, we formally define the effective dimension of the problem, $k_\text{eff}$, to be the number of signal eigenvalues above the phase transition. That is,

\begin{equation}
k_\text{eff} = \left|\{i:\widehat{\sigma}_{i_\text{rmt}}^2>\sqrt{c}\}\right|
\end{equation}

Note that $k_\text{eff}\leq\widehat{k}$.

\section{Family of Matched Subspace Detectors}\label{sec:detectors}

The Neyman Pearson Lemma (see \cite{van1968detection}) states that the solution to (\ref{eq:maximization}) is a likelihood ratio test (LRT)

\begin{equation*}
\Lambda(w) \detgtrless \eta
\end{equation*}

where $\Lambda(w) = \frac{f(w|H_1)}{f(w|H_0)}$ and $\eta$ satisfies $P(\Lambda(w)\leq\eta|H_0)=\alpha$.

Returning to the particular problem, by properties of Gaussian random variables, $y|H_0\sim\mathcal{N}(0,I)$ and $y|H_1\sim\mathcal{N}(0,U_1\Sigma_1U_1^T+I)$. For the processed data, $w=\widehat{U}^T_1y$, using properties of Gaussian random variables $w|H_0\sim\mathcal{N}(0,I_{\widehat{k}})$ and $w|H_1\sim\mathcal{N}(0, \widehat{U}_1^TU_1\Sigma_1U_1^T\widehat{U}_1 +I_{\widehat{k}})$.

\subsection{Main Results}\label{sec:main results}

We will consider 3 different detectors for our test data $w$. The first is an oracle detector, which will assume that $U_1$ and $\Sigma_1$ are known. The purpose of this is to give an upper bound on a detector's performance. The second is a plug-in detector which will approximate the oracle classifier by simply plugging in maximum likelihood (ML) estimates of $\widehat{U}_1$  and $\widehat{\Sigma}_1$ for our unknown $U_1$ and $\Sigma_1$. The third is an optimal detector which uses  random matrix theory to form an approximation to the oracle classifier. It will utilize the fact that the parameter estimates are indeed noisy estimates of the true parameters and will only include $k_\text{eff}$ dimensions of our estimated subspace. Table \ref{table: main results} summarizes the main results of this paper. It includes the statistic for each detector as well as its distribution under each hypothesis.

\begin{table*}[!ht]
\centering
\begin{tabular}{cclclcl}\toprule
 Detector & \phantom{a} & Detector Statistic $\Lambda(w)$ & \phantom{a} & Null Hypothesis Distribution $\Lambda|H_0$& \phantom{a} & Simple Hypothesis Distribution $\Lambda|H_1$\\
\midrule
Oracle && $ w^T\left[I-\left(\widehat{U}_1^TU_1\Sigma_1U_1^T\widehat{U}_1+I\right)^{-1}\right]w$ &&  && \\
Plug-in && $\sum_{i=1}^{\widehat{k}}w_i^2\frac{\widehat{\sigma}_i^2}{\widehat{\sigma}_i^2+1}$ && $\sum_{i=1}^{\widehat{k}}\left(\frac{\sigma_i^2}{1+\sigma_i^2}\right)\chi^2_{1i}$ && $\sum_{i=1}^{\widehat{k}}\left(\frac{\sigma_i^2\left(\sigma^2_i|<u_i,\widehat{u}_i>|^2+1\right)}{1+\sigma_i^2}\right)\chi^2_{1i}$\\
%Energy &&$\sum_{i=1}^{\widehat{k}} w_i^2 $ && $\chi^2_{{\widehat{k}}}$ && $\sum_{i=1}^{\widehat{k}}\left(\sigma^2_i|<u_i,\widehat{u}_i>|^2+1\right)\chi^2_{1i}$\\
 Optimal&& $\sum_{i=1}^{k_\text{eff}}w_i^2\frac{|<u_i,\widehat{u}_i>|^2_{\text{rmt}}\widehat{\sigma}_{i_\text{rmt}}^2}{|<u_i,\widehat{u}_i>|^2_{\text{rmt}}\widehat{\sigma}_{i_\text{rmt}}^2 + 1}$ && $\sum_{i=1}^{k_\text{eff}}\left(\frac{\sigma_i^2|<u_i,\widehat{u}_i>|^2_{\text{rmt}}}{1+\sigma_i^2|<u_i,\widehat{u}_i>|^2_{\text{rmt}}}\right)\chi^2_{1i}$ && $\sum_{i=1}^{k_\text{eff}}\left(\sigma^2_i|<u_i,\widehat{u}_i>|^2_{\text{rmt}}\right)\chi^2_{1i}$\\
\bottomrule
\end{tabular}
\caption{Main Results}
\label{table: main results}
\end{table*}

\subsection{Oracle Detector}\label{sec:oracle}

The oracle detector assumes that $\Sigma_1$ and $U_1$ are both known. The LRT statistic for our processed data $w$ is

\begin{equation*}
\Lambda(w)=\frac{\mathcal{N}(0,\widehat{U}_1^TU_1\Sigma_1U_1^T\widehat{U}_1 +I)}{\mathcal{N}(0,I_{\widehat{k}})}
\end{equation*}

After simplification of this expression using the natural logarithm operator as a monotonic operation, the oracle statistic becomes

\begin{equation}\label{eq:oracle stat}
\boxed{\Lambda_{\text{oracle}}(w) = w^T\left[I-\left(\widehat{U}_1^TU_1\Sigma_1U_1^T\widehat{U}_1+I\right)^{-1}\right]w}
\end{equation}

and the oracle detector is

\begin{equation}\label{eq:oracle classifier}
\boxed{\Lambda_{\text{oracle}}(w) \detgtrless \ln(\eta_{\text{oracle}})}
\end{equation}

where $\eta_{\text{oracle}}$ satisfies $P(\Lambda_{\text{oracle}}(w)\leq\ln\left(\eta_{\text{oracle}}\right)|H_0)=\alpha$.

\subsection{Plug-in Detector}\label{sec:plugin}

As is the case in this particular problem, $U_1$ and $\Sigma_1$ are unknown, and therefore (\ref{eq:oracle stat}) cannot be computed directly. It is common practice to plug in ML estimates for the unknown parameters in the oracle detector to form a GLRT as similarly done in \cite{jin2005cfar} and \cite{mcwhorter2003matched}. To form a ML estimate of this covariance matrix, the training data are stacked as columns in a matrix $Y=[y_1,\dots,y_m]$. The sample covariance matrix, $S_1=\frac{1}{m}YY^H$, is then constructed. With some algebra, it may be shown that

%Taking $U_2=U_1^{\perp}$ to be the orthogonal complement of $U_1 $, the unknown covariance may be written as
%
%\begin{equation*}
%\begin{aligned}
%&U_1\Sigma U_1^T+I
%&&=\left[\begin{array}{lr} U_1 & U_2\end{array}\right]\left[\begin{array}{lr}\Sigma_1+I_k & 0\\ 0 & I_{n-k}\end{array}\right]\left[\begin{array}{lr}U_1^T \\ U_2^T\end{array}\right]\\
%\end{aligned}
%\end{equation*}
%
%Clearly this expression is an eigenvalue decomposition of the covariance matrix of $Y$. Therefore, by taking the largest $k$ eigenvalues and corresponding eigenvectors of $S_1$, an estimate of both the signal subspace, $U_1$, and the signal covariance matrix, $\Sigma_1$, can be formed. However, recall that $k$ is unknown. But, $\widehat{k}$ is provided as an estimate of the dimension of the signal subsapce. Therefore, the estimates are constructed by taking the largest $\widehat{k}$ eigenvalues and corresponding eigenvectors of $S_1$. Defining the eigenvalue decomposition $S_1=V\Lambda V^T$ where $V=[v_1,\dots, v_n]$ and $\Lambda = \diag(\lambda_1,\dots,\lambda_n)$ such that $\lambda_1\geq\lambda_2\geq\dots\geq\lambda_n$ the estimates become

\begin{equation}\label{eq:param estims}
\begin{aligned}
&\widehat{U}_1=[v_1 \dots v_{\widehat{k}}]\\
&\widehat{\sigma}_i^2 = \lambda_i -1 \text{ for } i=1,\dots,\widehat{k}\\
\end{aligned}
\end{equation}

where $\lambda_1,\dots,\lambda_{\widehat{k}}$ are the largest $\widehat{k}$ eigenvalues of $S_1$ and $v_1,\dots,v_{\widehat{k}}$ are the corresponding eigenvectors. Define the signal covariance matrix estimate as $\widehat{\Sigma}_1=\diag(\widehat{\sigma}_1^2,\dots,\widehat{\sigma}_{\widehat{k}}^2)$.

By replacing the unknown parameters in (\ref{eq:oracle stat}) with the estimated parameters in (\ref{eq:param estims}), the plug-in detector statistic becomes:

\begin{equation*}
\Lambda_{\text{plugin}}(w)= w^T\left(I-\left[\widehat{U}_1^H\widehat{U}_1\widehat{\Sigma}_1\widehat{U}_1^H\widehat{U}_1 + I\right]^{-1}\right)w\\
%&&& = w^T\left(I-\left(\widehat{\Sigma}_1+I\right)^{-1}\right)w\\
%&&& = w^T\left(I-\diag\left(\widehat{\sigma}_i^2+1\right)^{-1}\right)w\\
\end{equation*}

This simplifies to

\begin{equation}\label{eq:plugin stat}
\boxed{\Lambda_{\text{plugin}}(w) = w^T\diag\left(\frac{\widehat{\sigma}^2_i}{1+\widehat{\sigma}^2_i}\right)w=\sum_{i=1}^{\widehat{k}}w_i^2\frac{\widehat{\sigma}_i^2}{\widehat{\sigma}_i^2+1}}
\end{equation}

and our detector becomes

\begin{equation}\label{eq:plugin classifier}
\boxed{\Lambda_{\text{plugin}}(w) \detgtrless \ln(\eta_{\text{plugin}})}
\end{equation}

where $\eta_{\text{plugin}}$ satisfies $P(\Lambda_{\text{plugin}}(w)\leq\ln\left(\eta_{\text{plugin}}\right)|H_0)=\alpha$.

The plug-in detector assumes that the estimated signal subspace $\widehat{U}_1$ is equal to the true subspace $U_1$ and that the estimated signal covariance $\widehat{\Sigma}_1$ is equal to the true signal subspace $\Sigma_1$. It also assumes that the provided subspace dimension estimate $\widehat{k}$ is also equal to the true underlying dimension of our signal subspace $k$. Intuitively, making these assumptions should lead to sub-optimal performance which is shown to be the case later in the paper.

%\section{Energy Detector}\label{sec:energy}
%
%The energy detector is derived by making an even further assumption that $\Sigma_1=\beta I$, where $\beta>0$ is an arbitrary constant. This detector assumes that the signal strength is the same for every dimension in the underlying signal subspace. Incorporating this assumption into the plug-in statistic, the statistic becomes
%
%\begin{equation*}
%\begin{aligned}
%&\Lambda(w)
%&&= \sum_{i=1}^k w_i^2\left(\frac{\beta}{\beta+1}\right)\\
%&&&=\left(\frac{\beta}{\beta+1}\right)\sum_{i=1}^{\widehat{k}} w_i^2
%\end{aligned}
%\end{equation*}
%
%Defining
%
%\begin{equation}\label{eq:energy stat}
%\boxed{\Lambda_{\text{energy}}=\sum_{i=1}^{\widehat{k}} w_i^2}
%\end{equation}
%
%and incorporating the constant $\frac{\beta}{\beta+1}$ into $\eta_{\text{plugin}}$ the energy detector becomes
%
%\begin{equation}\label{eq:energy classifier}
%\boxed{\Lambda_{\text{energy}}(w) \detgtrless \ln(\eta_{\text{energy}})}
%\end{equation}
%
%where $\eta_{\text{energy}}$ satisfies $P(\Lambda_{\text{energy}}(w)\leq\ln\left(\eta_{\text{energy}}\right)|H_0)=\alpha$.
%
%The energy detector statistic can be written $\sum_{i=1}^{\widehat{k}}w_i^2=w^Tw=y^T\widehat{U}_1\widehat{U}_1^Ty$. It becomes evident where the energy detector gets its name as $y^T\widehat{U}_1\widehat{U}_1^Ty$ is the energy of the test sample in the estimated signal subspace. The energy detector declares the test sample to be signal if the energy in the estimated signal subspace is above a desired threshold. The energy detector makes an even further simplifying assumption so intuition hints that the energy detector is also sub-optimal.

\subsection{Optimal Detector}\label{sec:optimal}

Consider the conditional distribution $w|H_1\sim\mathcal{N}(0,\widehat{U}_1^TU_1\Sigma_1 U_1^T\widehat{U_1}+I)$. The covariance matrix of this distribution contains the term $\widehat{U}_1^TU_1$ which has diagonal entries $\langle \widehat{u}_i,u_i\rangle$ for $i=1,\dots,\min\{\widehat{k},k\}$. By plugging in $\widehat{U}_1$ for $U_1$, the plug-in detector assumes that $\langle \widehat{u}_i,u_i\rangle=1$.This assumption is incorrect as shown in \cite{paul2007asymptotics}. As derived in (\ref{eq:angles}), because the estimated parameters are noisy estimates, the estimated eigenvectors of $S_1$ do not equal the true eigenvectors of the signal covariance matrix. In fact, depending on the unknown signal strengths, $\sigma_i^2$, these estimates may be random i.e. $\langle \widehat{u}_i,u_i\rangle=0$. Intuition says that using parameter estimates which take into consideration that the eigenvalues and eigenvectors of our sample covariance matrix are noisy should lead to an increased performance in detection ability. To derive a detector using these estimates, we first make a diagonal approximation of the matrix inverse in (\ref{eq:oracle stat}). After some algebra, we have

\begin{equation*}
\tilde{\Lambda}(w)\approx w^T\left(\diag\left(\frac{|<u_i,\widehat{u}_i>|^2\sigma_i^2}{|<u_i,\widehat{u}_i>|^2\sigma_i^2 + 1}\right)\right)w
%&&= w^T\left[I-\left(\widehat{U}_1^HU_1\Sigma_1U_1^H\widehat{U}_1+I\right)^{-1}\right]w\\
%&&&\approx w^T\left(I-\left[\diag\left(|<u_i,\widehat{u}_i>|^2\sigma_i^2\right)+I\right]^{-1}\right)w\\
\end{equation*}

However, $\sigma_i^2$ and $|<u_i,\widehat{u}_i>|^2$ are unknown. Instead of using $\widehat{\sigma}_i^2$ and estimating $|<u_i,\widehat{u}_i>|^2=1$ as the plug-in classifier does, we use the random matrix theory expressions derived in Section \ref{sec:params} which consider the error in estimating the eigenvalues and eigenvectors of $S_1$.

Using (\ref{eq:cov}) and (\ref{eq:angles}), our random matrix theory statistic becomes

\begin{equation*}
\Lambda_{\text{optimal}}(w)= \sum_{i=1}^{\widehat{k}}w_i^2\frac{|<u_i,\widehat{u}_i>|^2_{\text{rmt}}\widehat{\sigma}_{i_\text{rmt}}^2}{|<u_i,\widehat{u}_i>|^2_{\text{rmt}}\widehat{\sigma}_{i_\text{rmt}}^2 + 1}
%&&= w^T\diag\left(\frac{|<u_i,\widehat{u}_i>|^2_{\text{rmt}}\widehat{\sigma}_{i_\text{rmt}}^2}{|<u_i,\widehat{u}_i>|^2_{\text{rmt}}\widehat{\sigma}_{i_\text{rmt}}^2 + 1}\right)w \\
\end{equation*}

However, as $|<u_i,\widehat{u}_i>|^2=0$ when $\widehat{\sigma}_{i_\text{rmt}}^2$ drops below $\sqrt{c}$, this sum ignores all indices whose signal eigenvalue estimate is below this critical value. Instead, the optimal detector only sums those components with significant signal eigenvalues. The number of significant signal eigenvalues was defined as $k_\text{eff}$. Our final optimal statistic is

\begin{equation}\label{eq:rmt stat}
\boxed{\Lambda_{\text{optimal}}(w)= \sum_{i=1}^{k_\text{eff}}w_i^2\frac{|<u_i,\widehat{u}_i>|^2_{\text{rmt}}\widehat{\sigma}_{i_\text{rmt}}^2}{|<u_i,\widehat{u}_i>|^2_{\text{rmt}}\widehat{\sigma}_{i_\text{rmt}}^2 + 1}}
\end{equation}

and the optimal detector becomes

\begin{equation}\label{eq:rmt classifier}
\boxed{\Lambda_{\text{optimal}}(w) \detgtrless \ln(\eta_{\text{optimal}})}
\end{equation}

where $\eta_{\text{optimal}}$ satisfies $P(\Lambda_{\text{optimal}}(w)\leq\eta_{\text{optimal}}|H_0)=\alpha$.

\section{Theoretical ROC Curve Derivation}\label{sec:roc}

A standard way to compare the plug-in and optimal detectors derived in (\ref{eq:plugin classifier}) and (\ref{eq:rmt classifier}) respectively is to compute their ROC curves \cite{fawcett2006introduction}. For a particular statistic $\Lambda(w)$, to compute theoretical ROC curves, we must compute

\begin{equation}\label{eq:target cdf}
\begin{aligned}
&P_D = P(\Lambda(w) \geq y| w\in H_1)\\
&P_F = P(\Lambda(w) \geq y| w\in H_0)\\
\end{aligned}
\end{equation}

for $-\infty<y<\infty$. To do this, we explore the conditional CDF under each hypothesis for the statistics (\ref{eq:plugin stat}) and (\ref{eq:rmt stat}).

The conditional distributions of our test samples under each hypothesis are $w|H_0\sim\mathcal{N}(0,I)$ and $w|H_1\sim\mathcal{N}(0,\widehat{U}^T_1U_1\Sigma_1U_1^T\widehat{U}_1+I)$. Because of the diagonal covariance matrix under $H_0$, $w_i|H_0\sim\mathcal{N}(0,1)$ are i.i.d for $i=1,\dots,\widehat{k}$ and $w_i^2|H_0\sim\chi_1^2$ are i.i.d for $i=1,\dots,\widehat{k}$. By making a diagonal approximation of the  covariance matrix under $H_1$,  $w_i|H_1\approx\mathcal{N}(0,\sigma^2_i|<u_i,\widehat{u}_i>|^2+1)$ are i.i.d for $i=1,\dots,\widehat{k}$. Therefore, for $i=1,\dots,\widehat{k}$

\begin{equation*}
\frac{w_i^2|H_1}{\sigma^2_i|<u_i,\widehat{u}_i>|^2+1}\sim\chi_1^2
\end{equation*}

We use these two conditional distributions to determine the distribution of each detector under each hypothesis. Table \ref{table: main results} summarizes the results after substituting the conditional distributions for $w$ into each detector's statistic.

Evident in Table \ref{table: main results}, all distributions are a weighted sum of independent chi-square random variables with 1 degree of freedom. That is the distributions take the form

\begin{equation*}
\Lambda = \sum_{i=1}^r\lambda_i\chi^2_{1i}
\end{equation*}

where $\lambda_i$ is the appropriate weighting, unique to each statistic under each hypothesis. The CDF of a chi-square random variable is known in closed form. However, the CDF of a weighted sum of independent chi-square random variables is not known in closed form. To evaluate (\ref{eq:target cdf}), we use a saddlepoint approximation of the CDF of $\Lambda$ by employing the generalized Lugannani-Rice formula proposed in \cite{wood1993saddlepoint}. To then compute a theoretical ROC curve, $y$ is swept over $(0,\infty)$ and for each value of $y$, the saddlepoint approximation of the CDF under each hypothesis is computed. This generates a set of points $(P_F,P_D)$ which approximate the theoretical ROC curve.

\begin{figure}
\centering
\includegraphics[width=3.5in]{figures/roc_curves.png}
\caption{Empirical and theoretical ROC curves for the oracle, plug-in, and optimal matched subspace detectors. Empirical ROC curves were simulated with $n=100$, $m=50$, $k=2$, $\widehat{k}=2$, $\sigma_1=5$, $\sigma_2=0.5$. However, as $\sigma_2$ is below the critical threshold, $k_{\text{eff}} = 1$. The empirical ROC curves were computed using 2000 test samples and averaged over 25 trials using algorithms 2 and 4 of \cite{fawcett2006introduction}. The theoretical ROC curves were computed as described in Section \ref{sec:roc}. Because $\widehat{k}\neq k_{\text{eff}}$, we observe a performance gain when using the optimal detector. In fact, the optimal detector operates at almost the same $(P_F, P_D)$ as the oracle detector. The theoretical ROC curves match the empirical ones for each detector.}
\label{fig:roc}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=3.5in]{figures/k_hat_graph.png}
\caption{Empirical exploration of the achieved probability of detection, $P_D$, for a fixed probability of false alarm, $P_F=0.01$, for various $\widehat{k}$. For the simulation, $n=100$, $m=50$, $k=6$, $\Sigma^{1/2} = \diag({\bf{5,4,3,2}},0.6,0.5)$ so that $k_{\text{eff}}=4$. $P_D$ calculation was computed using 2000 test samples and averaged over 25 trials. The optimal $\widehat{k}$ resulting in the largest $P_D$ is not the true $k$, but rather $k_\text{eff}$.}
\label{fig:khat}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=3.5in]{figures/samples_graph.png}
\caption{Empirical exploration of the number of test samples required to achieve a desired $P_F=0.001$ for different $P_D$. In this setup, $N$ test samples are averaged together and then sent to each detector. For the simulation, $n=100$, $m=50$, $k=6$, $\widehat{k}=6$,$\Sigma^{1/2} = \diag({\bf{3,2}},0.5,0.4,0.3,0.2)$ so that $k_{\text{eff}}=2$. $P_D$ calculation was computed using 2000 test samples and averaged over 10 trials. Because the optimal detector only uses $k_\text{eff}$ dimensions, it requires less test samples to achieve a desired $(P_F, P_D)$ than the plug-in detector.}
\label{fig:samples}
\end{figure}

\section{Simulation Results and Discussion}\label{sec:disc}

\subsection{ROC curves}

Evident in Figure \ref{fig:roc}, the theoretical ROC curves match the empirical ROC curves. Our saddlepoint approximation to the CDF of a weighted sum of independent chi square random variables is valid. As evident by their ROC curves, the optimal detector operates at almost the same $(P_F, P_D)$ as the oracle detector. Therefore, the random matrix theory approximations of unknown parameters are very accurate. Finally, we note the sub-optimality of the plug-in detector. This is expected as it incorrectly assumes that estimated eigenvalues and eigenvectors are exact.

\subsection{Optimality of $k_\text{eff}$}

Evident in Figure \ref{fig:khat}, the optimal $\widehat{k}$ id $k_\text{eff}$. Underestimating $k_\text{eff}$ drastically decreases performance for both detectors. However, the plug-in and optimal detectors operates at roughly the same $P_D$ showing that the different shrinkage weights applied by each detector do not make a difference in detection ability. When overestimating $k_\text{eff}$, the plug-in detector observes a decrease in performance while the optimal detector maintains the same performance as if $\widehat{k}=k_\text{eff}$. Thus, we do not pay a price for overestimating our dimension with the optimal detector. This makes sense because the optimal detector will only sum to a maximum of $k_\text{eff}$ indices as evident in (\ref{eq:rmt stat}). As if often the case, the dimension is purposefully overestimated to ensure that all signal dimensions are included. However as evident in Figure \ref{fig:khat}, doing so with the plug-in detector will degrade performance. In fact, even if $\widehat{k}=k$, the plug-in detector still is suboptimal in this example. Only when $\widehat{k}=k_\text{eff}$ will the optimal performance be achieved. The cosine-squared term which determines $k_{\text{eff}}$ arises in other applications such as array processing \cite{cox1973resolving}.

\subsection{Averaging over Test Samples}

As evident in Figure \ref{fig:samples}, the optimal detector requires less samples than the plug-in detector to achieve a desired $(P_F, P_D)$. This arises from the fact that the optimal detector ignores noisy signal components while the plug-in detector uses them. Because each test sample is pruned to ignore noisy components, we can average less samples to achieve a desired accuracy.

\section{Conclusion}\label{sec:concl}
We have derived three stochastic matched subspace detectors. Through an ROC analysis, we have demonstrated the sub-optimality of the plug-in detector. This arises from the false assumption that estimated eigenvalues and eigenvectors of the sample signal covariance matrix are exact estimates of the true eigenvalues and eigenvectors of our signal covariance matrix. By using random matrix theory to characterize the accuracy of eigenvalues and eigenvectors estimates from limited training data, we have derived an optimal detector. ROC analysis demonstrated that the optimal detector outperforms the plug-in detector. This arises from the optimal detector using the effective number of signal subspace dimensions, $k_\text{eff}$, which was shown to be the optimal dimension estimate for a matched subspace detection problem. Reweighing (or shrinking) dimensions was shown to have little effect on detector performance which is controlled by using the effective dimension, $k_\text{eff}$.


% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use \appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


%\appendices
%\section{Proof of the First Zonklar Equation}
%Appendix one text goes here.
%
%% you can choose not to have a title for an appendix
%% if you want by leaving the argument blank
%\section{}
%Appendix two text goes here. 