\documentclass[11pt]{article}
\usepackage{times,amsmathamssymb,algirthm,algorithmic,geometry,bbm}

\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}




\begin{document}

\textit{Claim 5.1:}  We conjecture that this result holds for the general case of $i\neq
j$, $i=1,\dots,\widehat{k}$, $j=1,\dots,k$, not just when $\widehat{k}=\keff=k$. Consider
the case when $k=1$. For $j>2$, if $\widehat{\lambda}_j$ is an eigenvalue of
$\widehat{X}_n=X_n(I_n+\sigma^2uu^H)$, then it satisfies
$\det(\widehat{\lambda}_jI_n-X_n(I_n+\sigma^2uu^H)) =
\det(\widehat{\lambda}_jI_n-X_n)\det(I_n-(\widehat{\lambda}_jI_n-X_n)^{-1}X_n\sigma^2uu^H)=0$. Therefore,
if $\widehat{\lambda}_j$ is not an eigenvalue of $X_n$, the corresponding unit norm
eigenvector $\widehat{v}_j$ is in the kernel of
$I_n-(\widehat{\lambda}_jI_n-X_n)^{-1}X_n\sigma^2uu^H$. Therefore
\begin{equation*}
  |\langle \widehat{v}_j,u\rangle |^2 = \frac{1}{\sigma^4u^HX_n\left(\widehat{\lambda}_jI_n-X_n\right)^{-2}X_nu}.
\end{equation*}
Recall that Weyl's interlacing lemma for eigenvalues gives $\lambda_j(X_n)\leq
\widehat{\lambda}_j\leq \lambda_{j-1}(X_n)$. Letting $X_n=V_n\Lambda_nV_n^H$ and
$w=V_n^Hu$, we see the importance of the
asymptotic spacing of eigenvalues of $X_n$ in
%\begin{equation*}
%  u^HX_n(\widehat{\lambda}_jI_n-X_n)^{-2}X_nu =\sum_{\ell=1}^n\frac{|w_\ell|^2\lambda_\ell^2(X_n)}{\left(\widehat{\lambda}_j-\lambda_\ell\right)^2}\geq\frac{|w_{j-1}|^2\lambda_{j-1}^2(X_n)}{|\lambda_{j-1}-\lambda_j|^2}+\frac{|w_{j}|^2\lambda_j^2(X_n)}{|\lambda_{j-1}-\lambda_j|^2}.
%\end{equation*}
\begin{equation*}
  u^HX_n(\widehat{\lambda}_jI_n-X_n)^{-2}X_nu
  =\sum_{\ell=1}^n\frac{|w_\ell|^2\lambda_\ell^2(X_n)}{\left(\widehat{\lambda}_j-\lambda_\ell\right)^2}\geq
  \frac{\min_j\lambda_j^2(X_n)\min_j|w_j|^2}{\max_j |\lambda_{j-1}-\lambda_j|^2}
\end{equation*}
In  \cite{jiang2004limiting} it is shown that $\min_j\lambda_j^2(X_n)=\lambda_n^2(X_n)
\convas (1-\sqrt{c})^2$. The typical spacing between eigenvalues is $O(1/n)$ while the
typical magnitude of $w_i^2$ is $O(1/n)$. Therefore, the above inequality will typically be $O(n)$ and we get the desired
result of $|\langle \widehat{v}_j,u\rangle |^2\convas 0$. More generally, it is the
behavior of the largest eigenvalue gap and the smallest element of $w_i$ that drives this
convergence. Thus, so long as the eigenvector whose elements are $w_i$ are delocalized
(having elements of $O(1/\sqrt{n})$ and the smallest gap between $k$ successive
eigenvalues is at least as large as $O(1/n + \epsilon)$, we may bound the right hand side
of the above inequality. The claim follows after applying a similarity transform as in the
proof of Theorem 5.1.
 
%As in the above proof, we may apply a similarity transform to make the claim about the
%eigenvectors of the sample covariance matrix. This argument can be extended to the
%multi-rank case to show the importance of $|\lambda_i-\lambda_{i-k}|^2$, that is, the gap
%(or spacing) of $k$ successive eigenvalues of $X_n$. Understanding this term is an important future research area. 
\end{document}