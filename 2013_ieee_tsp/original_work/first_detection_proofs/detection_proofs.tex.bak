\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{amstext}
\usepackage{babel,color}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{epstopdf}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
%\usepackage[parfill]{parskip}    % Activate to begin paragraphs with an empty line rather than an indent
\parskip = 0.03in

\newtheorem{theorem}{Theorem}

\input my_header.tex

\begin{document}

\section*{Balanced Setting Nearest Neighbor Proof}

We consider the following binary classification problem:

\begin{equation}
y=\left\{
\begin{aligned}
&U_1x+z
&& y\in H_1\\
&U_2x+z
&& y\in H_2\\
\end{aligned}\right.
\end{equation}

where $z\sim\mathcal{N}(0,\sigma^2I), x\sim\mathcal{N}(0,\Sigma)$ with known diagonal $\Sigma$. We assume $U_1,U_2\in\mathbb{C}^{p\times k}$ are known and have column rank $k$, which is also known.

As $x$ is unknown, we consider the generalized likelihood ratio test (GLRT)

\begin{equation}\label{eq:glrt}
\begin{aligned}
&\Lambda(y)=\frac{\max_xf_2(y)}{\max_xf_1(y)}
\end{aligned}
\end{equation}

where
\begin{equation}
\begin{aligned}
&\text{Declare } H_1 \text{ if}
&& \Lambda(y) < \eta\\
& \text{Declare } H_2 \text{ if}
&& \Lambda(y) > \eta
\end{aligned}
\end{equation}

where $\eta = \frac{\text{Prob}(H_1)}{\text{Prob}(H_2)}$. We assume that $H_1$ and $H_2$ are equally likely so that $\eta = 1$.

Now, under $H_1,y_1\sim\mathcal{N}(U_1x,\sigma^2I)$ and under $H_2,y_2\sim\mathcal{N}(U_2x,\sigma^2)$.

To solve $\max_xf_i(y)$ we find the MLE estimate of $x$. We have

\begin{equation}
f_i(y;x) = (2\pi\sigma^2)^{-p/2}\exp\{-\frac{1}{2\sigma^2}(y-U_ix)^H(y-U_ix)\}
\end{equation}

and

\begin{equation}\label{eq:log like}
\ln(f_i(y;x)) = -(p/2)(2\pi\sigma^2)-\frac{1}{2\sigma^2}(y-U_ix)^H(y-U_ix)
\end{equation}

The first term of (\ref{eq:log like}) is constant. As we wish to maximize (\ref{eq:log like}), this is equivalent to minimizing $(y-U_ix)^H(y-U_ix)$. Noticing that is this is the error of the the least-squares solution, we recognize that

\begin{equation}\label{eq:mle}
\hat{x}_{MLE} = (U_i^HU_i)^{-1}U_i^Hy
\end{equation}

Plugging our MLE estimates (\ref{eq:mle}) into our GLRT (\ref{eq:glrt}) we have

\begin{equation}
\Lambda(y) = \frac{(2\pi\sigma^2)^{-p/2}\exp\{-\frac{1}{2\sigma^2}(y-U_2(U_2^HU_2)^{-1}U_2^Hy)^H(y-U_2(U_2^HU_2)^{-1}U_2^Hy)\}}{(2\pi\sigma^2)^{-p/2}\exp\{-\frac{1}{2\sigma^2}(y-U_1(U_1^HU_1)^{-1}U_1^Hy)^H(y-U_1(U_1^HU_1)^{-1}U_1^Hy)\}}
\end{equation}

which simplifies to

\begin{equation}
\hat{\Lambda}(y)=\|(I-U_1(U_1^HU_1)^{-1}U_1^H)y\|_F^2 -\|(I-U_2(U_2^HU_2)^{-1}U_2^H)y\|_F^2
\end{equation}

where

\begin{equation}
\begin{aligned}
&\text{Declare } H_1 \text{ if}
&& \hat{\Lambda}(y) < 2\ln(\eta)\\
& \text{Declare } H_2 \text{ if}
&& \hat{\Lambda}(y) > 2\ln(\eta)
\end{aligned}
\end{equation}

Recalling that $\eta=1$ this reduces to

\begin{equation}
i_{\text{oracle}}=\argmin_{i\in\{1,2\}}\|((I-U_i(U_i^HU_i)^{-1}U_i^H)y\|_F^2
\end{equation}

If we further assume that the columns of $U_1,U_2$ are orthonormal, this reduces to

\begin{equation}
i_{\text{oracle}}=\argmin_{i\in\{1,2\}}\|((I-U_iU_i^H)y\|_F^2
\end{equation}

\section*{Balanced Setting Stochastic Proof}

We consider the following binary classification problem:

\begin{equation}
y=\left\{
\begin{aligned}
&U_1x_1+z
&& y\in H_1\\
&U_2x_2+z
&& y\in H_2\\
\end{aligned}\right.
\end{equation}

where $z\sim\mathcal{N}(0,\sigma^2I), x_1\sim\mathcal{N}(0,\Sigma_1),x_2\sim\mathcal{N}(0,\Sigma_2)$ with known diagonal $\Sigma$. We assume $U_1,U_2\in\mathbb{C}^{p\times k}$ are known and have orthonormal columns and thus have rank $k$, which is also known. We also assume that $x$ and $z$ are independent. We therefore have that

\begin{equation}
\begin{aligned}
&y|H_1\sim\mathcal{N}(0,\sigma^2I+U_1\Sigma_1U_1^H)\\
&y|H_2\sim\mathcal{N}(0,\sigma^2I+U_2\Sigma_2U_2^H)
\end{aligned}
\end{equation}

We then consider the likelihood ratio test

\begin{equation}
\begin{aligned}
&\Lambda(y)
&&=
&&&\frac{f(y|H_2)}{f(y|H_1)}\\
&&&=
&&&\frac{(2\pi)^{-p/2}\det(\sigma^2I+U_2\Sigma_2U_2^H)^{-1/2}\exp\{-\frac{1}{2}y^H(\sigma^2I+U_2\Sigma_2U_2^H)^{-1}y\}}{(2\pi)^{-p/2}\det(\sigma^2I+U_1\Sigma_1U_1^H)^{-1/2}\exp\{-\frac{1}{2}y^H(\sigma^2I+U_1\Sigma_1U_1^H)^{-1}y\}}\\
&&&=
&&&\frac{C}{\det(\sigma^2I+U_1\Sigma_1U_1^H)^{-1/2}}\exp\{-\frac{1}{2}y^H\left[(\sigma^2I+U_2\Sigma_2U_2^H)^{-1} - (\sigma^2I+U_2\Sigma_2U_2^H)^{-1}\right]y\}\\
\end{aligned}
\end{equation}

This simplifies to

\begin{equation}\label{eq:stoch LRT1}
\hat{\Lambda}(y)=y^H\left[(\sigma^2I+U_2\Sigma_2U_2^H)^{-1} - (\sigma^2I+U_2\Sigma_2U_2^H)^{-1}\right]y
\end{equation}

where

\begin{equation}\label{eq:LRT rule1}
\begin{aligned}
&\text{Declare } H_1 \text{ if}
&& \hat{\Lambda}(y) < 2\ln\left(\eta\frac{\det(\sigma^2I+U_1\Sigma_1U_1^H)^{1/2}}{\det(\sigma^2I+U_2\Sigma_2U_2^H)^{1/2}}\right)\\
& \text{Declare } H_2 \text{ if}
&& \hat{\Lambda}(y) > 2\ln\left(\eta\frac{\det(\sigma^2I+U_1\Sigma_1U_1^H)^{1/2}}{\det(\sigma^2I+U_2\Sigma_2U_2^H)^{1/2}}\right)\\
\end{aligned}
\end{equation}

Using the Sherman-Morrison-Woodberry matrix inversion lemma, we may simplify (\ref{eq:stoch LRT1})

\begin{equation}
\hat{\Lambda}(y)=y^H\left[\frac{1}{\sigma^2}I-\frac{1}{\sigma^2}U_1(\Sigma_1^{-1}+\frac{1}{\sigma^2}U_1^HU_1)^{-1}U_1^H\frac{1}{\sigma^2} - (\frac{1}{\sigma^2}I-\frac{1}{\sigma^2}U_2(\Sigma_2^{-1}+\frac{1}{\sigma^2}U_2^HU_2)^{-1}U_2^H\frac{1}{\sigma^2})\right]y
\end{equation}

Recalling that $U_1,U_2$ have orthonormal columns, we can simplify this to

\begin{equation}
\hat{\Lambda}(y)=\frac{1}{\sigma^2}y^H\left[U_2(\sigma^2\Sigma_2^{-1}+I)^{-1}U_2^H-U_1(\sigma^2\Sigma_1^{-1}+I)^{-1}U_1^H\right]y
\end{equation}

We may also simplify our expressions in (\ref{eq:LRT rule1}) by noting that since $U_1$ and $U_2$ have orthonormal columns by noting that

\begin{equation}
\det(\sigma^2I+U_i\Sigma_iU_i^H)^{1/2} = (\sigma^2)^{p-k}\det(\sigma^2I_k+\Sigma_i)~~i=1,2
\end{equation}

Thus, defining

\begin{equation}\label{eq:LRT}
\tilde{\Lambda}(y)=y^H\left[U_2(\sigma^2\Sigma_2^{-1}+I)^{-1}U_2^H-U_1(\sigma^2\Sigma_1^{-1}+I)^{-1}U_1^H\right]y
\end{equation}

we have the decision rule

\begin{equation}\label{eq:LRT rule2}
\begin{aligned}
&\text{Declare } H_1 \text{ if}
&& \tilde{\Lambda}(y) < \sigma^2\ln\left(\eta^2\frac{\det(\sigma^2I_k+\Sigma_1)}{\det(\sigma^2I_k+\Sigma_2)}\right)\\
& \text{Declare } H_2 \text{ if}
&& \tilde{\Lambda}(y) > \sigma^2\ln\left(\eta^2\frac{\det(\sigma^2I_k+\Sigma_1)}{\det(\sigma^2I_k+\Sigma_2)}\right)\\
\end{aligned}
\end{equation}

We now consider 4 different cases
\begin{enumerate}
\item Equal Covariances of $I_k$
\item Equal Covariances of $\Sigma$
\item $k_1\neq k_2$, but both covariances are identity ($\Sigma_1=I_{k_1}, \Sigma_2=I_{k_2}$)
\item $k_1\neq k_2$, covariances are not identity ($\Sigma_1, \Sigma_2$)
\end{enumerate}

We consider each case individually,

{\bf{Case 1}}

When both covariances are equal and identity, (\ref{eq:LRT}) becomes

\begin{equation}
\begin{aligned}
&\tilde{\Lambda}(y)
&&=
&&&y^H\left[U_2(\sigma^2I+I)^{-1}U_2^H-U_1(\sigma^2I+I)^{-1}U_1^H\right]y\\
&&&=
&&&\frac{1}{1+\sigma^2}y^H\left[U_2U_2^H-U_1U_1^H\right]y\\
\end{aligned}
\end{equation}

Thus, defining

\begin{equation}
\tilde{\Lambda}_1(y)=y^H\left[U_2U_2^H-U_1U_1^H\right]y
\end{equation}

we have the decision rule

\begin{equation}
\begin{aligned}
&\text{Declare } H_1 \text{ if}
&& \tilde{\Lambda}_1(y) < 2(\sigma^2+1)\sigma^2\ln(\eta)\\
& \text{Declare } H_2 \text{ if}
&& \tilde{\Lambda}_1(y) > 2(\sigma^2+1)\sigma^2\ln(\eta)\\
\end{aligned}
\end{equation}

{\bf{Case 2}}

When both covariances are equal but not identity ($\Sigma_1=\Sigma_2=\Sigma$), (\ref{eq:LRT}) becomes

\begin{equation}
\begin{aligned}
&\tilde{\Lambda}(y)
&&=
&&&y^H\left[U_2(\sigma^2\Sigma+I)^{-1}U_2^H-U_1(\sigma^2\Sigma+I)^{-1}U_1^H\right]y\\
&&&=
&&&y^H\left(\left[U_2 -U_1\right](\sigma^2\Sigma+I)^{-1}\left[\begin{array}{c} U_2^H \\ U_1^H \end{array}\right]\right)y\\
\end{aligned}
\end{equation}

Thus, defining

\begin{equation}
\tilde{\Lambda}_2(y)=y^H\left(\left[U_2 -U_1\right](\sigma^2\Sigma+I)^{-1}\left[\begin{array}{c} U_2^H \\ U_1^H \end{array}\right]\right)y\\
\end{equation}

we have the decision rule

\begin{equation}
\begin{aligned}
&\text{Declare } H_1 \text{ if}
&& \tilde{\Lambda}_2(y) < 2\sigma^2\ln(\eta)\\
& \text{Declare } H_2 \text{ if}
&& \tilde{\Lambda}_2(y) > 2\sigma^2\ln(\eta)\\
\end{aligned}
\end{equation}

{\bf{Case 3}}

We now consider the case when $k_1\neq k_2$ but $\Sigma_1=I_{k_1}$ and $\Sigma_2=I_{k_2}$. (\ref{eq:LRT}) becomes

\begin{equation}
\begin{aligned}
&\tilde{\Lambda}(y)
&&=
&&&y^H\left[U_2(\sigma^2I_{k_2}+I_{k_2})^{-1}U_2^H-U_1(\sigma^2I_{k_1}+I_{k_1})^{-1}U_1^H\right]y\\
&&&=
&&&\frac{1}{1+\sigma^2}y^H\left[U_2U_2^H-U_1U_1^H\right]y\\
\end{aligned}
\end{equation}

Thus, defining

\begin{equation}
\tilde{\Lambda}_3(y)=y^H\left[U_2U_2^H-U_1U_1^H\right]y
\end{equation}

we have the decision rule

\begin{equation}
\begin{aligned}
&\text{Declare } H_1 \text{ if}
&& \tilde{\Lambda}_3(y) < 2(\sigma^2+1)\sigma^2\ln(\eta)\\
& \text{Declare } H_2 \text{ if}
&& \tilde{\Lambda}_3(y) > 2(\sigma^2+1)\sigma^2\ln(\eta)\\
\end{aligned}
\end{equation}

which is clearly the same solution as case 1. 

{\bf{Case 4}}

Finally, we consider when $k_1\neq k_2$ and $\Sigma_1\neq \Sigma_2$.

Equations (\ref{eq:LRT}) and (\ref{eq:LRT rule2}) specify our detector as they are as general as possible.

Finally we consider the case when $\eta=1$. In all cases, this forces the right side of all our decision rules to 0. For cases 1 and 3, this simply becomes the nearest neighbor detector. For cases 2 and 4, this becomes a weighted nearest neighbor problem, where we scale our $U$ matrices by a matrix which incorporates our noise variance, $\sigma^2$ and our signal covariance.

\section*{Isotropically Random Noise Detector Proof}

We consider the following binary classification problem:

\begin{equation}
Y=\left\{
\begin{aligned}
&U_1X+W
&& Y\in H_1\\
&U_2X+W
&& Y\in H_2\\
\end{aligned}\right.
\end{equation}

where $W\in\mathbb{C}^{p\times n}$ is isotropically random noise such that $f(W) = f(Z^HWV)$ where $Z\in\mathbb{C}^{p\times p}, V\in\mathbb{C}^{n\times n}$ are orthogonal. X=$\left[x_1,\dots,x_n\right]$ such that $x_i\sim\mathcal{N}(0,\Sigma)$ for $i=1,\dots n$ with known diagonal $\Sigma\in\mathbb{C}^{p\times p}$. We assume $U_1,U_2\in\mathbb{C}^{p\times k}$ are known and have column rank $k$, which is also known.

As $x$ is unknown, we consider the generalized likelihood ratio test (GLRT)

\begin{equation}\label{eq:glrt}
\begin{aligned}
&\Lambda(y)=\frac{\max_xf_2(y)}{\max_xf_1(y)}
\end{aligned}
\end{equation}

where
\begin{equation}
\begin{aligned}
&\text{Declare } H_1:
&& \Lambda(y) < \eta\\
& \text{Declare } H_2:
&& \Lambda(y) > \eta
\end{aligned}
\end{equation}

where $\eta = \frac{\text{Prob}(H_1)}{\text{Prob}(H_2)}$. We assume that $H_1$ and $H_2$ are equally likely so that $\eta = 1$.

\section*{Gaussian Case}

We assume that $W=\left[w_1,\dots,w_n\right]$ is Gaussian such that $w_i\sim\mathcal{N}(0,\sigma^2 I)$

We also have $Y=\left[y_1,\dots,y_n\right]$. Under $H_1$, we have that $y_i\sim\mathcal{N}(U_1x_i,\sigma^2I)$ and under $H_2$, we have that $y_i\sim\mathcal{N}(U_2x_i,\sigma^2I)$ where the $y_i$ are independent regardless of the hypothesis. Therefore, we have

\begin{equation}
\begin{aligned}{cll}
&f_j(Y)
&&=
&&&\prod_{i=1}^nf_j(y_i)\\
&&&=
&&&\prod_{i=1}^n(2\pi\sigma^2)^{-p/2}\exp\{-\frac{1}{2\sigma^2}(y_i-U_jx_i)^H(y_i-U_jx_i)\}\\
&&&=
&&&(2\pi\sigma^2)^{-pn/2}\exp\{-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-U_jx_i)^H(y_i-U_jx_i)\}\\
&&&=
&&&(2\pi\sigma^2)^{-pn/2}\exp\{-\frac{1}{2\sigma^2}/Tr((Y-U_jX)^H(Y-U_jX))\}\\
&&&=
&&&(2\pi\sigma^2)^{-pn/2}\exp\{-\frac{1}{2\sigma^2}\|Y-U_jX\|_F^2\}\\
\end{aligned}
\end{equation}

Our GLRT becomes

\begin{equation}
\Lambda(Y) = \frac{f_2(Y)}{f_1(Y)}=\frac{(2\pi\sigma^2)^{-pn/2}\exp\{-\frac{1}{2\sigma^2}\|Y-U_2X\|_F^2\}}{(2\pi\sigma^2)^{-pn/2}\exp\{-\frac{1}{2\sigma^2}\|Y-U_1X\|_F^2\}}
\end{equation}

which simplifies to

\begin{equation}\label{eq:gauss LRT}
\hat{\Lambda}(Y) = \|Y-U_1X\|_F^2-\|Y-U_2X\|_F^2
\end{equation}

where

\begin{equation}
\begin{aligned}
&\text{Declare } H_1 \text{ if}
&& \hat{\Lambda}(y) < 2\ln(\eta)\\
& \text{Declare } H_2 \text{ if}
&& \hat{\Lambda}(y) > 2\ln(\eta)
\end{aligned}
\end{equation}

Recognizing (\ref{eq:gauss LRT}) as a difference of matrix least squares, we may use the orthogonal invariance of the Frobenius norm to rewrite (\ref{eq:gauss LRT}) as

\begin{equation}\label{eq:gauss LRT2}
\hat{\Lambda}(Y) = \|U_1^HY-X\|_F^2-\|U_2^HY-X\|_F^2
\end{equation}

Let $U_1^HY=A=U_A\Sigma_AV_B^T$ and $U_2^HY=B=U_B\Sigma_BV_B^T$, where $\Sigma_A = \diag(\sigma_{A1},\dots,\sigma_{An}), \Sigma_A = \diag(\sigma_{B1},\dots,\sigma_{Bn})$. $X$ is simply the best rank-k matrix which minimizes the Frobenius norm difference between $A$ and $B$. Then the Eckart Young Mirsky Theorem states that these errors are the sum the last $p-k$ singular values. Therefore, (\ref{eq:gauss LRT2}) becomes

\begin{equation}\label{eq:gauss LRT3}
\hat{\Lambda}(Y) = \sum_{i=k+1}^p\sigma^2_{Ai} - \sum_{i=k+1}^p\sigma^2_{Bi}
\end{equation}

Since $\eta=1$, we have the decision

\begin{equation}
\begin{aligned}
&\text{Declare } H_1 \text{ if}
&& \hat{\Lambda}(y) < 0\\
& \text{Declare } H_2 \text{ if}
&& \hat{\Lambda}(y) > 0
\end{aligned}
\end{equation}

which simply chooses the hypothesis with the lowest reconstruction error.

\end{document}
