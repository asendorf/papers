\IEEEPARstart{T}{he} matched subspace detector (MSD) is a powerful technique, used widely in signal processing  \cite{scharf1991statistical} and machine learning \cite{friedman2001elements}, to detect a signal, which is assumed to lie in a low-rank subspace, buried in noise. The performance of a MSD when the signal subspace is known a priori has been extensively studied (see, for example, \cite{scharf1994matched,jin2005cfar,mcwhorter2003matched, vincent2008matched,besson2006cfar}).

\begin{comment}
 In particular, \cite{scharf1994matched} and \cite{vincent2008matched} consider the setting where the signal lies at a fixed but unknown position in the known subspace. Scharf and Friedlander \cite{scharf1994matched} show that for this setting, the generalized likelihood ratio test (GLRT) is the uniformly most powerful invariant detector. The stochastic setting, considered in \cite{jin2005cfar} and \cite{mcwhorter2003matched}, assumes that the signal is placed randomly in the known subspace.
\end{comment}

In contrast, this paper considers the previously unstudied setting where the signal subspace is unknown and must be estimated from finite, noisy, signal-bearing training data. We are motivated by real-world applications (such as, for example, detection of targets using wavefronts that have propagated through dynamic random media) where an analytical model for the signal subspace is either not known or is inaccurate. In these applications, noise-free training data is unavailable but the practitioner has access to a dataset consisting of noisy, signal-bearing observations collected in a variety of representative  experimental conditions.

In such a setting, an estimate of the unknown low-rank signal subspace may be formed from an eigen-decomposition of the sample covariance matrix of the training data. This subspace estimate may then be `plugged-in' to the detectors derived assuming a known subspace. Because of additive noise and finite training data, the estimated subspace is noisy and we expect a degradation in the performance of these plug-in detectors. Analytically quantifying this performance degradation as a function of the system dimensionality, number of samples, and eigen-SNR is the focus and main contribution of this paper.

This performance characterization relies on recent results from random matrix theory (RMT) \cite{paul2007asymptotics,benaych2011eigenvalues} that precisely quantify the accuracy of the subspace estimate. We characterize the ROC performance of a MSD assuming deterministic and stochastic models for the test vector and also derive a new RMT detector that incorporates this knowledge of the accuracy of the estimated subspace. We considered a basic treatment of the stochastic MSD in \cite{asendorf2011msd} and the deterministic MSD, specifically in the context of missing data, in \cite{asendorf2012msd}.

Our analysis highlights the importance of using the $\keff$ \textit{informative} signal subspace components, where $\keff$ is a function of the system dimensionality, number of samples, and eigen-SNR. By only utilizing the $k_\text{eff}$ \textit{informative} signal subspace components, the new RMT detector avoids the possible performance degradation suffered by the plug-in detector, which may use uninformative components for detection. When the plug-in detector uses exactly $k_\text{eff}$ subspace components, it achieves the same performance as the RMT detector.

%However, the plug-in detector is suboptimal when it uses more than the $k_\text{eff}$ \textit{informative} signal subspace components that can be reliably estimated from finite, noisy training data.

%We consider both a stochastic and deterministic model for the testing data. Under both testing settings, the new RMT detector outperforms the standard plug-in detector. By placing a prior on the deterministic signal vector, the stochastic setting is recovered by integrating with respect to the signal prior.

The paper is organized as follows. We describe the generative models for the training data and test vector and formally pose the questions addressed herein in Section \ref{sec:prob_state}. Section \ref{sec:rmt} contains pertinent results from RMT and justifies our definition in (\ref{eq:keff}) of $\keff$, the (effective) number of informative subspace components. Of particular importance is Corollary \ref{corr:matrix}, which justifies a diagonal approximation for a certain type of matrix that appears throughout our derivations. In sections \ref{sec:msd_stoch} and \ref{sec:msd_determ} we derive the plug-in and the new RMT detectors for the stochastic and deterministic test vector models, respectively. Aided by RMT, we derive the (asymptotic) distribution of the test statistics in Section \ref{sec:roc_theory}; Tables \ref{table:summary_stoch} and \ref{table:summary_determ} summarize the resulting conditional distributions for the plug-in and RMT detector statistics. These distributions are a weighted sum of chi-square random variables from which we can compute ROC curves via a saddlepoint approximation of the distributions obtained. We validate our asymptotic ROC predictions in Section \ref{sec:results} and demonstrate the optimality of using the $k_\text{eff}$ informative subspace components. We provide concluding remarks in Section \ref{sec:conclusion}.
