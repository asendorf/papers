
MULTIPLE CASES


First consider the case when $k=1$. In this setting, $u=U$ is a $n\times 1$ unit-norm vector (sampled with uniform distribution on the unit sphere) and $\sigma^2=\Sigma > 0$ is a scalar. Define $P_n=\sigma^2uu^H$, which is a rank-1 matrix. Let $Z_n$ be a $n\times m$ real or complex matrix with independent $\mathcal{CN}(0,1)$ entries. Let $X_n=\frac{1}{m}Z_nZ_n^H$, which is a random Wishart matrix, have eigenvalues $\lambda_1\left(X_n\right)\geq\dots\geq\lambda_n\left(X_n\right)$. Let $\widehat{X}_n=X_n\left(I_n+P_n\right)$. $X_n$ and $P_n$ are independent by assumption. %Define the empirical eigenvalue distribution as $\mu_{X_n}=\frac{1}{n}\sum_{j=1}^n\delta_{\lambda_j\left(X_n\right)}$. We assume that as $n\to\infty$, $\mu_{X_n}\overset{\text{a.s.}}{\longrightarrow}\mu_X$. 

For $i=2,\dots,n$, let $\widehat{v}_i$ be the eigenvector of $\widehat{X}_n$ associated with the largest $i$-th largest eigenvalue, $\widehat{\lambda}_i$, of $\widehat{X}_n$.  By the eigenvalue master equation, $\widehat{X}_n\widehat{v}_i=\widehat{\lambda}_i\widehat{v}_i$, it follows that
\begin{equation}\label{eq:eig_master}
  \sigma^2\left(\widehat{\lambda}_iI_n-X_n\right)^{-1}X_nuu^H\widehat{v}_i=\widehat{v}_i
\end{equation}
Without loss of generality, we may assume that $X_n$ is diagonal, with entries $\lambda_1\left(X_n\right),\dots,\lambda_n\left(X_n\right)$. We may make this assumption as we could diagonalize $X_n$ and incorporate the eigenvectors of $X_n$ into $u$, i.e. a change of basis. Since $\widehat{v}_i$ is constrained to be unit norm (i.e. $\widehat{v}_i^H\widehat{v}_i = 1$),
\begin{equation}\label{eq:eig_scalar}
  1 = |\langle u,\widehat{v}_i\rangle|^2\sigma^4u^HX_n\left(\widehat{\lambda}_iI_n-X_n\right)^{-2}X_n.
\end{equation}
Specifically, (\ref{eq:eig_scalar}) gives
\begin{equation}\label{eq:in_prod}
  |\langle u,\widehat{v}_i\rangle|^2 = \frac{\sigma^{-4}}{u^HX_n\left(\widehat{\lambda}_iI_n-X_n\right)^{-2}X_nu}
\end{equation}
The denominator of (\ref{eq:in_prod}) is 
\begin{equation}
\begin{aligned}
&u^HX_n\left(\widehat{\lambda}_iI_n-X_n\right)^{-2}X_nu &&= \sum_{\ell=1}^n\frac{|u_\ell|^2\lambda_\ell^2}{\left(\widehat{\lambda}_i-\lambda_\ell\right)^2}\\
&&&\geq \underbrace{\frac{|u_{i-1}|^2\lambda_{i-1}^2}{\left(\widehat{\lambda}_i-\lambda_{i-1}\right)^2}}_{=:1/a} + \underbrace{\frac{|u_{i}|^2\lambda_{i}^2}{\left(\widehat{\lambda}_i-\lambda_{i}\right)^2}}_{=:1/b}\\
\end{aligned}
\end{equation}
With probability 1, $a,b>0$. Therefore, for $i=2,\dots,n$
\begin{equation*}
  \begin{aligned}
    &|\langle u,\widehat{v}_i\rangle|^2 &&= \frac{\sigma^{-4}}{u^HX_n\left(\widehat{\lambda}_i-X_n\right)^{-2}X_nu}\\
    &&&\leq \frac{\sigma^{-4}}{1/a+1/b}\leq \sigma^{-4}(a+b)
  \end{aligned}
\end{equation*}
Since $\sigma^2>0$ we may ignore this constant and focus solely on the limiting behavior of $a+b$
\begin{equation*}
  \begin{aligned}
    &|\langle u,\widehat{u}_i\rangle|^2 &&\leq a+b = \frac{\left(\widehat{\lambda}_i-\lambda_{i-1}\right)^2}{\lambda_{i-1}^2|u_{i-1}|^2}+ \frac{\left(\widehat{\lambda}_i-\lambda_{i}\right)^2}{\lambda_i^2|u_{i}|^2}\\
    &&& \leq \frac{|\lambda_i-\lambda_{i-1}|^2}{\lambda_{i-1}^2|u_{i-1}|^2} + \frac{|\lambda_i-\lambda_{i-1}|^2}{\lambda_i^2|u_{i}|^2} \,\,\,\text{(By Weyl's interlacing lemma } \lambda_i\leq \widehat{\lambda}_i\leq\lambda_{i-1})\\
    &&&\leq \frac{|\lambda_i-\lambda_{i-1}|^2}{\lambda_{i-1}^2}\left(\frac{1}{|u_{i-1}|^2} + \frac{1}{|u_i|^2}\right)\\
    &&&= \underbrace{\frac{n|\lambda_i-\lambda_{i-1}|^2}{\lambda_{i-1}^2}}_{=:t_1}\underbrace{\left(\frac{1}{n|u_{i-1}|^2} + \frac{1}{n|u_i|^2}\right)}_{=:t_2}\\
  \end{aligned}
\end{equation*}

Since $u$ is a random unit-norm vector sampled with uniform distribution on the unit sphere, $t_2\convas 2$. We then must characterize the limiting behavior of $t_1$ which depends on $|\lambda_i-\lambda_{i-1}|^2$, the gap (or spacing) between successive eigenvalues of $X_n$. Since $X_n$ is drawn from the Gaussian Unitary Ensemble (GUE), the spacings are $O(?)$ \cite{someone}. Therefore, for $\epsilon>0$
\begin{equation*}
P(|t_1|>\epsilon) = P(|t_1|^4 > \epsilon^4)\leq \frac{E[|t_1|^4]}{\epsilon^4}= \frac{k^{8}n^{-4/3}}{\epsilon^4}
\end{equation*}
Summing over $n$ yields
\begin{equation*}
\sum_{n=1}^\infty P(|t_1|>\epsilon) \leq \frac{k^{8}}{\epsilon^4}\sum_{n=1}^\infty \frac{1}{n^{4/3}}< \infty
\end{equation*}
Therefore, by the Borel Cantelli lemma, $P(|t_1|>\epsilon \,\text{ i.o.}) = 0$ which by definition means that $t_1\convas0$. Therefore, for $i=2,\dots,n$, $|\langle u,\widehat{v}_i\rangle|^2\convas 0$. 

Recall that, when $k=1$, our observed vectors $y_i\in\complex^{n\times 1}$ have covariance matrix $\sigma^2uu^H+I_n=P_n+I_n$. Therefore, our observation matrix, $Y_n$ which is a $n\times m$ matrix, may be written $Y_n=\left(P_n+I_n\right)^{1/2}Z_n$. The sample covariance matrix, $S_n=\frac{1}{m}Y_nY_n^H$, may be written $S_n=\left(I_n+P_n\right)^{1/2}X_n\left(I_n+P_n\right)^{1/2}$. By similarity transform, if $\widehat{v}_i$ is a unit-norm eigenvector of $\widehat{X}_n$ then $\widehat{w}_i=\left(I_n+P_n\right)^{1/2}\widehat{v}_i$ is an eigenvector of $S_n$. If $\widehat{u}_i=\widehat{w}_i/\|\widehat{w}_i\|$ is a unit-norm eigenvector of $S_n$, it follows that for $i=2,\dots,n$
\begin{equation*}
|\langle u,\widehat{u}_i\rangle|^2=\frac{\left(\sigma^2+1\right)|\langle u_j,\widehat{v}_i\rangle|^2}{\sigma^2|\langle u_j,\widehat{v}_i\rangle|^2+1}
\end{equation*}
As $|\langle u,\widehat{v}_i\rangle|^2\convas 0$ for all $i=2,\dots,n$ it follows that $|\langle u,\widehat{u}_i\rangle|^2\convas 0$ for all $i=2,\dots,n$.

