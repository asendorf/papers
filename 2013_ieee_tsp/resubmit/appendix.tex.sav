\documentclass[12pt]{article}

\usepackage{cite,times, fullpage}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsmath,color}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}



\input my_header.tex

\begin{document}

Let $U_{n,k}$ be a $n\times k$ real or complex matrix with orthonormal columns, $u_i$ $1\leq i\leq k$, and let $\Sigma = \diag\left(\sigma_1^2,\dots,\sigma_k^2\right)$ such that $\sigma_1^2>\sigma_2^2>\dots>\sigma_k^2>0$ for $k\geq 1$ and $n\gg k$.

Recall that our observed vectors $y_i\in\complex^{n\times 1}$ have covariance matrix $U_{n,k}\Sigma U_{n,k}^H+I_n$. Define $P_n=U_{n,k}\Sigma U_{n,k}^H$ so that $P_n$ is rank-$k$. Let $Z_n$ be a $n\times m$ real or complex matrix with independent $\mathcal{N}\left(0,1\right)$ entries.

Let $X_n=\frac{1}{m}Z_nZ_n^H$, which is a random Wishart matrix, have eigenvalues $\lambda_1\left(X_n\right)\geq\dots\geq\lambda_n\left(X_n\right)$. Define the empirical eigenvalue distribution as $\mu_{X_n}=\frac{1}{n}\sum_{j=1}^n\delta_{\lambda_j\left(X_n\right)}$. We assume that as $n\to\infty$, $\mu_{X_n}\overset{\text{a.s.}}{\longrightarrow}\mu_X$.

Therefore, our observation matrix, $Y_n$ which is a $n\times m$ matrix, may be written $Y_n=\left(P_n+I_n\right)^{1/2}Z_n$. The sample covariance matrix, $S_n=\frac{1}{m}Y_nY_n^H$, may be written $S_n=\left(I_n+P_n\right)^{1/2}X_n\left(I_n+P_n\right)^{1/2}$. $X_n$ and $P_n$ are independent by assumption.

We will show that if $\widetilde{v}_i$ is an eigenvector of $S_n$, then for $j\neq i$, $i=1,\dots,n$, $j=1,\dots,k$, $|\langle u_j, \widetilde{v}_i\rangle|^2\overset{\text{a.s.}}{\longrightarrow}0$.

By similarity transform, the matrix $\tilde{X_n}=X_n\left(I_n+P_n\right)$ has the same eigenvalues as $S_n$. If $\widetilde{u}_i$ is a unit-norm eigenvector of $\widetilde{X}_n$ then $\widetilde{w}_i=\left(I_n+P_n\right)^{1/2}\widetilde{u}_i$ is an eigenvector of $S_n$.

Let $\widetilde{u}_i$ be an arbitrary unit eigenvector of $\widetilde{X}_n$. By the eigenvalue master equation, $\widetilde{X}_n\widetilde{u}_i=\widetilde{\lambda}_i\widetilde{u}_i$, it follows that
\begin{equation}\label{eq:eval_master}
\begin{aligned}
%&\widetilde{X}_n\widetilde{u}_i&&=\widetilde{\lambda}_i\widetilde{u}_i\\
%&X_n\left(I_n+P_n\right)\widetilde{u}_i&&=\widetilde{\lambda}_i\widetilde{u}_i\\
%&X_n\left(I_n+U_{n,k}\Sigma U_{n,k}^H\right)\widetilde{u}_i&&=\widetilde{\lambda}_i\widetilde{u}_i\\
%&X_nU_{n,k}\Sigma U_{n,k}^H\widetilde{u}_i&&=\left(\widetilde{\lambda}_iI_n-X_n\right)\widetilde{u}_i\\
%&\left(\widetilde{\lambda}_iI_n-X_n\right)^{-1}X_nU_{n,k}\Sigma U_{n,k}^H\widetilde{u}_i&&=\widetilde{u}_i\\
&U_{n,k}^H\left(\widetilde{\lambda}_iI_n-X_n\right)^{-1}X_nU_{n,k}\Sigma U_{n,k}^H\widetilde{u}_i&&=U_{n,k}^H\widetilde{u}_i.\\
\end{aligned}
\end{equation}
Without loss of generality, we may assume that $X_n$ is diagonal, with entries $\lambda_1\left(X_n\right),\dots,\lambda_n\left(X_n\right)$. We may make this assumption as we could diagonalize $X_n$ and incorporate the eigenvectors of $X_n$ into $U_{n,k}$, i.e. a change of basis. Using this assumption, (\ref{eq:eval_master}) simplifies to
\begin{equation}\label{eq:t_trans}
\left[T_{\mu_{r,j}^{\left(n\right)}}\left(\widetilde{\lambda}_i\right)\right]_{r,j=1}^k \Sigma U_{n,k}^H\widetilde{u}_i=U_{n,k}^H\widetilde{u}_i
\end{equation}
where $\mu_{r,j}^{\left(n\right)}$, $r=1,\dots,k$, $j=1,\dots,k$, is the random complex measure defined by
\begin{equation*}
\mu_{r,j}^{\left(n\right)}=\sum_{\ell=1}^n\overline{u_{\ell,r}^{\left(n\right)}}u_{\ell,j}^{\left(n\right)}\delta_{\lambda_\ell\left(X_n\right)}
\end{equation*}
and $T_{\mu_{r,j}^{\left(n\right)}}$ is the T-transform defined by
\begin{equation*}
T_{\mu}\left(z\right) = \int\frac{t}{z-t}d\mu\left(t\right)\,\,\,\,\text{for} z\not\in\text{supp } \mu.
\end{equation*}
We may rewrite (\ref{eq:t_trans}) as
\begin{equation*}
\left(I_k-\left[\sigma_j^2T_{\mu_{r,j}^{\left(n\right)}}\left(\widetilde{\lambda}_i\right)\right]_{r,j=1}^k\right)U_{n,k}^H\widetilde{u}_i=0.
\end{equation*}
Therefore, $U_{n,k}^H\widetilde{u}_i$ must be in the kernel of $M_n\left(\widetilde{\lambda}_i\right)=I_k-\left[\sigma_j^2T_{\mu_{r,j}^{\left(n\right)}}\left(\widetilde{\lambda}_i\right)\right]_{r,j=1}^k$.
By Proposition 9.3 of \cite{benaych2011eigenvalues}
\begin{equation*}
\mu_{r,j}^{\left(n\right)}\overset{\text{a.s.}}{\longrightarrow}\begin{cases}\mu_X & \text{for } i=j \\ \delta_0 & \text{o.w.} \end{cases}
\end{equation*}
where $\mu_X$ is the limiting eigenvalue distribution of $X_n$. Therefore,
\begin{equation*}
M_n\left(\widetilde{\lambda}_i\right)\overset{\text{a.s.}}{\longrightarrow}\diag\left(1-\sigma_1^2T_{\mu_X}\left(\widetilde{\lambda}_i\right), \dots, 1-\sigma_k^2T_{\mu_X}\left(\widetilde{\lambda}_i\right)\right)
\end{equation*}
Assume that $i\leq k$ and $\sigma_i^2>1/T_{\mu_X}(b^+)$, where $b$ is the supremum of the support of $\mu_X$. As $\widetilde{\lambda}_i$ is the eigenvalue corresponding to the eigenvector $\widetilde{u}_i$, by Theorem 2.6 of \cite{benaych2011eigenvalues} $\widetilde{\lambda}_i\overset{\text{a.s.}}{\longrightarrow}T^{-1}_{\mu_X}\left(1/\sigma_i^2\right)$. Therefore,
\begin{equation}\label{eq:Mn}
M_n\left(\widetilde{\lambda}_i\right)\overset{\text{a.s.}}{\longrightarrow}\diag\left(1-\frac{\sigma_1^2}{\sigma_i^2}, \dots, 1-\frac{\sigma_{i-1}^2}{\sigma_i^2}, 0, 1-\frac{\sigma_{i+1}^2}{\sigma_i^2}, \dots, 1-\frac{\sigma_k^2}{\sigma_i^2}\right)
\end{equation}
 Recall that $U_{n,k}^H\widetilde{u}_i$ must be in the kernel of $M_n\left(\widetilde{\lambda}_i\right)$. Any limit point of $U_{n,k}^H\widetilde{u}_i$ is in the kernel of the matrix on the right hand side of (\ref{eq:Mn}). Therefore, for $i\neq j$, $i=1,\dots,k$, $j=1,\dots,k$, we must have that $\left(1-\frac{\sigma_j^2}{\sigma_i^2}\right)\langle u_j,\widetilde{u}_i\rangle=0$. As $\sigma_i^2\neq\sigma_j^2$ for this condition to be satisfied, we have that for $j\neq i$, $i=1,\dots,k$, $j=1,\dots,k$, $\langle u_j,\widetilde{u}_i\rangle\overset{\text{a.s.}}{\longrightarrow}0$. Therefore, $|\langle u_j,\widetilde{u}_i\rangle|^2\overset{\text{a.s.}}{\longrightarrow}0$ for $i\neq j$, $i=1,\dots,k$, $j=1,\dots,k$. This holds for all eigenvectors $\widetilde{u}_i$, $1\leq i\leq n$, of $\widetilde{X}_n$ as $\widetilde{u}_i$ was chosen arbitrarily.

TODO: approach if $i> k$ or $\sigma_i^2\leq 1/T_{\mu_X}(b^+)$.

Recall that by similarity transform, if $\widetilde{u}_i$ is a unit-norm eigenvector of $\widetilde{X}_n$ then $\widetilde{w}_i=\left(I_n+P_n\right)^{1/2}\widetilde{u}_i$ is an eigenvector of $S_n$. If $\widetilde{v}=\widetilde{w}_i/\|\widetilde{w}_i\|$ is a unit-norm eigenvector of $S_n$, it follows that
\begin{equation*}
|\langle u_j,\widetilde{v}_i\rangle|^2=\frac{(\sigma_i^2+1)|\langle u_j,\widetilde{u}_i\rangle|^2}{\sigma_i^2|\langle u_j,\widetilde{u}_i\rangle|^2+1}
\end{equation*}
As $|\langle u_j,\widetilde{u}_i\rangle|^2\overset{\text{a.s.}}{\longrightarrow}0$ for all $i\neq j$ it follows that $|\langle u_j,\widetilde{v}_i\rangle|^2\overset{\text{a.s.}}{\longrightarrow}0$ for all $i\neq j$ which was desired to be shown.

\end{document}


