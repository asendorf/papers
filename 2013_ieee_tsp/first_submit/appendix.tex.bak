\begin{proof}
We provide the proof of Theorem \ref{th:angles} when $i\neq j$.

Let $U_{n,k}$ be a $n\times k$ real or complex matrix with orthonormal columns, $u_i$ for $1\leq i\leq k$. Let $\Sigma = \diag\left(\sigma_1^2,\dots,\sigma_k^2\right)$ such that $\sigma_1^2>\sigma_2^2>\dots>\sigma_k^2>0$ for $k\geq 1$. Define $P_n=U_{n,k}\Sigma U_{n,k}^H$ so that $P_n$ is rank-$k$. Let $Z_n$ be a $n\times m$ real or complex matrix with independent $\mathcal{CN}\left(0,1\right)$ entries. Let $X_n=\frac{1}{m}Z_nZ_n^H$, which is a random Wishart matrix, have eigenvalues $\lambda_1\left(X_n\right)\geq\dots\geq\lambda_n\left(X_n\right)$. Let $\widehat{X}_n=X_n\left(I_n+P_n\right)$. $X_n$ and $P_n$ are independent by assumption. Define the empirical eigenvalue distribution as $\mu_{X_n}=\frac{1}{n}\sum_{j=1}^n\delta_{\lambda_j\left(X_n\right)}$. We assume that as $n\to\infty$, $\mu_{X_n}\overset{\text{a.s.}}{\longrightarrow}\mu_X$.

Let $k_\text{eff}=k$. For $i=1,\dots, k$, let $\widehat{v}_i$ be an arbitrary unit eigenvector of $\widehat{X}_n$. By the eigenvalue master equation, $\widehat{X}_n\widehat{v}_i=\widehat{\lambda}_i\widehat{v}_i$, it follows that
\begin{equation}\label{eq:eval_master}
\begin{aligned}
%&\widehat{X}_n\widehat{v}_i&&=\widehat{\lambda}_i\widehat{v}_i\\
%&X_n\left(I_n+P_n\right)\widehat{v}_i&&=\widehat{\lambda}_i\widehat{v}_i\\
%&X_n\left(I_n+U_{n,k}\Sigma U_{n,k}^H\right)\widehat{v}_i&&=\widehat{\lambda}_i\widehat{v}_i\\
%&X_nU_{n,k}\Sigma U_{n,k}^H\widehat{v}_i&&=\left(\widehat{\lambda}_iI_n-X_n\right)\widehat{v}_i\\
%&\left(\widehat{\lambda}_iI_n-X_n\right)^{-1}X_nU_{n,k}\Sigma U_{n,k}^H\widehat{v}_i&&=\widehat{v}_i\\
&U_{n,k}^H\left(\widehat{\lambda}_iI_n-X_n\right)^{-1}X_nU_{n,k}\Sigma U_{n,k}^H\widehat{v}_i&&=U_{n,k}^H\widehat{v}_i.\\
\end{aligned}
\end{equation}
Without loss of generality, we may assume that $X_n$ is diagonal, with entries $\lambda_1\left(X_n\right),\dots,\lambda_n\left(X_n\right)$. We may make this assumption as we could diagonalize $X_n$ and incorporate the eigenvectors of $X_n$ into $U_{n,k}$, i.e. a change of basis. Using this assumption, (\ref{eq:eval_master}) simplifies to
\begin{equation}\label{eq:t_trans}
\left[T_{\mu_{r,j}^{\left(n\right)}}\left(\widehat{\lambda}_i\right)\right]_{r,j=1}^k \Sigma U_{n,k}^H\widehat{v}_i=U_{n,k}^H\widehat{v}_i
\end{equation}
where $\mu_{r,j}^{\left(n\right)}$, $r=1,\dots,k$, $j=1,\dots,k$, is the random complex measure defined by
\begin{equation*}
\mu_{r,j}^{\left(n\right)}=\sum_{\ell=1}^n\overline{u_{\ell,r}^{\left(n\right)}}u_{\ell,j}^{\left(n\right)}\delta_{\lambda_\ell\left(X_n\right)}
\end{equation*}
and $T_{\mu_{r,j}^{\left(n\right)}}$ is the T-transform defined by
\begin{equation*}
T_{\mu}\left(z\right) = \int\frac{t}{z-t}d\mu\left(t\right)\,\,\,\,\text{for } z\not\in\text{supp } \mu.
\end{equation*}
We may rewrite (\ref{eq:t_trans}) as
\begin{equation*}
\left(I_k-\left[\sigma_j^2T_{\mu_{r,j}^{\left(n\right)}}\left(\widehat{\lambda}_i\right)\right]_{r,j=1}^k\right)U_{n,k}^H\widehat{v}_i=0.
\end{equation*}
Therefore, $U_{n,k}^H\widehat{v}_i$ must be in the kernel of $M_n\left(\widehat{\lambda}_i\right)=I_k-\left[\sigma_j^2T_{\mu_{r,j}^{\left(n\right)}}\left(\widehat{\lambda}_i\right)\right]_{r,j=1}^k$.
By Proposition 9.3 of \cite{benaych2011eigenvalues}
\begin{equation*}
\mu_{r,j}^{\left(n\right)}\overset{\text{a.s.}}{\longrightarrow}\begin{cases}\mu_X & \text{for } i=j \\ \delta_0 & \text{o.w.} \end{cases}
\end{equation*}
where $\mu_X$ is the limiting eigenvalue distribution of $X_n$. Therefore,
\begin{equation*}
M_n\left(\widehat{\lambda}_i\right)\overset{\text{a.s.}}{\longrightarrow}\diag\left(1-\sigma_1^2T_{\mu_X}\left(\widehat{\lambda}_i\right), \dots, 1-\sigma_k^2T_{\mu_X}\left(\widehat{\lambda}_i\right)\right)
\end{equation*}
As $k_\text{eff}=k$, $\sigma_i^2>1/T_{\mu_X}(b^+)$, where $b$ is the supremum of the support of $\mu_X$. As $\widehat{\lambda}_i$ is the eigenvalue corresponding to the eigenvector $\widehat{v}_i$, by Theorem 2.6 of \cite{benaych2011eigenvalues} $\widehat{\lambda}_i\overset{\text{a.s.}}{\longrightarrow}T^{-1}_{\mu_X}\left(1/\sigma_i^2\right)$. Therefore,
\begin{equation}\label{eq:Mn}
M_n\left(\widehat{\lambda}_i\right)\overset{\text{a.s.}}{\longrightarrow}\diag\left(1-\frac{\sigma_1^2}{\sigma_i^2}, \dots, 1-\frac{\sigma_{i-1}^2}{\sigma_i^2}, 0, 1-\frac{\sigma_{i+1}^2}{\sigma_i^2}, \dots, 1-\frac{\sigma_k^2}{\sigma_i^2}\right)
\end{equation}
Recall that $U_{n,k}^H\widehat{v}_i$ must be in the kernel of $M_n\left(\widehat{\lambda}_i\right)$. Therefore, any limit point of $U_{n,k}^H\widehat{v}_i$ is in the kernel of the matrix on the right hand side of (\ref{eq:Mn}). Therefore, for $i\neq j$, $i=1,\dots,k$, $j=1,\dots,k$, we must have that $\left(1-\frac{\sigma_j^2}{\sigma_i^2}\right)\langle u_j,\widehat{v}_i\rangle=0$. As $\sigma_i^2\neq\sigma_j^2$, for this condition to be satisfied we must have that for $j\neq i$, $i=1,\dots,k$, $j=1,\dots,k$, $\langle u_j,\widehat{v}_i\rangle\overset{\text{a.s.}}{\longrightarrow}0$. When $k_\text{eff}=k$, this holds for all eigenvectors $\widehat{v}_i$, $1\leq i\leq k$, of $\widehat{X}_n$ as $\widehat{v}_i$ was chosen arbitrarily.

Recall that our observed vectors $y_i\in\complex^{n\times 1}$ have covariance matrix $U_{n,k}\Sigma U_{n,k}^H+I_n=P_n+I_n$. Therefore, our observation matrix, $Y_n$ which is a $n\times m$ matrix, may be written $Y_n=\left(P_n+I_n\right)^{1/2}Z_n$. The sample covariance matrix, $S_n=\frac{1}{m}Y_nY_n^H$, may be written $S_n=\left(I_n+P_n\right)^{1/2}X_n\left(I_n+P_n\right)^{1/2}$. By similarity transform, if $\widehat{v}_i$ is a unit-norm eigenvector of $\widehat{X}_n$ then $\widehat{w}_i=\left(I_n+P_n\right)^{1/2}\widehat{v}_i$ is an eigenvector of $S_n$. If $\widehat{u}_i=\widehat{w}_i/\|\widehat{w}_i\|$ is a unit-norm eigenvector of $S_n$, it follows that
\begin{equation*}
\langle u_j,\widehat{u}_i\rangle=\frac{\sqrt{\sigma_i^2+1}\langle u_j,\widehat{v}_i\rangle}{\sqrt{\sigma_i^2|\langle u_j,\widehat{v}_i\rangle|^2+1}}
\end{equation*}
As $\langle u_j,\widehat{v}_i\rangle\overset{\text{a.s.}}{\longrightarrow}0$ for all $i\neq j$ it follows that $\langle u_j,\widehat{u}_i\rangle\overset{\text{a.s.}}{\longrightarrow}0$ for all $i\neq j$ which was desired to be shown.

An extension of this argument proves the general result when $i>k$ or when $\keff <k$; however, the derivation is more intricate and we omit it here for space considerations. An important assumption needed for the proof is that for fixed $i$ and $k$, $n |\lambda_{i+k} - \lambda_{i}|^{2} \convas 0$, which occurs due to  the `rigidity' of the eigenvalues of $X_n$. Tao  \cite{tao2012asymptotic}) recently proved that these conditions hold for the Wigner matrix. By the universality \cite{erdHos2011universality} properties of random matrices, this spacing assumption is justified for the non-zero eigenvalues of the Wishart matrix.

\end{proof}










