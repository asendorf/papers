\subsection{Training Data Model}\label{sec:training_data}We are given
  $m$ signal-bearing training vectors $y_i\in \complex^{n\times 1}$, $i=1,\dots,m$, modeled\footnote{For expositional simplicity, we have assumed that all our matrices and vectors are complex-valued; our results also hold for real-valued matrices and vectors.} as $y_i=Ux_i+z_i$ where $z_i\overset{\text{i.i.d.}}{\sim}\mathcal{CN}(0,I_n)$, $U$ is an unknown $n\times k$ complex matrix with orthonormal columns, and $x_i\overset{\text{i.i.d.}}{\sim}\mathcal{CN}(0,\Sigma)$ where $\Sigma=\diag(\sigma_1^2,\dots,\sigma_k^2)$ with $\sigma_1>\sigma_2>\dots>\sigma_k>0$ unknown. For each observation, $x_i$ and $z_i$ are independent. The dimension, $k$, of our subspace is unknown and we assume throughout that $k\ll n$ so that we have a low-rank signal embedded in a high-dimensional observation vector.

  Given a dimension estimate, $\widehat{k}$, and the signal bearing training data $Y = \begin{bmatrix} y_1 & \dots & y_m \end{bmatrix}$, we form a subspace estimate $\widehat{U}\in\complex^{n\times\widehat{k}}$ by taking the leading $\widehat{k}$ eigenvectors vectors of $YY^{H}/m$ and a signal covariance estimate $\widehat{\Sigma}\in\reals^{\widehat{k}\times\widehat{k}}$ (in a manner to be specified).

\subsection{Testing Data Model}
We will consider two models for the test vectors. In the stochastic setting, the test vector $y\in\complex^{n\times 1}$ is modeled as
\begin{equation}\label{eq:stoch_setup}
\text{Stochastic Model: }y=\left\{
\begin{aligned}
&z
&& y\in H_0:\text{ Noise only}\\
&Ux+z
&& y\in H_1:\text{ Signal-plus noise}\\
\end{aligned}\right. ,
\end{equation}
where $U$, $z$, and $x$ are modeled as described in Section \ref{sec:training_data}. This assumes that the signal, $Ux$, may lie anywhere in the subspace and whose position in the subspace is governed by the signal covariance matrix $\Sigma$.

In the deterministic setting, the test vector $y\in\complex^{n\times 1}$ is modeled as
\begin{equation}\label{eq:determ_setup}
\text{Deterministic Model: }y=\left\{
\begin{aligned}
&z
&& y\in H_0:\text{ Noise only}\\
&U\Sigma^{1/2} x+z
&& y\in H_1:\text{ Signal-plus noise}\\
\end{aligned}\right. ,
\end{equation}
where $U$, $\Sigma$, and $z$ are modeled as before. Here, in contrast to the stochastic setting, $x$ is a non-random deterministic vector. Thus the signal, $U\Sigma^{1/2}x$, lies at a fixed point in the unknown subspace. Note that placing a mean zero, identity covariance Gaussian prior on $x$ in (\ref{eq:determ_setup}) yields the stochastic model described in (\ref{eq:stoch_setup}).

\subsection{Problem 1: Characterize the ROC Performance Curves}\label{sec:problem 1}
Given an independent test observation from (\ref{eq:stoch_setup}) or (\ref{eq:determ_setup}), we first use $\widehat{U}$ to generate a $\widehat{k}\times 1$ test vector $w=\widehat{U}^Hy$. The vector $w$ is a sufficient statistic \cite{scharf1991statistical} when $\widehat{U} = U$. We focus on detectors of the form
\begin{equation}\label{eq:detector_form}
w^HDw\detgtrless\eta,
\end{equation}
where $D$ is a diagonal matrix and the test statistic $\Lambda(w) := w^HDw$ is compared against a threshold, $\eta$, set to achieve a prescribed false alarm rate $\alpha$. For detectors of this form and for test vectors modeled as (\ref{eq:stoch_setup}) or (\ref{eq:determ_setup}), our goal is to
\begin{center}
Predict $P_D=:\mathbb{P}(\text{Detection})$, for every $P_F:=\alpha \in (0,1)$ given $n$, $m$, $\widehat{k}$, $D$ and $\Sigma$.
\end{center}

This performance prediction relies on RMT results quantifying the accuracy of the subspace estimate $\widehat{U}$ as a function of the parameters listed.
%We will consider detectors, $g(w)\to\{H_0,H_1\}$, which solve
%\begin{equation}\label{eq:maximization}
%\begin{aligned}
%&\text{maximize}
%&& P_D=P\left(g(w)\to H_1 | w\in H_1\right)\\
%&\text{subject to}
%&& P_F=P\left(g(w)\to H_1 | w\in H_0\right)\leq\alpha\\
%\end{aligned}
%\end{equation}
%where $\alpha\in[0,1]$.

\subsection{Problem 2: Derive and Predict the Performance of a Detector that Exploits  Predictions of Subspace Accuracy}\label{sec:ps_prob2}
In the  Neyman-Pearson setting (see \cite{van1968detection}), the detector is a likelihood ratio test (LRT) taking the form
\begin{equation*}
\Lambda(w):=\dfrac{f(w|H_1)}{f(w|H_0)} \detgtrless \eta
\end{equation*}
where $\Lambda(w)$ is the test statistic, $\eta$ is the threshold set to achieve a given false alarm rate, and $w = \widehat{U}^{H}y$. Here, $\widehat{U}$ is a noisy estimate of the underlying $U$; its accuracy, relative to $U$, can be quantified using RMT. Therefore, $f(w|H_0)$ and $f(w|H_1)$, and consequently $\Lambda(w)$, depend on the `noisiness' of the estimated subspaces.  Our goal is thus to
\begin{quote}
Design \& analyze the performance of  a detector that exploits RMT predictions of subspace estimation accuracy.
\end{quote}
The design and performance prediction aspect of this problem will provide insights on when, if, and how the performance of plug-in detectors that do not exploit the knowledge of subspace estimation accuracy can be improved.


%and we employ the generalized likelihood ratio test (GLRT). It is standard to substitute estimates $\widehat{U}$  and $\widehat{\Sigma}$ for the unknown $U$ and $\Sigma$ in the oracle test statistic \cite{jin2005cfar,mcwhorter2003matched}. This resulting plug-in detector assumes that the parameter estimates are exact. Through our analysis, we characterize the performance loss associated with making this assumption and derive a new detector which can systematically avoid this performance loss. By relying on the subspace accuracy estimates presented in Theorem \ref{th:angles}, the new RMT detector only utilizes $\min(\widehat{k},k_\text{eff})$ subspace components to form an approximation to the oracle detector. In both testing scenarios, the plug-in and RMT detectors take the desired form of (\ref{eq:detector_form}).
